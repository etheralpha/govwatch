{"rss":{"@version":"2.0","@xmlns:discourse":"http://www.discourse.org/","@xmlns:atom":"http://www.w3.org/2005/Atom","@xmlns:dc":"http://purl.org/dc/elements/1.1/","channel":{"title":"Ethereum Research - Latest topics","link":"https://ethresear.ch/latest","description":"Latest topics","lastBuildDate":"Tue, 02 Apr 2024 03:16:14 +0000","atom:link":{"@href":"https://ethresear.ch/latest.rss","@rel":"self","@type":"application/rss+xml"},"item":[{"title":"SKDE: Enhancing Rollup Composability with Trustless Sequencing","dc:creator":"wooju","category":"Uncategorized","description":"<ul>\n<li><em>by Hankyung Ko(<a class=\"mention\" href=\"https://ethresear.ch/u/hankyungko\">@HankyungKo</a>) and Chanyang Ju(<a class=\"mention\" href=\"https://ethresear.ch/u/wooju\">@wooju</a>), Researcher at</em> <em><a href=\"https://twitter.com/radius_xyz\" rel=\"noopener nofollow ugc\">Radius</a></em> <em>. Thanks to</em> <em><a href=\"https://twitter.com/Hyunxukee\" rel=\"noopener nofollow ugc\">Tariz</a></em> <em>and</em> <em><a href=\"https://twitter.com/ZeroKnight_eth\" rel=\"noopener nofollow ugc\">AJ</a></em> <em>for reviewing this post.</em></li>\n<li><em>Your feedback and opinions are highly valued. For the original content, please visit</em> <em><a href=\"https://hackmd.io/@Radius/rJUjYRwyA\" rel=\"noopener nofollow ugc\">our blog</a></em> <em>.</em></li>\n</ul>\n<h1><a name=\"h-1-introduction-1\" class=\"anchor\" href=\"https://ethresear.ch#h-1-introduction-1\"></a>1. Introduction</h1>\n<hr>\n<p><a href=\"https://www.theradius.xyz/\" rel=\"noopener nofollow ugc\">Radius</a> is at the forefront of enhancing rollup composability through the development of ‘Shared Sequencer’. Enhancing composability between rollups allows users to access more opportunities, such as arbitrage, enhancing the blockchain ecosystem’s fluidity and potential for innovation. Service providers benefit as well, as they can manage multiple app-specific rollups and support seamless operations, scaling their services effectively. Our research indicates that the ‘Shared Sequencer’ is foundational to achieving the vision of inter-rollup composability. The ‘Shared Sequencer’ ensures atomic inclusion of transactions across multiple rollups or between Layer 1 (L1) and Layer 2 (L2) networks, representing a critical infrastructure component in our system.</p>\n<p>The ‘Shared Sequencer’ must be a neutral entity, more so than any other within the ecosystem. As the entity responsible for constructing blocks across multiple rollups, it must operate without bias, adhering strictly to predefined rules. The essence of being the most neutral sequencer means that the sequencing of blocks must be free from any hidden agendas, creating a trustable system where blocks are produced based solely on these established rules. This neutrality is paramount to prevent any potential for manipulation or unfair advantage within the network.</p>\n<p>Many of the security concerns in blockchain are addressed through cryptography or crypto-economic (optimistic) solutions, with the latter being effective when actions can be retrospectively verified for legitimacy. However, the role of the sequencer presents unique challenges; if a sequencer acts maliciously, such as censoring transactions or engaging in front-running attacks, these actions are not easily identifiable after the fact. Therefore, the development of a neutral shared sequencer necessitates a cryptographic approach. We see potential in an ‘encrypted mempool,’ where the encryption of transaction details hinders the ability of malicious actors to censor, reorder, or manipulate transactions for their benefit, fostering a more secure and trustworthy blockchain environment.</p>\n<h2><a name=\"h-11-delay-encryption-within-an-encrypted-mempool-2\" class=\"anchor\" href=\"https://ethresear.ch#h-11-delay-encryption-within-an-encrypted-mempool-2\"></a>1.1. Delay Encryption within an Encrypted Mempool</h2>\n<hr>\n<p>We have concentrated on designing an encrypted mempool using ‘Delay encryption,’ a cryptographic algorithm that requires a predetermined amount of computation time to obtain the decryption key, utilizing timelock puzzles. This design ensures that the sequencer can commit to the transaction order before reaching the timelock parameter, thus enforcing an unbiased ordering process. The use of delay encryption introduces a mechanism where transactions are secured and ordered without the possibility of tampering or bias, establishing a fair and transparent sequencing process.</p>\n<p>There are currently two main methods to create a censorship-resistant sequencer using delay encryption: Radius’s original <a href=\"https://ethresear.ch/t/mev-resistant-zk-rollups-with-practical-vde-pvde/12677\">Practical Verifiable Delay Encryption (PVDE)</a> and Scroll’s Multiparty Delay Encryption (MDE). Both methods have their trade-offs. PVDE achieves complete trustlessness, allowing users to trust that their transactions are securely included in a block without relying on any external entities. However, this comes at the cost of higher computational expenses for both users and sequencers. On the other hand, MDE reduces computational costs for users and sequencers by introducing multiparty key generation, but this necessitates a 1-out-of-N trust assumption and, as our analysis shows, incurs significant gas fees for storing keys publicly with each block.</p>\n<p>This article introduces Radius’s new encrypted mempool model, Single Key Delay Encryption (SKDE), which inherits the advantages of MDE while addressing its two main drawbacks: the trust assumption and high gas fees. We will briefly discuss how PVDE and MDE achieve their respective trade-offs and then delve into the innovative aspects of our SKDE model. The article will also present experimental results comparing the three models, showcasing the effectiveness and efficiency of SKDE in enhancing the security and trustworthiness of blockchain transaction sequencing.</p>\n<h1><a name=\"h-2-background-3\" class=\"anchor\" href=\"https://ethresear.ch#h-2-background-3\"></a>2. Background</h1>\n<hr>\n<p>Delay encryption is a cryptographic tool that employs time-lock puzzles to enforce a predetermined delay on the availability of a decryption key. This method ensures that encrypted data cannot be decrypted until a specified amount of computational work, often measured in time, has been completed.</p>\n<p>As its core, delay encryption involves two main phases: encryption and timed decryption. During the encryption phase, data is encrypted using a standard cryptographic algorithm, and a time-lock puzzle is generated. This puzzle is constructed in such a way that solving it requires a predictable amount of computational effort, effectively creating a “delay” before the information can be accessed.</p>\n<h2><a name=\"h-21-radius-original-pvde-4\" class=\"anchor\" href=\"https://ethresear.ch#h-21-radius-original-pvde-4\"></a>2.1. Radius’ Original PVDE</h2>\n<hr>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/5/5b65fa0efb6b614032462656e116a754e20a36b5.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/5b65fa0efb6b614032462656e116a754e20a36b5\" title=\"pvde\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/5/5b65fa0efb6b614032462656e116a754e20a36b5_2_690x352.jpeg\" alt=\"pvde\" data-base62-sha1=\"d2xVUAYYwibeDJd3iMegmbAxdBP\" width=\"690\" height=\"352\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/5/5b65fa0efb6b614032462656e116a754e20a36b5_2_690x352.jpeg, https://ethresear.ch/uploads/default/optimized/2X/5/5b65fa0efb6b614032462656e116a754e20a36b5_2_1035x528.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/5/5b65fa0efb6b614032462656e116a754e20a36b5_2_1380x704.jpeg 2x\" data-dominant-color=\"F3F3F3\"></a></div><p></p>\n<p><em>&lt;The overview of Radius’ original PVDE model&gt;</em></p>\n<p>Radius’s original Practical Verifiable Delay Encryption (PVDE) is a sophisticated approach to securing transactions through the use of symmetric key encryption. This method places the onus of encryption key generation directly on the user, effectively eliminating the need for trust assumptions. This trustless model is pivotal in ensuring that users retain complete control over their transaction privacy without depending on external validators or third parties.</p>\n<p>Nevertheless, the implementation of PVDE brings to the fore certain computational challenges, particularly attributed to its reliance on zero-knowledge proofs (ZKPs) and the requirement for sequencers to solve timelock puzzles for each transaction. The computational overhead is twofold:</p>\n<ol>\n<li><strong>Zero-Knowledge Proofs</strong>: Users are required to conduct ZKP proving to validate their encryption keys and the integrity of the encryption process. While ZKPs offer a powerful means to achieve privacy and security by enabling the verification of information without revealing the underlying data, generating these proofs entails a significant computational effort on the part of the users.</li>\n<li><strong>Timelock Puzzle Solving by Sequencers</strong>: Beyond the ZKP-related overhead, sequencers face an additional, substantial computational burden due to the necessity of solving a timelock puzzle for each transaction. Given a timelock parameter set to, for instance, 100 milliseconds, a sequencer operating on a single thread could decrypt fewer than 10 transactions per second. This limitation poses a significant bottleneck, severely impacting the system’s usability and scalability.</li>\n</ol>\n<p>Furthermore, PVDE employs the zk-friendly Poseidon encryption algorithm for its encryption scheme. While Poseidon is lauded for its compatibility with zero-knowledge proof systems, it is inherently more computationally intensive than more conventional encryption algorithms. This additional complexity contributes to the decryption process’s overall computational demands on the sequencer.</p>\n<h2><a name=\"h-22-scrolls-public-key-delay-encryption-with-multiparty-key-generation-5\" class=\"anchor\" href=\"https://ethresear.ch#h-22-scrolls-public-key-delay-encryption-with-multiparty-key-generation-5\"></a>2.2. Scroll’s Public Key Delay Encryption with Multiparty Key Generation</h2>\n<hr>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/9/989b498b598377ee23d05d57a556dcb742d5af6f.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/989b498b598377ee23d05d57a556dcb742d5af6f\" title=\"mde\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/9/989b498b598377ee23d05d57a556dcb742d5af6f_2_690x366.jpeg\" alt=\"mde\" data-base62-sha1=\"lM1haHjfOsyOq9gGBBAcOQr84gf\" width=\"690\" height=\"366\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/9/989b498b598377ee23d05d57a556dcb742d5af6f_2_690x366.jpeg, https://ethresear.ch/uploads/default/optimized/2X/9/989b498b598377ee23d05d57a556dcb742d5af6f_2_1035x549.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/9/989b498b598377ee23d05d57a556dcb742d5af6f_2_1380x732.jpeg 2x\" data-dominant-color=\"E2E9EF\"></a></div><p></p>\n<p><em>&lt;The Overview of Scroll’s MDE model&gt;</em></p>\n<p>Unlike PVDE, which relies on symmetric key encryption, Scroll’s MDE utilizes asymmetric key encryption. This shift brings about a new approach to generating encryption and extraction <span class=\"math\">(enc, ext)</span> key pairs. Rather than being generated by the user, these key pairs are created by a committee, which operates under the assumption that at least one member is trustworthy. Once the <span class=\"math\">(enc, ext)</span> key pair is made public, the decryption key can be extracted from the <span class=\"math\">ext</span> key after a specified amount of computational effort, equivalent to a set period.</p>\n<p>For sequencers, this model simplifies the process significantly since <strong>only one timelock puzzle needs to be solved</strong>, making the timelock parameter more manageable. Furthermore, users are not required to generate ZKP proof for their transactions, and the system does not rely on zk-friendly encryption, leading to more efficient decryption. Consequently, this methodology allows for accommodating more transactions within a given time slot.</p>\n<h3><a name=\"advantages-6\" class=\"anchor\" href=\"https://ethresear.ch#advantages-6\"></a><strong>Advantages</strong></h3>\n<ul>\n<li><strong>User’s Computation Efficiency</strong>: Scroll’s MDE offers a more efficient computational process for users compared to Radius’s PVDE, as it eliminates the need for generating zero-knowledge proofs. This enhancement is due to a shift in responsibility for creating Timelock Puzzle (TLP) parameters. In PVDE’s architecture, users were tasked with generating TLPs, which necessitated proving the legitimacy of their puzzles to prevent potential attacks that could influence the sequencer’s computation load. However, in MDE, the burden of TLP generation does not fall on individual users, substantially reducing the risk of such attacks. This change eliminates the need for users to prove the proper encryption of their transactions using their keys, streamlining the user experience without compromising security.</li>\n<li><strong>Sequencer’s Computation Efficiency</strong>: The requirement to solve only one timelock puzzle significantly reduces the computational burden on sequencers. This efficiency boosts the capacity to process a higher volume of transactions in a predetermined time slot.</li>\n</ul>\n<h3><a name=\"limitations-7\" class=\"anchor\" href=\"https://ethresear.ch#limitations-7\"></a><strong>Limitations</strong></h3>\n<ul>\n<li><strong>1-out-of-N Trust Model</strong>: The system necessitates a descent to a 1-out-of-N trust model, where trust is decentralized as much as possible to the KeyGen Committee. This approach introduces a reliance on the committee’s integrity for key pair generation.</li>\n<li><strong>Periodic Public Key Pair Storage</strong>: Each period requires the public storage (e.g., on Ethereum) of committee members’ partial key pairs. Assuming there are 20 sequencers, and without considering range proofs, this could result in a fee of approximately 0.768 ETH per block, illustrating a significant cost implication.</li>\n</ul>\n<p>In summary, Scroll’s MDE marks a pivotal evolution in encrypted mempool technology, offering enhanced computational efficiencies for both users and sequencers. However, it also introduces specific trade-offs, including the need for a trust model reliant on a committee and potential cost associated with the system’s operation. These factors present a complex yet promising landscape for MEV mitigation strategies within blockchain networks.</p>\n<h1><a name=\"h-3-radius-new-skde-8\" class=\"anchor\" href=\"https://ethresear.ch#h-3-radius-new-skde-8\"></a>3. Radius’ new SKDE</h1>\n<hr>\n<p>Our Single Key Delay Encryption (SKDE) design represents a step forward from Scroll’s Multiparty Delay Encryption (MDE), inheriting its two primary advantages while addressing its limitations. Here’s a breakdown of how the SKDE design enhances the framework:</p>\n<h3><a name=\"advantages-inherited-from-scrolls-mde-9\" class=\"anchor\" href=\"https://ethresear.ch#advantages-inherited-from-scrolls-mde-9\"></a><strong>Advantages Inherited from Scroll’s MDE</strong></h3>\n<ul>\n<li><strong>User’s and Sequencer’s Computational Efficiency</strong>: Like Scroll’s MDE, our SKDE design maintains high computational efficiency for both users and sequencers. It simplifies the decryption process and eliminates the need for users to generate zero-knowledge proofs (zkp) for their transactions.</li>\n</ul>\n<h3><a name=\"solutions-to-limitations-10\" class=\"anchor\" href=\"https://ethresear.ch#solutions-to-limitations-10\"></a><strong>Solutions to Limitations</strong></h3>\n<ol>\n<li><strong>Reduced Public Storage Cost</strong>: Our design introduces an entity responsible for performing aggregate operations, significantly reducing the cost associated with storing partial key pairs in a public repository. We have reduced the gas fees from the previously estimated 0.768 ETH to merely 0.008 ETH for storing essential data on a platform like Ethereum.\n<ul>\n<li>The legitimacy of <strong>the aggregator’s operations is proven through Zero-Knowledge Proofs (ZKP)</strong>, where <strong>only compressed information (hash) of the partial keys is made public</strong>. Each committee member signs the hash to validate that it corresponds to the key they generated. This method ensures privacy and integrity without revealing the actual keys, while also drastically cutting down on the blockchain storage costs associated with our system.</li>\n</ul>\n</li>\n<li><strong>Minimized Trust Assumptions</strong>: We have designed the system to allow individuals who do not trust the committee to participate in the key generation process directly. This inclusivity enhances the trustlessness of the system by decentralizing key generation further.</li>\n</ol>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/5/5e16de604cc5e7a8700ac56c58f91015c516ab07.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/5e16de604cc5e7a8700ac56c58f91015c516ab07\" title=\"Trust Assumptions\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/5/5e16de604cc5e7a8700ac56c58f91015c516ab07_2_690x298.jpeg\" alt=\"Trust Assumptions\" data-base62-sha1=\"dqlSrojtCzSO79Y5P6MgTNWFBqL\" width=\"690\" height=\"298\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/5/5e16de604cc5e7a8700ac56c58f91015c516ab07_2_690x298.jpeg, https://ethresear.ch/uploads/default/optimized/2X/5/5e16de604cc5e7a8700ac56c58f91015c516ab07_2_1035x447.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/5/5e16de604cc5e7a8700ac56c58f91015c516ab07_2_1380x596.jpeg 2x\" data-dominant-color=\"F3F3F3\"></a></div><p></p>\n<p><em></em></p>\n<h3><a name=\"design-implications-11\" class=\"anchor\" href=\"https://ethresear.ch#design-implications-11\"></a><strong>Design Implications</strong></h3>\n<p>With the SKDE framework, the partial keys are no longer fully public, diverging from Scroll’s MDE where all entities could directly verify partial keys and aggregate only the valid ones. As a result:</p>\n<ul>\n<li><strong>Implemented Robust Crypto-Economic Model for Partial Key Verification</strong>: We have designed and implemented a robust economic model for the verification of partial keys. This model is crafted to incentivize honest participation and deter malicious activities effectively. It strikes a balance between ensuring privacy and maintaining the system’s integrity through trust, showcasing our proactive approach in fortifying the blockchain ecosystem against vulnerabilities.</li>\n</ul>\n<p>In summary, our SKDE design not only adopts the computational efficiencies found in Scroll’s MDE but also addresses its shortcomings by reducing public storage costs and minimizing trust assumptions. By incorporating an aggregation entity validated through ZKP and allowing direct user participation in key generation, we aim to foster a more secure, efficient, and trustless environment for MEV mitigation.</p>\n<h2><a name=\"h-31-main-idea-of-our-skde-12\" class=\"anchor\" href=\"https://ethresear.ch#h-31-main-idea-of-our-skde-12\"></a>3.1. Main idea of our SKDE</h2>\n<hr>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/9/98e2c52a286cee5920ca741ed7565fee5e272d35.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/98e2c52a286cee5920ca741ed7565fee5e272d35\" title=\"skde\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/9/98e2c52a286cee5920ca741ed7565fee5e272d35_2_690x383.jpeg\" alt=\"skde\" data-base62-sha1=\"lOuqyDIF15QlI5u3W5jHlfI0ce1\" width=\"690\" height=\"383\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/9/98e2c52a286cee5920ca741ed7565fee5e272d35_2_690x383.jpeg, https://ethresear.ch/uploads/default/optimized/2X/9/98e2c52a286cee5920ca741ed7565fee5e272d35_2_1035x574.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/9/98e2c52a286cee5920ca741ed7565fee5e272d35_2_1380x766.jpeg 2x\" data-dominant-color=\"E3E8EE\"></a></div><p></p>\n<p><em></em></p>\n<p>The Single Key Delay Encryption (SKDE) process involves several entities: the Key Generation Committee, Key Aggregator, Sequencer, and Users. Each plays a crucial role in the secure and efficient handling of encrypted transactions.</p>\n<blockquote>\n<p><strong>Key Generation Committee</strong>: Responsible for initiating the encryption process by generating partial keys.</p>\n<p><strong><code>[Phase 1]</code></strong></p>\n<ol>\n<li>Generate a partial key pair <span class=\"math\">(k_{enc}, k_{ext})</span>.</li>\n<li>Produce a hash <span class=\"math\">h</span> to compress <span class=\"math\">(k_{enc}, k_{ext})</span>.</li>\n<li>Sign the compressed <span class=\"math\">h</span>, creating a signature <span class=\"math\">\\sigma</span>.</li>\n<li>Create a proof <span class=\"math\">\\pi</span> that validates the partial key’s integrity.</li>\n<li>Send the generated information <span class=\"math\">(k_{enc}, k_{ext}, h, \\sigma, \\pi)</span> to the Key Aggregator and the Committee.</li>\n</ol>\n<p><strong><code>[Phase 2]</code></strong> (After “Key aggregation” process)</p>\n<ol>\n<li>Validate the integrity of all partial keys, as the initial steps performed by the Key Aggregator.</li>\n<li>If any discrepancies are found or if the aggregation process does not meet specified conditions—such as <span class=\"math\">\\pi_{agg}</span> verification, inclusion of invalid key pairs, or omission of valid key pairs—claims are raised to the Key Aggregator.</li>\n</ol>\n<p><strong>Key Aggregator</strong>: Serves as the central figure in consolidating partial keys and making them known to Sequencers.</p>\n<ol>\n<li>Compute <span class=\"math\">h</span> using the partial keys and verify the signatures <span class=\"math\">\\sigma</span>.</li>\n<li>Verify the validity of all partial keys using <span class=\"math\">\\pi</span>.</li>\n<li>Exclude any invalid partial keys and aggregate the rest.</li>\n<li>Generate a proof of aggregation <span class=\"math\">\\pi_{agg}</span>.</li>\n<li>Announce the Aggregated key <span class=\"math\">(k_{enc}^{agg}, k_{ext}^{agg})</span> to Sequencers.</li>\n</ol>\n<p><strong>Sequencer</strong>: Responsible for finalizing the transaction sequence and decryption process.</p>\n<ol>\n<li>Verify <span class=\"math\">\\pi_{agg}</span>.</li>\n<li>Upon User request, provide the Aggregated encryption key <span class=\"math\">k_{enc}^{agg}</span>.</li>\n<li>Upon receiving an Encrypted Transaction, commit its order and using the Aggregated extraction key <span class=\"math\">k_{ext}^{agg}</span> to compute the decryption key <span class=\"math\">k_{dec}^{agg}</span>.</li>\n<li>Decrypting the transactions.</li>\n</ol>\n<p><strong>User</strong>: Engages with the Sequencer to encrypt transactions using the Aggregated encryption key <span class=\"math\">k_{enc}^{agg}</span>.</p>\n<ol>\n<li>Query the Sequencer for the Encryption key.</li>\n<li>Encrypt their transaction using the received Encryption key <span class=\"math\">k_{enc}^{agg}</span> and send <span class=\"math\">CT</span> to the Sequencer.</li>\n</ol>\n</blockquote>\n<h2><a name=\"h-32-addressed-security-challenges-13\" class=\"anchor\" href=\"https://ethresear.ch#h-32-addressed-security-challenges-13\"></a>3.2. Addressed security challenges</h2>\n<hr>\n<p>Our Single Key Delay Encryption (SKDE) design effectively navigates through several security challenges inherent in encrypted mempool systems. By addressing the following attack scenarios, SKDE enhances the integrity and confidentiality of transactions within the blockchain. Here’s how the SKDE framework addresses these challenges:</p>\n<blockquote>\n<p><strong>1. Key Contamination Attack</strong> : A potential attack where an invalid partial key pair <span class=\"math\">(k_{enc}, k_{ext})</span> contaminates the aggregated key, rendering all encrypted transactions undecryptable and compromising the Sequencing Layer.</p>\n<ol>\n<li><strong>Partial Key Verifiability</strong>: The SKDE design ensures that each partial key pair’s validity can be verified, preventing the Key Aggregator from generating a contaminated aggregated key. We achieve this through:\n<ul>\n<li>Creating proofs <span class=\"math\">\\pi</span> for key validity, utilizing Sigma protocols for verifying the relationship of partial key pairs and range proofs to ensure aggregated keys are the unique combination of partial keys.</li>\n</ul>\n</li>\n<li><strong>Key Generation Non-repudiation</strong>: Ensures that Key Setup Committee members cannot deny having generated an invalid partial key. This is done by signing the hash of the partial key, allowing the Aggregator to verify this commitment and depend on crypto-economics to encourage claims against discrepancies.</li>\n<li><strong>Aggregator Responsibility</strong>: The Aggregator is held accountable for excluding invalid partial keys, backed by a penalty protocol that enforces economic consequences for contaminating the block with invalid keys.</li>\n</ol>\n<hr>\n<p><strong>2. Decryption Key Leakage Attack</strong> : A scenario where the Aggregator prematurely discloses an aggregated key <span class=\"math\">(k_{enc}^{agg}, k_{ext}^{agg})</span>, potentially exposing transaction privacy and enabling MEV attacks.</p>\n<ol>\n<li>\n<p><strong>Aggregate Computation Verifiability</strong>: By generating zero-knowledge proofs that validate the legitimacy of aggregate operations, SKDE ensures that the public can verify the correctness of the published aggregated key.</p>\n</li>\n<li>\n<p><strong>Commitment Consistency Verifiability</strong>: The framework relies on cryptographic economics to verify that the commitments made public by the Aggregator match the partial keys used in the aggregation process.</p>\n</li>\n<li>\n<p><strong>Timelock Privacy</strong>: SKDE’s design guarantees that, given a minimum of one honest Sequencer, no collusion among the others can compromise the privacy of a user’s transaction until the designated time has passed.</p>\n<ul>\n<li>Additionally, SKDE allows users to partake in the key generation process, offering a fundamental property where, regardless of any collusion among committee members, the confidentiality of the decryption key is preserved. This inclusion ensures that no single group can undermine the system’s security, significantly reinforcing the trustworthiness of the key generation and aggregation phases.</li>\n</ul>\n</li>\n</ol>\n<hr>\n<p><strong>3. Mismatched Key Replay Attack</strong> : A scenario where the encryption key delivered to the user does not match the actual aggregated key, allowing Sequencers or intermediaries to decrypt and re-encrypt transactions under a guise of normalcy.</p>\n<ol>\n<li>\n<p><strong>Mitigation</strong>: Users can verify the aggregated encryption key published in the block against the key received. If discrepancies are found, users have the ability to claim against the Sequencer they communicated with, enhancing transaction privacy and deterring potential MEV attacks.</p>\n<ul>\n<li>Improving proving performance is an ongoing research area that could enable users to verify the validity of keys upon receipt instantaneously, further mitigating the risk of MITM attacks. Our commitment to advancing SKDE’s proving performance promises to strengthen the security framework against sophisticated attack vectors, ensuring a more secure and trustless environment for all network participants.</li>\n</ul>\n</li>\n</ol>\n</blockquote>\n<h2><a name=\"h-33-the-lifecycle-of-a-key-14\" class=\"anchor\" href=\"https://ethresear.ch#h-33-the-lifecycle-of-a-key-14\"></a>3.3. The lifecycle of a key</h2>\n<hr>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/2/273d8fd884dc7350b31e4c7bcebee14fe2013df1.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/273d8fd884dc7350b31e4c7bcebee14fe2013df1\" title=\"key_lifecycle\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/2/273d8fd884dc7350b31e4c7bcebee14fe2013df1_2_690x119.jpeg\" alt=\"key_lifecycle\" data-base62-sha1=\"5B8w3KxBxyVCvwopof7CyvBM3D3\" width=\"690\" height=\"119\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/2/273d8fd884dc7350b31e4c7bcebee14fe2013df1_2_690x119.jpeg, https://ethresear.ch/uploads/default/optimized/2X/2/273d8fd884dc7350b31e4c7bcebee14fe2013df1_2_1035x178.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/2/273d8fd884dc7350b31e4c7bcebee14fe2013df1_2_1380x238.jpeg 2x\" data-dominant-color=\"F2F0F0\"></a></div><br>\nIn the lifecycle of a key within systems like Single Key Delay Encryption (SKDE), we can delineate three distinct phases.<p></p>\n<blockquote>\n<p><strong>Preparation Phase</strong>: During this initial stage, the key is generated and undergoes several crucial steps to ensure its readiness for encryption purposes. Key activities include:</p>\n<ul>\n<li>Generation of the partial keys, marking its inception.</li>\n<li>To share the partial keys among Sequencers.</li>\n<li>Aggregation of partial keys to form a robust, unified encryption key.</li>\n<li>Dissemination of the aggregated key for upcoming encryption tasks.</li>\n</ul>\n<hr>\n<p><strong>Accessibility Phase</strong>: This phase is characterized by the system’s readiness to accept and process transactions using the previously prepared key. It involves:</p>\n<ul>\n<li>Collection of encrypted transactions from users.</li>\n<li>To Solve the key, ensuring readiness for decryption.</li>\n<li>Execution of fair sequencing and transmission of order commitments.</li>\n<li>Production of proofs for the aggregation process, affirming the validity of the encryption key.</li>\n</ul>\n<hr>\n<p><strong>Lockdown Phase</strong>: The final stage locks the usage of aggregated key and focuses on transitioning from encryption to decryption. This phase includes:</p>\n<ul>\n<li>Decryption of the amassed transactions, rendering them into their original, intelligible state.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/4/47c4c39b11428643f12782db9a05646489fe40ec.png\" data-download-href=\"https://ethresear.ch/uploads/default/47c4c39b11428643f12782db9a05646489fe40ec\" title=\"key_cycle_period\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/4/47c4c39b11428643f12782db9a05646489fe40ec_2_690x286.png\" alt=\"key_cycle_period\" data-base62-sha1=\"aeTsRpX8Yc0NfS8RHVcU3w0DlcE\" width=\"690\" height=\"286\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/4/47c4c39b11428643f12782db9a05646489fe40ec_2_690x286.png, https://ethresear.ch/uploads/default/optimized/2X/4/47c4c39b11428643f12782db9a05646489fe40ec_2_1035x429.png 1.5x, https://ethresear.ch/uploads/default/original/2X/4/47c4c39b11428643f12782db9a05646489fe40ec.png 2x\" data-dominant-color=\"F3F2F3\"></a></div></li>\n</ul>\n</blockquote>\n<p>So far, we have discussed the birth and eventual retirement of a single key within our system. Now, we turn our attention to the cyclical nature of key generation and how it perpetuates throughout the system’s operation. Given that all entities utilize a single key, there is a need for a defined periodic cycle to manage its use. We have partitioned the key’s lifecycle into three distinct phases to delineate when users can access and utilize the key. Also, the length of the Lockdown phase directly influences the volume of transactions that can be decrypted and ordered within the given cycle. Therefore, we have established our key usage cycle around the Lockdown phase, setting a rhythm for the generation, active use, and phasing out of keys.</p>\n<p>This systematic cycle ensures that key generation and retirement are synchronized with the network’s operational needs, facilitating a seamless flow of transaction processing and maintaining the security and efficiency of our encrypted mempool system.</p>\n<h1><a name=\"h-4-experimental-results-15\" class=\"anchor\" href=\"https://ethresear.ch#h-4-experimental-results-15\"></a>4. Experimental results</h1>\n<hr>\n<p>In our journey to refine encrypted mempool technology and bolster MEV mitigation efforts, Radius has achieved substantial progress by rolling out the Curie Testnet and Portico Testnet, both powered by the original Practical Verifiable Delay Encryption (PVDE). Additionally, we have implemented Scroll’s Multiparty Delay Encryption (MDE) and our Single Key Delay Encryption (SKDE). In this chapter, we present the experimental analysis that offers a detailed comparison of the computational costs entailed by the PVDE, MDE, and SKDE frameworks for each participating entity.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/6/65724d17b398784454f5c8d1a8b0dbe0e74e80af.png\" data-download-href=\"https://ethresear.ch/uploads/default/65724d17b398784454f5c8d1a8b0dbe0e74e80af\" title=\"performance_table\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/6/65724d17b398784454f5c8d1a8b0dbe0e74e80af_2_690x254.png\" alt=\"performance_table\" data-base62-sha1=\"etr72CorZ0iK2xjWJeMp5jpdhIX\" width=\"690\" height=\"254\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/6/65724d17b398784454f5c8d1a8b0dbe0e74e80af_2_690x254.png, https://ethresear.ch/uploads/default/optimized/2X/6/65724d17b398784454f5c8d1a8b0dbe0e74e80af_2_1035x381.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/6/65724d17b398784454f5c8d1a8b0dbe0e74e80af_2_1380x508.png 2x\" data-dominant-color=\"EBECED\"></a></div><p></p>\n<p><em></em><br>\nThe experiments are conducted on an Apple M3 Pro with 18GB memory. The Zero-Knowledge Proofs (ZKP) utilized in PVDE and SKDE are both constructed using the Halo2 proving system. In this context, <span class=\"math\">n</span> represents the number of users and <span class=\"math\">c</span> denotes the number of Key Generation Committee members. <span class=\"math\">T_s</span> denotes a short-cycle timelock parameter for PVDE, and <span class=\"math\">T_l</span> is a long-cycle timelock parameter for MDE and SKDE.</p>\n<blockquote>\n<p><span class=\"math\">T_s</span> in PVDE : This parameter is designed to accommodate the generation of a time lock puzzle for every individual transaction within PVDE. It is set to ensure that there is sufficient time to send an order commit to the user before the conclusion of the time lock parameter. The shorter cycle is necessitated by the per-transaction time lock puzzle approach, requiring a quicker turnaround to maintain transaction flow and commit timing.</p>\n<p><span class=\"math\">T_l</span> in MDE and SKDE : Unlike PVDE, MDE and SKDE operate under a model where only one time lock puzzle needs to be solved within a defined cycle. This allows for a longer time lock parameter <span class=\"math\">T_l</span>, providing the flexibility to accumulate transactions over a longer period. The extended time lock cycle facilitates not only a First-Come-First-Served (FCFS) sequencing but also enables the aggregation of transactions to apply predetermined rules (such as fee auctions) for ordering and committing transactions. This longer duration ensures there is ample time to order transactions according to these rules, enhancing the system’s capacity to handle transactions efficiently and securely.</p>\n</blockquote>\n<p>Through this analysis, we have determined that SKDE stands out in terms of computational efficiency for both users and sequencers. Moreover, it has reduced the gas fee significantly, which is required for storing partial keys each block period. It is also reasonable to project that if blob storage were to be factored into the equation, both MDE and SKDE would likely see a corresponding decrease in fees.</p>\n<p>The introduction of an aggregator entity makes an advancement in our framework, adding a layer of complexity and efficiency. The security of this aggregator is ensured through the application of Zero-Knowledge Proofs (ZKP) and crypto-economic principles, forming a robust design that balances privacy, efficiency, and trust.</p>\n<p>Regarding the <strong>ZKP</strong> entry in the table, it indicates that the aggregator’s computational effort is denoted as <strong>ZKP</strong> + <span class=\"math\">(78c)ms</span>. The ZKP proving computation is linear to the committee size <span class=\"math\">c</span>. Current ongoing research efforts are focused on optimizing the <strong>ZKP</strong> computation of the aggregator, aiming to minimize its operational overhead while maintaining the integrity and efficacy of the system.</p>\n<h1><a name=\"h-5-conclusion-16\" class=\"anchor\" href=\"https://ethresear.ch#h-5-conclusion-16\"></a>5. Conclusion</h1>\n<hr>\n<p>In summarizing the key points from our exploration of PVDE, MDE, and SKDE, we draw conclusions across several critical dimensions: security, user computation, sequencer computation, aggregator computation, and public storage gas fee.</p>\n<blockquote>\n<p><strong>Security</strong>: Our SKDE design approaches ‘trustlessness’, being architected to minimize trust assumptions wherever possible. We have identified potential attack vectors, defined security requirements formally, and proved mathematically to validate these security claims. This rigorous approach ensures a high level of confidence in the robustness of our system against MEV vulnerabilities.</p>\n<p><strong>User Computation</strong>: Unlike previous PVDE, SKDE does not require users to generate individual timelock puzzles, eliminating the risk of DDoS attacks stemming from malformed puzzles. This change effectively removes the need for user-generated proofs, resulting in a revolutionary decrease in user-side computational requirements.</p>\n<p><strong>Sequencer Computation</strong>: By only needing to solve a single timelock puzzle for a defined cycle, SKDE significantly lightens the computational load for sequencers, compared to the multiple puzzle resolutions required in PVDE.</p>\n<p><strong>Aggregator Computation</strong>: The aggregator is a novel entity introduced within the SKDE framework. It was developed to address the significant gas fees associated with MDE’s public key storage. The aggregator performs computational tasks including the creation of ZKPs, contributing to the overall system efficiency while ensuring the integrity of the aggregation process.</p>\n<p><strong>Public Storage Gas Fee</strong>: SKDE achieves a remarkable reduction in the gas fees for public storage, mainly due to the reduced frequency and size of key information that needs to be stored on-chain.</p>\n</blockquote>\n<p>Acknowledging PVDE’s strength in offering a ‘fully trustless’ environment, Radius plans to provide both PVDE and SKDE as viable options, catering to diverse needs within the blockchain ecosystem. While SKDE offers reduced computational demands and gas fees, PVDE remains a powerful choice for those prioritizing maximal trustlessness. Both systems signify Radius’ commitment to offering tailored solutions that enhance security, efficiency, and composability in the rollup landscape.</p>\n<h1><a name=\"references-17\" class=\"anchor\" href=\"https://ethresear.ch#references-17\"></a>References</h1>\n<hr>\n<p>[KAJ+23] Khajehpour, Amirhossein, et al. “<a href=\"https://eprint.iacr.org/2023/1612\" rel=\"noopener nofollow ugc\">Mitigating MEV via Multiparty Delay Encryption.</a>” <em>Cryptology ePrint Archive</em> (2023).<br>\n[PVDE] <a href=\"https://ethresear.ch/t/mev-resistant-zk-rollups-with-practical-vde-pvde/12677\" class=\"inline-onebox\">MEV-resistant ZK-Rollups with Practical VDE (PVDE)</a><br>\n[Curie] <a href=\"https://docs.theradius.xyz/testnet/curie-testnet\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Curie Testnet | Radius</a><br>\n[Portico] <a href=\"https://docs.theradius.xyz/testnet/portico-testnet\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Portico Testnet | Radius</a></p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/skde-enhancing-rollup-composability-with-trustless-sequencing/19185\">Read full topic</a></p>","link":"https://ethresear.ch/t/skde-enhancing-rollup-composability-with-trustless-sequencing/19185","pubDate":"Tue, 02 Apr 2024 03:16:14 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19185"},"source":{"@url":"https://ethresear.ch/t/skde-enhancing-rollup-composability-with-trustless-sequencing/19185.rss","#text":"SKDE: Enhancing Rollup Composability with Trustless Sequencing"},"filter":true},{"title":"Radius SKDE: Enhancing Rollup Composability with Trustless Sequencing","dc:creator":"wooju","category":"Uncategorized","description":"<p>(topic deleted by author)</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/radius-skde-enhancing-rollup-composability-with-trustless-sequencing/19184\">Read full topic</a></p>","link":"https://ethresear.ch/t/radius-skde-enhancing-rollup-composability-with-trustless-sequencing/19184","pubDate":"Tue, 02 Apr 2024 02:59:59 +0000","discourse:topicPinned":"No","discourse:topicClosed":"Yes","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19184"},"source":{"@url":"https://ethresear.ch/t/radius-skde-enhancing-rollup-composability-with-trustless-sequencing/19184.rss","#text":"Radius SKDE: Enhancing Rollup Composability with Trustless Sequencing"},"filter":true},{"title":"DA Honey Trap: When a DA Attack Becomes Significantly Easy","dc:creator":"leohio","category":"Optimisitic Rollup","description":"<h3><a name=\"tldr-1\" class=\"anchor\" href=\"https://ethresear.ch#tldr-1\"></a>TL;DR</h3>\n<p>Actual simulations were performed on the cost of attacks against the DA Layer and the probability of being attackable.</p>\n<ol>\n<li>The cost of an attack on the DA Layer is totally unlike that of a PoS double vote.</li>\n<li>The probability of attackability increases rapidly when the degree of redundancy of data holding nodes falls below a certain number like 200.</li>\n</ol>\n<h2><a name=\"withhold-attack-2\" class=\"anchor\" href=\"https://ethresear.ch#withhold-attack-2\"></a>Withhold Attack:</h2>\n<p>The block withholding attack is always the biggest issue in Layer 2. If finality is confirmed in a state where the block data or part of it is withheld, in the case of finality using zkp, everyone’s assets will be frozen. And in a security model like ORU that goes through a fraud proof period, the attacker can extract all the assets.</p>\n<p>As a current countermeasure, DAS is basically used to prevent withhold attacks in the DA Layer. By having randomly selected nodes hold finely chunked data in a redundant manner and having validators verify it with KZG commitments, etc., it becomes extremely difficult to lose a block. However, while it is cryptographically possible to prove data possession, <a href=\"https://arxiv.org/abs/1809.09044\" rel=\"noopener nofollow ugc\">it is not possible to prove that withholding was not done</a>.</p>\n<h2><a name=\"bpvalidator-collusion-economics-3\" class=\"anchor\" href=\"https://ethresear.ch#bpvalidator-collusion-economics-3\"></a>BP/Validator Collusion Economics:</h2>\n<p>The economics of block producers and validators for blocks containing directly verified data differs completely before and after the separation of the DA Layer. For example, in the previous Ethereum or Bitcoin, the probability of blocks continuing after a block containing malicious transactions was extremely low. This is because subsequent BPs would surely confirm that the content is malicious by verifying it, and fork. In other words, to be malicious, you need to deceive future validators regarding the transactions. However, for blocks with a separated DA area that is not directly verified in the future, there is no need to deceive future validators in order to pass blocks with withheld data.</p>\n<p>Incentives for Validator Collusion<br>\nPros:</p>\n<ol>\n<li>Can steal all assets of ORU</li>\n<li>Can receive block rewards</li>\n</ol>\n<p>Cons:</p>\n<ol>\n<li>Reputation gets bad</li>\n</ol>\n<p>In other words, unlike the case of Ethereum PoS, which uses the term “slashing” in the same way, attackers can obtain rewards while also obtaining attack gains, and the cost is not defined in the protocol. Basically, the economic security of slashing cannot be applied to the incentives for data retention and attack, and it will depend on non-incentivized honesty assumptions or community reputation.</p>\n<p>Even so, if this attack can be established with 1/N honest security, it can be said to be sufficiently prevented. Is the security assumption of the DA Layer 1/N?</p>\n<h2><a name=\"h-1n-security-assumption-of-dal-4\" class=\"anchor\" href=\"https://ethresear.ch#h-1n-security-assumption-of-dal-4\"></a>1/N Security Assumption of DAL</h2>\n<p>In DAS, since data is chunked in 2D and distributed redundantly, colluding to withhold seems to be very troublesome. However, considering that you only need to be able to withhold even a part of the block data to perform a DA attack, the assumption of DA attack in DAL including DAS can and should be simplified and calculated as follows:</p>\n<p>Let’s assume a protocol where a single chunk of data like a block is recovered from m redundant nodes, and n such data chunks are posted over a certain period of time, say a year. (Note that this m is neither the total number of nodes holding chunked data in DAS, nor the number of light clients in Celestia!)</p>\n<p>Let’s say that a ratio s (0&lt;s&lt;1) of m are fully aligned nodes, i.e., always online and never malicious or fooled. For DA attacks, unlike explicit double voting in PoS or DPoS, you can contribute to the attack just by going offline for a few days. Therefore, it is reasonable to count nodes that are not honest and online as attackers, rather than considering what percentage of attackers there are.</p>\n<p>Let’s say the threshold for block production is t (0&lt;t&lt;1), where everyone’s DAS proof and signatures need to be collected. (If all are required, a liveness attack that simply stops the blockchain becomes possible. You need a threshold.)</p>\n<p>What do the attackers need to do? The attackers succeed in the attack only if they manage to assign all m<em>t of the nodes selected for block production from m to the m</em>(1-s) group. It is a kind of 1/N security, but if they succeed even once in n repetitions of this game, the entire attack succeeds.</p>\n<p><span class=\"math\">\nattackrisk_1: 1 - (1- {}_{m(1-s)}\\mathrm{C}_{mt} / {}_m\\mathrm{C}_{mt} )^n\n</span></p>\n<p>This can be expressed with <a href=\"https://en.wikipedia.org/wiki/Hypergeometric_distribution\" rel=\"noopener nofollow ugc\">the probability distribution function of hypergeometric distribution f(k,N,K,n)</a> as well.</p>\n<p><span class=\"math\">\nattackrisk_1: 1 - (1- f(mt,m,m(1-s),mt) )^n \n</span></p>\n<p>Let’s actually plug in some typical values. Let’s assume we want to guarantee security for 3 years. s is conservatively about 1/5, or optimistically 1/3. Let’s use 1/5 here. Assuming that 2/3 of the nodes are fully aligned (never go offline or absolutely never join an attack) for a long period of time is extremely scary. With a simple block interval of 15 seconds, n is 6307200. Let’s set the threshold t to 1/2.</p>\n<p>For example, if m is 100,<br>\n<span class=\"math\">\nattackrisk_1: 1 - (1- {}_{80}\\mathrm{C}_{50} / {}_{100}\\mathrm{C}_{50} )^{6307200} = 0.42\n</span></p>\n<p>You can use this python code to calculate it or play with numbers.</p>\n<pre><code class=\"lang-auto\">import math\nm=100; s=0.2; t=0.5; n = 6307200\nm1s = int(m*(1-s)); mt = int(m*t)\nattackrisk = 1 - (1 - float(math.comb(m1s, mt))/float(math.comb(m, mt)))**n\nprint(attackrisk)\n</code></pre>\n<p><strong>There is a 42% chance of being attacked once within 3 years.</strong> This is not a double spend attack, but in the case of ORU, all funds are completely drained. I just put the normal numbers which you can come up with easily.</p>\n<p>And when m=50, it is almost 100% likely to be attacked.</p>\n<p>When m=200, it can be kept to a practically safe probability of around 4.2*10^-8, <em><strong>showing how crucial it is to increase the number of m even if it’s not a significant difference</strong></em>. With this model, it can be shown that security improvement by chunking is effective, and also that it is barely above the passing line.</p>\n<p>In the case of chunking data like DAS, m increases but n also increases greatly, and security improvement is only possible if the increase in m cancels out the increase in n. For any blockchain system, including Ethereum, there has been no successful case of controlling the number of nodes as desired in the long term. If we expect an increase in m by reducing the load, we need to try various models to look at the relationship between data size and available nodes (= the relationship between c and c’ described later).</p>\n<p>Here is a favorably interpreted model for the case where data size is reduced by chunking:</p>\n<p><span class=\"math\">\nattackrisk_2: 1 - (1- {}_{m(1-s)c'}\\mathrm{C}_{mtc'} / {}_{mc'}\\mathrm{C}_{mtc'} )^{nc}\n</span><br>\nwhere c is the number of divisions by chunking, and c’ is the term representing the increase in m due to that.</p>\n<p><span class=\"math\">attackrisk_2  &lt; attackrisk_1</span> when</p>\n<p><span class=\"math\">\n(1- {}_{m(1-s)c'}\\mathrm{C}_{mtc'} / {}_{mc'}\\mathrm{C}_{mtc'} )^{c} &gt; 1- {}_{m(1-s)}\\mathrm{C}_{mt} / {}_m\\mathrm{C}_{mt}\n</span></p>\n<p>While the notation itself is simple, it seems very difficult to back-calculate the proper relationship between c and c’ from this inequality. (We can express <span class=\"math\">{}_{m(1-s)}\\mathrm{C}_{mt} / {}_m\\mathrm{C}_{mt}</span> as the cumulative distribution function of the hypergeometric distribution and approximating it with the Poisson distribution using the fact that n  is large enough. But it  introduces Γ function, ruining all the appetite for wonder.)</p>\n<p>Since it is difficult to derive a general solution, let’s plug in various values based on m=200, s=0.2, t=0.5. This program plots the relationship between c and c’ when attackrisk_1 and attackrisk_2 are equal under the above conditions.</p>\n<blockquote>\n<p>c: 2   c’: 1.0<br>\nc: 5   c’: 1.02<br>\nc: 10   c’: 1.05<br>\nc: 50   c’: 1.15<br>\nc: 200   c’: 1.16<br>\nc: 500   c’: 1.17<br>\nc: 600   c’: 1.17<br>\nc: 700   c’: 1.17</p>\n</blockquote>\n<p>The code is <a class=\"attachment\" href=\"https://ethresear.ch/uploads/short-url/2Tmx4CYVtvrT59vO1LO6gN1NOp9.txt\">here</a> (547 Bytes)</p>\n<p>Even if one data is divided into c=700, c’ is only about 1.2, meaning m increases by only about 20%, which is of order log(n). Assuming that the total number of nodes is divided by the number of groups c since the data is divided by c, the total number of nodes turns out to be <span class=\"math\">t_o = c log(c) * m</span>. (Otherwise, there is no point in chunking if security only decreases.)</p>\n<p>First, it seems reasonable to try <a href=\"https://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"noopener nofollow ugc\">a lognormal distribution</a> for the relationship between data size <span class=\"math\">d = d' / c</span> (d’ is the origial data size) and probability of node, which relates to <span class=\"math\">t_o</span>. This is the distribution that shows the relationship between the amount of wealth and the number of people for that wealth in the world, so it makes sense to apply this distribution if we consider storage as wealth.  The probability density function of the lognormal distribution looks close to <a href=\"https://www.wolframalpha.com/input/?i=Solve%5B%28logx%29%2Fx-y%3D%3D0%2Cx%5D\" rel=\"noopener nofollow ugc\">the inverse function of log(x)/x</a>, so we can expect the number of nodes needed to maintain security after chunking almost follows this distribution. Conversely, if we assume this distribution, it does not seem security has improved significantly.</p>\n<p>And, please note that this expectation can be said on the favorable model.</p>\n<h1><a name=\"inference-and-conclusion-5\" class=\"anchor\" href=\"https://ethresear.ch#inference-and-conclusion-5\"></a>Inference and Conclusion</h1>\n<p>DA Layers are fine.<br>\nA DA Layer with a small m is spicy if it carries an ORU where DA attacks can be incentivized. We should also be careful about cases where trying to improve security ends up increasing n and becoming more dangerous. Even if fancy cryptography is used to create a DA, we still need to know what m and n are, and it solves nothing if they are bad.<br>\nRegarding chunking, assuming that the increase in nodes follows a lognormal distribution, it may be admitted that it is useful, but if m falls below a number like 200 and is small, any effort is futile.</p>\n<p>By the way, what would be a good term to call this security assumption of “in order to attack, you have to happen to be assigned all the bad actors to all the seats” instead of calling it a 1/N honesty assumption?</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/da-honey-trap-when-a-da-attack-becomes-significantly-easy/19183\">Read full topic</a></p>","link":"https://ethresear.ch/t/da-honey-trap-when-a-da-attack-becomes-significantly-easy/19183","pubDate":"Tue, 02 Apr 2024 00:59:54 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19183"},"source":{"@url":"https://ethresear.ch/t/da-honey-trap-when-a-da-attack-becomes-significantly-easy/19183.rss","#text":"DA Honey Trap: When a DA Attack Becomes Significantly Easy"},"filter":false},{"title":"On-chain Distributed RNG using ZK?","dc:creator":"kladkogex","category":"Multiparty Computation","description":"<p>I am looking to create a simple <strong>smart contract  based algorithm</strong>  that provides <strong>common random number generation</strong>, provided that <span class=\"math\">t</span> out of <span class=\"math\">N</span> participants are honest, where <span class=\"math\">t * 2 &gt; N</span></p>\n<p>The algorithm has the following properties</p>\n<ul>\n<li>at the end, all honest participants know the common random number <span class=\"math\">R</span></li>\n<li>malicious participants are not able to influence <span class=\"math\">R</span> or make the algorithm stuck.</li>\n</ul>\n<p><em>Preliminary setup</em></p>\n<p>Each participant <span class=\"math\">j</span> register her public keys <span class=\"math\">P[j]</span> with a smartcontract <span class=\"math\">DRNGManager</span>.</p>\n<p>Together with the public key <span class=\"math\">P[j]</span> each participant  submits to <span class=\"math\">DRNGManager</span> a ZK proof that <span class=\"math\">P[j]</span> is valid and that she knows the corresponding private key.</p>\n<p><em>Commit phase</em>  <strong>(10 minutes)</strong></p>\n<p>Each participant <span class=\"math\">j</span> generates a random  EC polynomial of degree <span class=\"math\">t</span>  <span class=\"math\">POLY[j]()</span>.<br>\nThe participant then generates a vector of polynomial evaluations <span class=\"math\">A[i] = [POLY[j](i)]</span> at <span class=\"math\">N</span> integer points  <span class=\"math\">i</span>.</p>\n<p>The participant will then encrypt the evaluations to obtain  a vector of encrypted polynomial evaluations <span class=\"math\">G_[j] = [Encrypt(POLY[j](i))]</span>.<br>\nIt then submits to <code>DRNGManager</code></p>\n<ul>\n<li>\n<p>vector <span class=\"math\">G[j][i]</span></p>\n</li>\n<li>\n<p>commitment to <span class=\"math\">POLY[j]</span></p>\n</li>\n<li>\n<p>a ZK-proof that <span class=\"math\">G[j][i]</span> were correctly generated from <span class=\"math\">POLY[j]</span>.</p>\n</li>\n</ul>\n<p>DRNGManager verifies ZK-proof on receipt</p>\n<p>After the commit phase, <span class=\"math\">DRNGManager</span> will contain  <span class=\"math\">j</span> valid vectors <span class=\"math\">G[j]</span>, where <span class=\"math\">j &gt;= t</span>.</p>\n<p><em>Reveal phase</em>. <strong>(10 minutes)</strong></p>\n<p>Each participant <span class=\"math\">j</span> will be able to  decrypt and reveal to <span class=\"math\">DRNGManager</span> a vector of points <span class=\"math\">POLY[j](i)</span>. The participants will then submit these vector to <span class=\"math\">DRNGManager</span> together with a ZK proof that reveal was done correctly.</p>\n<p>After the reveal phase, <span class=\"math\">DRNGManager</span>  will include <span class=\"math\">k</span> reveals, where <span class=\"math\">k &gt;= t</span>.</p>\n<p><em>RNG computation phase</em>. For each committed polynomial <span class=\"math\">POLY[j]</span>, each participant is then able calculate random number <span class=\"math\">R[j]</span> = <span class=\"math\">POLY[j](0)</span>. The common random is then XOR of all <span class=\"math\">R[j]</span></p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/on-chain-distributed-rng-using-zk/19176\">Read full topic</a></p>","link":"https://ethresear.ch/t/on-chain-distributed-rng-using-zk/19176","pubDate":"Mon, 01 Apr 2024 18:38:26 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19176"},"source":{"@url":"https://ethresear.ch/t/on-chain-distributed-rng-using-zk/19176.rss","#text":"On-chain Distributed RNG using ZK?"},"filter":false},{"title":"Increasing Gas Limit Without State Growth Implications Through Periodic Scheduled Downtime","dc:creator":"MaxResnick","category":"Uncategorized","description":"<p>Lately there has been increased discussion of raising the block gas limit from 30m gas to 40m gas. Critics of this proposal argue that increasing the gas limit risks excessive state bloat which could impact the ability to run full nodes on consumer hardware. With stateless light clients still far away we argue that a different solution could allow us to increase the gas limit without contributing to state bloat. We suggest periodic scheduled downtime outside 9 am-5 pm EST.</p>\n<p>Periodic scheduled downtime has a number of advantages:</p>\n<ol>\n<li>\n<p>Reduced state growth: By having scheduled downtime, the Ethereum network would have regular periods where no new transactions are being processed. This means that the state would not be growing during these times, helping to control the overall state size.</p>\n</li>\n<li>\n<p>Node maintenance: Scheduled downtime provides an opportunity for node operators to perform necessary maintenance tasks, such as pruning the state, switching to RETH, or plugging in more hard drives. This can help ensure that nodes continue to run efficiently and can handle the increased gas limit.</p>\n</li>\n<li>\n<p>Missed Slots: Fewer blocks per day means fewer opportunities for missed slots caused by optimistic relays and other bugs.</p>\n</li>\n<li>\n<p>Network resilience: Regularly scheduled downtime can help identify and address potential issues with the network before they become critical. For example, if an entity is engaging in a socially slashable activity, regularly scheduled downtime allows time for us to coordinate with the social slashing committee and prepare a hard fork before activity resumes.</p>\n</li>\n<li>\n<p>MEV smoothing: Data shows that MEV rewards are higher during EST business hours (see Figure 1 below for Average MEV-Boost payments by hour of day, this is based on 200,000 blocks from … to … ). This is unfair to solo stakers who randomly propose their blocks at night. By scheduling downtime at night, we avoid the unfair distribution of MEV rewards.</p>\n</li>\n</ol>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/f/f3ae5fbdbaf3ed34099a896b63eacd0244a9655d.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/f3ae5fbdbaf3ed34099a896b63eacd0244a9655d\" title=\"meme\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/f/f3ae5fbdbaf3ed34099a896b63eacd0244a9655d_2_690x377.jpeg\" alt=\"meme\" data-base62-sha1=\"yLHCqLH2EbfxaXL213ACftKmWtv\" width=\"690\" height=\"377\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/f/f3ae5fbdbaf3ed34099a896b63eacd0244a9655d_2_690x377.jpeg, https://ethresear.ch/uploads/default/optimized/2X/f/f3ae5fbdbaf3ed34099a896b63eacd0244a9655d_2_1035x565.jpeg 1.5x, https://ethresear.ch/uploads/default/original/2X/f/f3ae5fbdbaf3ed34099a896b63eacd0244a9655d.jpeg 2x\" data-dominant-color=\"F1F1F2\"></a></div><p></p>\n<p>Of course, there are some disadvantages— a key one that comes to mind is that this arrangement may be unfair to Solo stakers in RoW: While a 9 am – 5pm chain will be ideal for solo stakers based in North America, it can be incredibly inconvenient for a staker in e.g. Asia (straightforward calculations show that this will be 11pm–7am for a staker in Singapore). Nevertheless we believe the stated advantages overwhelm these.</p>\n<p>In conclusion, implementing periodically scheduled downtime outside of peak hours could provide a balanced solution that allows for increased gas limits while mitigating the risks of excessive state growth. As the Ethereum network continues to evolve, it is crucial to explore and adopt innovative approaches to scaling that maintain the network’s decentralization and accessibility.</p>\n            <p><small>3 posts - 3 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/increasing-gas-limit-without-state-growth-implications-through-periodic-scheduled-downtime/19175\">Read full topic</a></p>","link":"https://ethresear.ch/t/increasing-gas-limit-without-state-growth-implications-through-periodic-scheduled-downtime/19175","pubDate":"Mon, 01 Apr 2024 16:24:21 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19175"},"source":{"@url":"https://ethresear.ch/t/increasing-gas-limit-without-state-growth-implications-through-periodic-scheduled-downtime/19175.rss","#text":"Increasing Gas Limit Without State Growth Implications Through Periodic Scheduled Downtime"},"filter":true},{"title":"Reward curve with tempered issuance: EIP research post","dc:creator":"aelowsson","category":"Economics","description":"<h1><a name=\"reward-curve-with-tempered-issuance-eip-research-post-1\" class=\"anchor\" href=\"https://ethresear.ch#reward-curve-with-tempered-issuance-eip-research-post-1\"></a>Reward curve with tempered issuance: EIP research post</h1>\n<p>By <a href=\"https://twitter.com/weboftrees\">Anders</a> <a href=\"https://warpcast.com/anderselowsson\">Elowsson</a></p>\n<p><em>My aim with this post is to present the rationale for a reward curve with tempered issuance. It is a longer format of the forthcoming EIP, offering a more detailed account of the benefits and the trade-offs that must be balanced, as well as a comparison with alternative options. I will try to answer any questions you may have in the comments!</em></p>\n<h2><a name=\"h-1-overview-2\" class=\"anchor\" href=\"https://ethresear.ch#h-1-overview-2\"></a>1. Overview</h2>\n<p>Validators began receiving execution layer rewards with The Merge, and liquidity improved when withdrawals were enabled with Shapella. Both upgrades have served to increase the equilibrium quantity of stake. There is a broad consensus that the current deposit size keeps Ethereum sufficiently secure (e.g., <a href=\"https://www.reddit.com/r/ethereum/comments/191kke6/comment/kh79gh1\">1</a>, <a href=\"https://www.reddit.com/r/ethereum/comments/191kke6/comment/kh7do9k/\">2</a>, <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-12-security-and-deposit-size-4\">3</a>). Yet issuance will rise substantially as more stake is deposited under the current reward curve. Excessive incentives for staking, beyond what is necessary for security, can unfortunately over time turn into <a href=\"https://en.wikipedia.org/wiki/Subsidy#Perverse_subsidies\">perverse subsidies</a>, with many downsides. This post will explore the benefits of moderating issuance and available options. It concludes that Ethereum should adopt the <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-55-potential-candidate-for-a-new-reward-curve-23\">candidate reward curve</a>, designed to effectively moderate quantity staked while still maintaining reliable consensus incentives, economic security, resistance to discouragement attacks and cartelization attacks, a favorable composition of the staking set, and viable conditions for solo staking. The candidate reward curve divides the equation of the current reward curve by <span class=\"math\">1+D/k</span>. The single adjusted variable <span class=\"math\">k</span> then comes to define the quantity staked at the peak issuance point, which also corresponds to the point where issuance is halved relative to the current reward curve. A setting of <span class=\"math\">k=2^{26}</span> (77.3M ETH) is suggested for the forthcoming hard fork, with a subsequent final adjustment to <span class=\"math\">k=2^{25}</span> (33.6M ETH).</p>\n<h4><a name=\"section-2-rationale-3\" class=\"anchor\" href=\"https://ethresear.ch#section-2-rationale-3\"></a>Section 2 – Rationale</h4>\n<p>The equilibrium yield offered to stakers corresponds to the cost that the marginal staker assigns to staking, i.e., their indifference point between staking and not staking. Section 2.1 shows that if Ethereum offers a higher yield than necessary for maintaining security, it compels its users to incur higher costs, degrading user utility in aggregate. This explains why all ETH holders can benefit from an issuance reduction, something which is also illustrated in Figure 2. The macro perspective is reviewed in Section 2.2. As the quantity of stake grows, one or a few liquid staking tokens (LSTs) may come to supplant ETH as money in Ethereum. Positive network externalities could then lead to both lower user utility and protocol decentralization. Their derivative nature can also erode the social layer’s capacity for upholding Ethereum’s intended consensus process. Section 2.3 suggests that Ethereum should adopt a graduated approach to the proposed changes. Taking a smaller step in the next hard fork will signal intent without being too disruptive, smooth out disequilibria, and allow for an intermediate evaluation of effects on stake composition and quantity.</p>\n<h4><a name=\"section-3-three-options-for-the-reward-curve-4\" class=\"anchor\" href=\"https://ethresear.ch#section-3-three-options-for-the-reward-curve-4\"></a>Section 3 – Three options for the reward curve</h4>\n<p>Section 3 presents three options for the reward curve. The candidate reward curve is Option A, with an issuance level that slowly falls as the quantity staked increases beyond desirable levels. For Option B, the issuance level will instead approach an asymptotic maximum. Option C is the current reward curve, with a rising issuance across the full staking range. The options are compared in Section 3.4, showing how the reward curves diverge as the quantity staked increases.</p>\n<h4><a name=\"section-4-trade-offs-and-priors-5\" class=\"anchor\" href=\"https://ethresear.ch#section-4-trade-offs-and-priors-5\"></a>Section 4 – Trade-offs and priors</h4>\n<p>Section 4 provides a deeper examination of issuance policy trade-offs than what is reasonable in an EIP. Yield variability for solo stakers is first explored across the different options in Section 4.1. Section 4.2 then reviews how the staking yield may affect the proportion of solo stakers under equilibrium. The biggest risk is that disadvantageous economies of scale make solo staking less viable. However, at a higher quantity staked, dominant staking service providers (SSPs) can offer lower fees, better LST money, and lower risk due to emerging moral hazard. The candidate reward curve offers a positive yield across the full range—low enough to make a high quantity stake unlikely, yet sufficiently high as to never force out solo stakers on efficient setups under more improbable scenarios. It is noted that priors regarding both the supply curve and the level of the maximum extractable value (MEV) are relevant to the design. A low issuance yield at a higher quantity staked can pre-emptively counter a rise in MEV. A scenario with a lower supply curve under the current level of MEV is also analyzed in Section 4.2.3, aiding the understanding of how a reward curve helps the consensus mechanism absorb delegating stakers with low reservation yields to ensure that solo stakers always are provided with some yield. The broader composition of the staking set is explored in Section 4.3, emphasizing that competition for delegated stake will take place across diverse market segments. Seeing that the cost of running a staking node will not be made prohibitively high relative to the yield, the proposed change will allow variety in preferences and circumstances between delegators to play a more central role in shaping the composition of the staking set. Section 4.4 finally plots the “isoproportion map”, presenting the conditions under which a change to issuance policy is profitable to stakers.</p>\n<h4><a name=\"section-5-security-considerations-6\" class=\"anchor\" href=\"https://ethresear.ch#section-5-security-considerations-6\"></a>Section 5 – Security considerations</h4>\n<p>Security considerations are also afforded more space than what may be typical for an EIP. This includes a tabulation of economic security under the different reward curves, as well as a review of consensus incentives, focusing on the proportion of the yield that is rewarded for attestation duties, as opposed to proposer duties. Sections 5.3-4 take a closer look at discouragement attacks and cartelization attacks. These attacks will become more favorable under stricter reward curves where issuance decreases substantially (particularly under a design with negative issuance levels), and are therefore important to keep in mind. The candidate reward curve does not materially exacerbate the risks of these attacks.</p>\n<h4><a name=\"section-6-conclusions-and-discussion-7\" class=\"anchor\" href=\"https://ethresear.ch#section-6-conclusions-and-discussion-7\"></a>Section 6 – Conclusions and discussion</h4>\n<p>Section 6.1 summarizes the advantages and disadvantages of different issuance levels, and Section 6.2 then presents the author’s conclusion that Option A—the candidate reward curve—is the best alternative for Ethereum. Option C can function as a provisional backup plan. The post concludes by exploring the unknown endgame, broadening the utility measure for token holders to incorporate the development of the entire ecosystem, and cautioning against exploiting users’ bounded rationality concerning yield.</p>\n<h2><a name=\"h-2-rationale-8\" class=\"anchor\" href=\"https://ethresear.ch#h-2-rationale-8\"></a>2. Rationale</h2>\n<h3><a name=\"h-21-user-utility-9\" class=\"anchor\" href=\"https://ethresear.ch#h-21-user-utility-9\"></a>2.1 User utility</h3>\n<p>Define the “reservation yield” as the lowest staking yield <span class=\"math\">y</span> at which an individual is willing to stake. Ethereum’s supply curve then emerges from prospective ETH holders’ reservation yields. Figure 1 shows a <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-21-supply-and-demand-9\">hypothetical supply curve</a> in blue that will be used in a few examples of this post. Total yearly rewards are <span class=\"math\">Y=yD</span>, where <span class=\"math\">D</span> is the quantity staked (“deposit size”). A holder’s reservation yield can be characterized as the “indifference point” where they derive as much utility from staking as from not staking. The area below the supply curve thus represents the implied aggregate cost to stakers <span class=\"math\">Y_c</span> and the area above represents their surplus <span class=\"math\">Y_s</span>, which combines into total rewards <span class=\"math\">Y_c+Y_s=Y</span>. Relevant costs (broadly defined) include hardware and other resources, upkeep, the acquisition of technical knowledge, illiquidity, trust in third parties and other factors increasing the risk premium, various opportunity costs, taxes, etc. A high yield compels users to incur higher costs than necessary for maintaining security, diminishing user utility in aggregate. Slashing risks or any bugs that would effectively burn the stake are from an “accounting perspective” less attributable as a pure cost at the aggregate protocol level, and taxes will by their nature actually increase costs for a user when its surplus rises. But the general principle of capturing costs below the supply curve as the issuance that does not generate any surplus to stakers is very useful for understanding staking economics and welfare.</p>\n<p>The aggregate difference in costs <span class=\"math\">Y'_c</span> between two policies can be computed as the definite integral of the equation for the inverse supply curve, bounded by the two equilibrium quantities of the comparative static. It is the darker blue region in Figure 1, with the green candidate reward curve (Option A) used as the new policy. The darker grey region in the figure instead represents the difference in the aggregate surplus <span class=\"math\">Y'_s</span>. For this hypothetical supply curve, the total reduction in rewards (and thus issuance) is around 700k ETH, of which 446k ETH is a cost reduction from the users’ perspective and 252k ETH is a surplus reduction with this specific hypothetical reward curve. Equations, calculations (integration by substitution) and estimates are not pertinent here; it suffices to conclude that the outcome with a similar shape will be a similar definite integral and cost/surplus respectively. If the supply curve is flatter around the comparative static, <span class=\"math\">Y'_c</span> becomes relatively larger and <span class=\"math\">Y'_s</span> relatively smaller, and vice versa.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/5/5dd5af69e35d1a3af03cc34e079b65918803b623.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/5dd5af69e35d1a3af03cc34e079b65918803b623\" title=\"Figure 1\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/5/5dd5af69e35d1a3af03cc34e079b65918803b623_2_690x410.jpeg\" alt=\"Figure 1\" data-base62-sha1=\"do6dPZcdne6nKI4XrEbH44P1dIv\" width=\"690\" height=\"410\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/5/5dd5af69e35d1a3af03cc34e079b65918803b623_2_690x410.jpeg, https://ethresear.ch/uploads/default/optimized/2X/5/5dd5af69e35d1a3af03cc34e079b65918803b623_2_1035x615.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/5/5dd5af69e35d1a3af03cc34e079b65918803b623_2_1380x820.jpeg 2x\" data-dominant-color=\"E8E8F5\"></a></div><p></p>\n<p><strong>Figure 1.</strong> Implied cost <span class=\"math\">Y_c</span> and surplus <span class=\"math\">Y_s</span> of staking with a hypothetical supply curve. A change in issuance policy shifts the equilibrium from the black square to the green circle. The cost savings <span class=\"math\">Y'_c</span> leads to aggregate welfare improvement as long as the protocol remains secure and decentralized, and the reduction in surplus <span class=\"math\">Y'_s</span> simply shifts some utility from stakers to non-stakers.</p>\n<p>Too often, all staking yield is considered as a surplus <span class=\"math\">Y_s</span>, with the implication that issuance policy is a zero-sum game. But due to the costs that must be borne by stakers, <strong>issuance policy is emphatically <em>not</em> zero sum</strong>. It is fundamental to understand that by having a higher yield, Ethereum steers its users to take on higher costs in aggregate. This is the reason why <a href=\"https://x.com/weboftrees/status/1710708846249865374\">everyone can gain</a> when keeping issuance at the minimum viable level, as long as they own the underlying ETH.</p>\n<p>To illustrate this, <a href=\"https://notes.ethereum.org/@anderselowsson/MinimumViableIssuance#Benefits-of-MVI-to-user-utility\">the attainable change to someone’s proportion of all ETH can be calculated</a> as</p>\n<div class=\"math\">\ny_p=\\frac{1+y}{1+s}-1.\n</div>\n<p>It can be interpreted as the <a href=\"https://en.wikipedia.org/wiki/Fisher_equation\">Fisher equation</a> adapted to Ethereum. The “proportional yield” <span class=\"math\">y_p</span> can be computed for both stakers and non-stakers, in the latter case with <span class=\"math\">y=0</span>. The circulating supply inflation rate <span class=\"math\">s=i-b</span> derives from the issuance rate <span class=\"math\">i=Y_i/S</span> and the burn rate <span class=\"math\">b=B/S=0.008</span>, where <span class=\"math\">Y_i</span> is yearly issuance, <span class=\"math\">S</span> is the circulating supply and <span class=\"math\">B</span> is yearly burn (using the average since The Merge). Further complexities regarding the interpretation of rates in light of compounding effects under a drift of the circulating supply are set aside. Figure 2 illustrates the hypothetical effect of an issuance reduction on user utility, utilizing a comparison of <span class=\"math\">y_p</span> at two different equilibria. It is shown here under the candidate reward curve—Option A—and was in <a href=\"https://x.com/weboftrees/status/1710706011185545671\">previous work</a> presented when halving issuance with the current reward curve (Option C), under a slightly different supply curve. Both <span class=\"math\">y</span> (red arrow) and <span class=\"math\">s</span> (orange arrow) are reduced approximately the same, so stakers are left virtually unaffected. Define the change in cardinal utility for a comparative static where <span class=\"math\">y_p</span> changes from <span class=\"math\">y^b_p</span> to <span class=\"math\">y^a_p</span> as</p>\n<div class=\"math\">\nu'=\\frac{1+y^a_p}{1+y^b_p}-1,\n</div>\n<p>but use the reservation yield as <span class=\"math\">y</span> for those who stop staking when computing <span class=\"math\">y^a_p</span>. Below that yield, they do not stake anyway, so they suffer no additional loss in utility as the yield decreases further. As shown in the bottom pane, de-stakers will under this definition derive a higher utility at the new equilibrium, with non-stakers clearly better off. The reason why welfare can improve in this manner is that the staking costs <span class=\"math\">Y'_c</span> of the blue area from Figure 1 are eliminated. Of course, from a protocol perspective, the question of whether solo stakers continue staking or not is also important. This is a separate topic further explored in Section 4.2. The analysis simply concludes that with the hypothetical supply curve, stakers will be left either unaffected (remaining as stakers if they have low reservation yields) or left even better off—as de-stakers.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/9/9653863b260144f60e47d8300c14bb59fc8b7f7a.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/9653863b260144f60e47d8300c14bb59fc8b7f7a\" title=\"Figure 2\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/9/9653863b260144f60e47d8300c14bb59fc8b7f7a_2_690x466.jpeg\" alt=\"Figure 2\" data-base62-sha1=\"lrQznxCOQflw9uOkxcI3xYFWg54\" width=\"690\" height=\"466\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/9/9653863b260144f60e47d8300c14bb59fc8b7f7a_2_690x466.jpeg, https://ethresear.ch/uploads/default/optimized/2X/9/9653863b260144f60e47d8300c14bb59fc8b7f7a_2_1035x699.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/9/9653863b260144f60e47d8300c14bb59fc8b7f7a_2_1380x932.jpeg 2x\" data-dominant-color=\"F7F5F5\"></a></div><p></p>\n<p><strong>Figure 2.</strong> The change in <span class=\"math\">y</span> and <span class=\"math\">s</span> between two hypothetical equilibria is utilized to isolate a change in cardinal utility <span class=\"math\">u'</span> for all token holders. Stakers are subjected to a reduction in yield <span class=\"math\">y</span>, but the reduction in the inflation rate <span class=\"math\">s</span> is similar, so they are left unaffected (<span class=\"math\">y_p</span> stays the same). De-stakers incur no further loss in utility once <span class=\"math\">y</span> falls below their reservation yield. They benefit together with non-stakers from the reduction in <span class=\"math\">s</span>.</p>\n<p>Schwarz-Shilling and Dietrich refer to <span class=\"math\">y_p</span> as the “<a href=\"https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751\">real total staking yield</a>” (they do not specify an equation in their post, but may imply that they use <span class=\"math\">y_p=y-s</span>; with <span class=\"math\">b=0</span> for visual clarity). Their plots across reward curves, including <a href=\"https://ethereum-magicians.org/t/electra-issuance-curve-adjustment-proposal/18825#proposal-for-electra-issuance-curve-adjustment-3\">plots of Option A</a>, are very useful for conceptualizing the measure itself.</p>\n<h3><a name=\"h-22-macro-perspective-10\" class=\"anchor\" href=\"https://ethresear.ch#h-22-macro-perspective-10\"></a>2.2 Macro perspective</h3>\n<p>The reward curve influences the proportion of all circulating ETH that is staked, and it is therefore important to examine the <a href=\"https://notes.ethereum.org/@anderselowsson/MinimumViableIssuance#Benefits-of-MVI-from-a-macro-perspective\"><em>macro perspective of Ethereum’s issuance policy</em></a>. This can be regarded as one component of a <a href=\"https://en.wikipedia.org/wiki/True_cost_accounting\">true cost accounting</a>, extending Section 2.1 to incorporate externalities.</p>\n<p>An LST exceeding critical thresholds regarding the proportion of stake under its control can gain outsized profits. This <a href=\"https://notes.ethereum.org/@djrtwo/risks-of-lsd#Stratum-for-cartelization\">stratum for cartelization</a> of block space can ultimately compromise the consensus mechanism. Risks to Ethereum are further exacerbated if an expansive issuance policy (which the current reward curve arguably represents) enables an LST to attain control over a significant proportion of the <em>total</em> ETH—propelled by network externalities of, e.g., the money function.  The compromised institution(s) then sits one layer above the consensus mechanism, namely the social layer. It became apparent with <a href=\"https://en.wikipedia.org/wiki/The_DAO\">The DAO</a> that if the proportion of the total circulating supply affected by an outcome grows sufficiently large, then the “social layer” may waver on its commitment to the underlying intended consensus process. If the community can no longer effectively intervene in the event of for example a 51 % liveness attack, then risk mitigation in the form of the <a href=\"https://www.youtube.com/watch?v=8DHGOlIlMvc&amp;t=1938s\">warning system</a> discussed by Buterin may not be effective. The proof-of-stake consensus mechanism has in this case through derivatives grown so interconnected with Ethereum’s users that it has overloaded its ultimate arbitrator, the social consensus mechanism. It is a special and in a way inverted case of issues Buterin <a href=\"https://vitalik.eth.limo/general/2023/05/21/dont_overload.html\">previously warned about</a>.</p>\n<p>If the issuance policy leads to a high proportion of all ETH being staked, one or a few LSTs may overtake as money in the Ethereum ecosystem, embedding themselves across every layer and application. As has <a href=\"https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751#network-effects-of-money-lsts-no-thanks-to-forced-risk-taking-8\">been noted</a>, this is a deliberate <a href=\"https://research.lido.fi/t/hasus-goose-submission-proposed-goals-for-lido-dao-to-consider/5590\">objective</a> of some SSPs. By moderating the issuance, each LST will have tougher competition with non-staked ETH, ensuring a <a href=\"https://x.com/fradamt/status/1760808900792594593\">trustless asset</a> within the ecosystem. The social layer will not become co-dependent on an outside organization and its issued derivative of ETH. The principal–agent problem (PAP) of delegating stake to a dominant LST can then be priced more accurately because moral hazard will be less likely to develop. <a href=\"https://x.com/weboftrees/status/1710713959362252884\">No LST will grow “too big to fail”</a> in the eyes of the Ethereum social layer. This pricing will reflect the fact that the agent acting on behalf of the delegator (or any party able to inject themselves into that relationship) gains greater opportunities to degrade consensus for its own profit the larger proportion of the stake it controls. The delegating staker must then continuously assess its security guarantees (e.g., the staking agent’s or injecting parties’ own value at risk), knowing that it may lose everything if the worst-case scenario ever comes to pass.</p>\n<p>One cost explored in Section 2.1 can also affect Ethereum at a macro level. In the case where everyone must stake to not see their savings eroded, differences in tax policies across jurisdictions can hamper geographical decentralization.</p>\n<h3><a name=\"h-23-graduated-approach-11\" class=\"anchor\" href=\"https://ethresear.ch#h-23-graduated-approach-11\"></a>2.3 Graduated approach</h3>\n<p>There are a few arguments for a graduated approach, taking a smaller step in the right direction in the next hard fork—evaluating the outcome—and if appropriate taking the final step:</p>\n<ul>\n<li>Compromising economic agents’ ability to plan ahead is welfare degrading. While there have been several proposals seeking to temper issuance dating back to at least 2021 (<a href=\"https://ethresear.ch/t/simplified-active-validator-cap-and-rotation-proposal/9022\">1</a>, <a href=\"https://ethresear.ch/t/circulating-supply-equilibrium-for-ethereum-and-minimum-viable-issuance-during-the-proof-of-stake-era/10954\">2</a>, <a href=\"https://notes.ethereum.org/@vbuterin/single_slot_finality#Economic-capping-of-total-deposits\">3</a>), individual solo stakers cannot be expected to follow the research debate all too closely. They may first pay attention once a decision has been taken or seriously contemplated. There is also a difference between signaled intentions to moderate issuance and a real decision having been made. At the same time, the quantity of stake has at this point already arguably exceeded levels providing sufficient security. Further growth must be considered welfare degrading <a href=\"https://www.reddit.com/r/ethereum/comments/14vpyb3/comment/jro1739\">as well</a>. A graduated approach can in this context:\n<ul>\n<li>Give solo stakers, and also SSPs, a longer and more gradual adaptation phase.</li>\n<li>Do something to temper the incentives to stake in the near term, while not being too disruptive.</li>\n<li>Convey the possibility of additional adjustments to users who may not follow ongoing discussions that closely.</li>\n</ul>\n</li>\n<li>Ignoring frictions in the decision to stake, a reduction in issuance will precipitate a temporary phase with below-equilibrium yields, until some stakers leave and a new equilibrium is established. A graduated reduction mitigates this problem. An even more gradual reduction, for instance, encoding a small adjustment to issuance every epoch, would bring more implementation overhead.</li>\n<li>It is not possible at this time to ascertain that the full issuance reduction will be necessary to temper the growth in the quantity of stake—it is only a reasonable estimate hinging on assumptions of frictions in the decision to stake or a falling supply curve. Therefore, it seems sensible to move gradually, at least as an acknowledgment of these uncertainties.</li>\n<li>There are a few properties of issuance level of particular interest going forward, such as the proportion of solo stakers retained. A graduated approach makes it possible to evaluate intermediate outcomes before implementing the full envisioned issuance reduction. Even if researchers are confident that the full reduction keeps relevant properties in balance, a graduated approach—with an intermediate evaluation before proceeding further—will make the change more palatable to those who may object.</li>\n</ul>\n<p>The next section will present reward curves that incorporate a potential intermediate step as part of a graduated approach.</p>\n<h2><a name=\"h-3-three-options-for-the-reward-curve-12\" class=\"anchor\" href=\"https://ethresear.ch#h-3-three-options-for-the-reward-curve-12\"></a>3. Three options for the reward curve</h2>\n<p>There are three fundamental paradigms that the reward curve can be designed according to under current circumstances (e.g., absence of MEV burn, no staking fee, and a desire to keep yield variability manageable). These were stipulated in <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-6-conclusion-and-discussion-24\">Table 2 in preceding work</a> and are summarized here in Table 1. Option A is the proposed reward curve. It is best at improving user utility and moderating quantity staked, as advocated in Sections 2.1-2.2. Option C provides a higher yield for (solo) staking in the scenario where almost everyone stakes. This may of course also be seen as a drawback, a trade-off further examined in Sections 4.1-4.3. Option B balances between these two approaches. Section 3.4 will plot the reward curves together for further comparison, both for a full reduction and a graduated approach.</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Issuance</th>\n<th>Benefit</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Option A:</strong></td>\n<td>Falls marginally at deposit sizes above sufficient security.</td>\n<td>Improves user utility and moderates quantity staked.</td>\n</tr>\n<tr>\n<td><strong>Option B:</strong></td>\n<td>Asymptotically approaches maximum fixed level.</td>\n<td>Balances benefits of Option A and C.</td>\n</tr>\n<tr>\n<td><strong>Option C:</strong></td>\n<td>Continues increasing indefinitely</td>\n<td>Higher yield for solo staking if quantity staked approaches maximum</td>\n</tr>\n</tbody>\n</table>\n</div><p><strong>Table 1.</strong> Three main categories for the reward curves, each explored in this section. Option A is favored by the author.</p>\n<h3><a name=\"h-31-option-a-proposed-reward-curve-13\" class=\"anchor\" href=\"https://ethresear.ch#h-31-option-a-proposed-reward-curve-13\"></a>3.1 Option A – Proposed reward curve</h3>\n<h4><a name=\"overview-14\" class=\"anchor\" href=\"https://ethresear.ch#overview-14\"></a>Overview</h4>\n<p>Option A attenuates issuance beyond a quantity of stake sufficient for security. It is designed to give very clear mental models: the value of a single adjusted variable <span class=\"math\">k</span> also defines the quantity of stake at both the peak issuance and the issuance halving point (relative to the current reward curve). The reward curve was presented as the <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-55-potential-candidate-for-a-new-reward-curve-23\">candidate reward curve</a> in a previous post on properties of issuance level. Figure 3 illustrates annualized issuance level across deposit size under perfect validator performance. Issuance of the current reward curve in black varies with deposit size according to the equation <span class=\"math\">Y_i=cF\\sqrt{D}</span>, with <span class=\"math\">F</span> set to 64 and the constant <span class=\"math\">c\\approx2.6</span>. The grey curve shows halved issuance (<span class=\"math\">F=32</span>). The proposed reward curve in green introduces a division by <span class=\"math\">1+D/k</span></p>\n<div class=\"math\">\nY_i=\\frac{cF\\sqrt{D}}{1+D/k},\n</div>\n<p>with <span class=\"math\">c</span> and <span class=\"math\">F</span>  unchanged. The full reduction is intended to be <span class=\"math\">k=2^{25}</span>, which gives a peak issuance slightly below 0.5M ETH at <span class=\"math\">2^{25}</span> (33.6M) ETH staked (plus sign), which is also the halving point. The dashed green curve with <span class=\"math\">k=2^{26}</span> (around 67.1M) indicates a potential step in a graduated approach. The fact that the reward curve stipulates a maximum issuance level, which cannot be exceeded, may be beneficial from a communication perspective. In the future, Ethereum’s reward curve shall transition to vary with deposit ratio <span class=\"math\">d=D/S</span> (see Section 6.3), which will then instead translate into a maximum circulating supply inflation rate.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/f/f4b1fef3ee5264c02f7bf663c0a3257df1f1706d.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/f4b1fef3ee5264c02f7bf663c0a3257df1f1706d\" title=\"Figure 3\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/f/f4b1fef3ee5264c02f7bf663c0a3257df1f1706d_2_690x404.jpeg\" alt=\"Figure 3\" data-base62-sha1=\"yUFR9kKp7rj2j05Y60w24ofsBfD\" width=\"690\" height=\"404\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/f/f4b1fef3ee5264c02f7bf663c0a3257df1f1706d_2_690x404.jpeg, https://ethresear.ch/uploads/default/optimized/2X/f/f4b1fef3ee5264c02f7bf663c0a3257df1f1706d_2_1035x606.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/f/f4b1fef3ee5264c02f7bf663c0a3257df1f1706d_2_1380x808.jpeg 2x\" data-dominant-color=\"F9FAF9\"></a></div><p></p>\n<p><strong>Figure 3.</strong> Issuance level for the proposed reward curve—Option A—in green, compared to the current reward curve in black. The dashed green line represents a stepwise reduction in a potential graduated approach. Halved issuance relative to the current reward curve is indicated in grey, with plus signs indicating both halving points and issuance peaks.</p>\n<p>Figure 4 illustrates the staking yield for Option A in green, assuming a realized extractable value (REV) of $V=300$k ETH/year (REV is MEV after builders take their cut and the current prevailing level is slightly above 300k). The equation for staking yield is <span class=\"math\">y=y_i+y_v</span>, with <span class=\"math\">y_i=Y_i/D</span> and <span class=\"math\">y_v=V/D</span>. For Option A, the equation for issuance yield thus becomes</p>\n<div class=\"math\">\ny_i=\\frac{cF}{\\sqrt{D}(1+D/k)},\n</div>\n<p>once again multiplying the denominator of the current reward curve (<span class=\"math\">y_i=cF/\\sqrt{D}</span>) by <span class=\"math\">1+D/k</span>. The same hypothetical supply curve as in Section 2.1 is included in Figure 4. It serves to illustrate how the propensity to supply stake may potentially vary across yield and deposit size a few years from now, and what the impact on the staking equilibrium then becomes with an altered reward curve. In this particular example, the equilibrium yield is reduced by 0.6 % (around 1/5), from 2.94 % with the current reward curve (black rectangle) to 2.34 % with the new reward curve (green circle). In the graduated approach, the equilibrium yield is reduced by only 0.43 % (around 1/7).</p>\n<p>A reasonable <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-22-influence-of-f-on-the-equilibrium-10\">assumption</a> is that the supply curve will gradually shift downwards over time as the staking experience simplifies and DeFi integrations improve. This will in essence depend on how the various costs outlined in Section 2.1 evolve. The faint blue curve could then be the supply curve after another year or two has passed. This would further reduce the yield under both the present and proposed reward curve. The supply curve may also under some circumstances shift upward, for instance, due to a consensus failure prompting stakers to reevaluate risks, or due to increased demand for non-staked ETH, elevating opportunity costs.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/b/bb00435c9bc5b2bce5cf3f4caf5e055920b1219a.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/bb00435c9bc5b2bce5cf3f4caf5e055920b1219a\" title=\"Figure 4\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/b/bb00435c9bc5b2bce5cf3f4caf5e055920b1219a_2_690x413.jpeg\" alt=\"Figure 4\" data-base62-sha1=\"qGhQKd6epf9EyoGzGpFXQVLJA4G\" width=\"690\" height=\"413\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/b/bb00435c9bc5b2bce5cf3f4caf5e055920b1219a_2_690x413.jpeg, https://ethresear.ch/uploads/default/optimized/2X/b/bb00435c9bc5b2bce5cf3f4caf5e055920b1219a_2_1035x619.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/b/bb00435c9bc5b2bce5cf3f4caf5e055920b1219a_2_1380x826.jpeg 2x\" data-dominant-color=\"F9FAF9\"></a></div><p></p>\n<p><strong>Figure 4.</strong> Staking yield (inclusive of REV) for the proposed reward curve—Option A—in green, compared to the current reward curve in black. The graduated approach is depicted in dashed green. Hypothetical changes in equilibrium from the black square to the green circles are indicated as a guideline, but remain speculative due to uncertainty regarding both the supply curve and future levels of REV.</p>\n<h4><a name=\"reward-variability-15\" class=\"anchor\" href=\"https://ethresear.ch#reward-variability-15\"></a>Reward variability</h4>\n<p>In preparation for the EIP, a <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-4-variability-in-rewards-for-solo-stakers-12\">simulation of reward variability</a> was performed with <a href=\"https://flashbots-data.s3.us-east-2.amazonaws.com/index.html\">current data for REV</a>. Figure 5 shows the simulated cumulative distribution function (CDF) of rewards for solo stakers at various deposit sizes for the proposed reward curve in its full implementation. Figure 6 instead shows the simulated outcome for the graduated approach.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/4/4269de3ae7e65eac785777084dd423808622379f.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/4269de3ae7e65eac785777084dd423808622379f\" title=\"Figure 5\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/4/4269de3ae7e65eac785777084dd423808622379f_2_690x379.jpeg\" alt=\"Figure 5\" data-base62-sha1=\"9twkJlLi6DYqbpbUcgJVvyv34Qv\" width=\"690\" height=\"379\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/4/4269de3ae7e65eac785777084dd423808622379f_2_690x379.jpeg, https://ethresear.ch/uploads/default/optimized/2X/4/4269de3ae7e65eac785777084dd423808622379f_2_1035x568.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/4/4269de3ae7e65eac785777084dd423808622379f_2_1380x758.jpeg 2x\" data-dominant-color=\"F7F7F5\"></a></div><p></p>\n<p><strong>Figure 5.</strong> Solo staker yield variability at various quantities staked under the candidate reward curve and present level of REV. Expected staking yields are indicated by dashed vertical lines.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/f/fff39e8c808be2d9cd92ee5970b7f1826111987e.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/fff39e8c808be2d9cd92ee5970b7f1826111987e\" title=\"Figure 6\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/f/fff39e8c808be2d9cd92ee5970b7f1826111987e_2_690x379.jpeg\" alt=\"Figure 6\" data-base62-sha1=\"AwfHPBTbBWkL4Us5WReTJsXG9A2\" width=\"690\" height=\"379\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/f/fff39e8c808be2d9cd92ee5970b7f1826111987e_2_690x379.jpeg, https://ethresear.ch/uploads/default/optimized/2X/f/fff39e8c808be2d9cd92ee5970b7f1826111987e_2_1035x568.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/f/fff39e8c808be2d9cd92ee5970b7f1826111987e_2_1380x758.jpeg 2x\" data-dominant-color=\"F7F7F5\"></a></div><p></p>\n<p><strong>Figure 6.</strong> Solo staker yield variability at various quantities staked under a graduated implementation of the candidate reward curve and present level of REV. Expected staking yields are indicated by dashed vertical lines.</p>\n<h3><a name=\"h-32-option-b-asymptotically-fixed-issuance-16\" class=\"anchor\" href=\"https://ethresear.ch#h-32-option-b-asymptotically-fixed-issuance-16\"></a>3.2 Option B – Asymptotically fixed issuance</h3>\n<h4><a name=\"overview-17\" class=\"anchor\" href=\"https://ethresear.ch#overview-17\"></a>Overview</h4>\n<p>A somewhat more modest option is to instead divide the present reward curve with <span class=\"math\">1+\\sqrt{D}/k</span>, giving an issuance of</p>\n<div class=\"math\">\nY_i=\\frac{cF\\sqrt{D}}{1+\\sqrt{D}/k}=\\frac{cF}{D^{-0.5}+k^{-1}}.\n</div>\n<p>The equation is in essence a log-logistic CDF, and issuance will thus approach an asymptotic maximum of <span class=\"math\">cFk</span> (but it may not come close to that value while Ethereum operates at its current circulating supply). Figure 7 illustrates a potential reward curve for Option B in orange with <span class=\"math\">k=2^{12}</span> and a potential graduated approach with <span class=\"math\">k=2^{13}</span>. As indicated in the figure, the variable <span class=\"math\">F</span> was adjusted to produce a similar issuance as Options A (and Option C) at specific levels, for easier comparison in subsequent sections. Hence, the halving point marked by a plus sign is also around <span class=\"math\">D=2^{25}</span>, and the graduated approach matches the issuance of Option A at the current quantity staked (31M ETH). But <span class=\"math\">F</span> could be re-adjusted to 64, which would increase the probability of an equilibrium at a lower quantity staked.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/a/a09030ca3df5130faccc0609d9f9f11e5d8edf11.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/a09030ca3df5130faccc0609d9f9f11e5d8edf11\" title=\"Figure 7\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/a/a09030ca3df5130faccc0609d9f9f11e5d8edf11_2_690x404.jpeg\" alt=\"Figure 7\" data-base62-sha1=\"mUpjYrVqYMCtpgnxHeTx1LjamB3\" width=\"690\" height=\"404\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/a/a09030ca3df5130faccc0609d9f9f11e5d8edf11_2_690x404.jpeg, https://ethresear.ch/uploads/default/optimized/2X/a/a09030ca3df5130faccc0609d9f9f11e5d8edf11_2_1035x606.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/a/a09030ca3df5130faccc0609d9f9f11e5d8edf11_2_1380x808.jpeg 2x\" data-dominant-color=\"FAFAF9\"></a></div><p></p>\n<p><strong>Figure 7.</strong> Issuance level for Option B in orange, relative to the current reward curve in black. The dashed orange line outlines a stepwise reduction in a potential graduated approach. Halved issuance relative to the current reward curve is indicated in grey with plus signs.</p>\n<p>The equation for issuance yield instead becomes</p>\n<div class=\"math\">\ny_i=\\frac{cF}{\\sqrt{D}+D/k}.\n</div>\n<p>It is plotted in Figure 10 in Section 3.4 together with the other reward curves.</p>\n<h3><a name=\"h-33-option-c-current-reward-curve-with-reduced-base-reward-factor-18\" class=\"anchor\" href=\"https://ethresear.ch#h-33-option-c-current-reward-curve-with-reduced-base-reward-factor-18\"></a>3.3 Option C – Current reward curve with reduced base reward factor</h3>\n<h4><a name=\"overview-19\" class=\"anchor\" href=\"https://ethresear.ch#overview-19\"></a>Overview</h4>\n<p>The simplest option is to reduce the base reward factor, keeping the current reward curve. The full reduction could then be <span class=\"math\">F=32</span>. It can be noted that this reward curve will reduce the yield relatively more at lower quantities of stake, so <span class=\"math\">F</span> can hardly be reduced below 32 from a security perspective, at least not at this point (see also Section 5.1). A graduated approach should opt for a range between 40-48. In this post, <span class=\"math\">F=44</span> will be illustrated, since the graduated reward curve will then coincide with all others for easier comparison. Figure 8 plots issuance. Yield is plotted in Figure 10 in Section 3.4 together with the other reward curves.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/9/92abc717a5ca3bb39c5391471677aa123014e602.png\" data-download-href=\"https://ethresear.ch/uploads/default/92abc717a5ca3bb39c5391471677aa123014e602\" title=\"Figure 8\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/9/92abc717a5ca3bb39c5391471677aa123014e602_2_690x398.png\" alt=\"Figure 8\" data-base62-sha1=\"kVvK312whXeTN5ed4HaQqCTQ2qu\" width=\"690\" height=\"398\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/9/92abc717a5ca3bb39c5391471677aa123014e602_2_690x398.png, https://ethresear.ch/uploads/default/optimized/2X/9/92abc717a5ca3bb39c5391471677aa123014e602_2_1035x597.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/9/92abc717a5ca3bb39c5391471677aa123014e602_2_1380x796.png 2x\"></a></div><p></p>\n<p><strong>Figure 8.</strong> Issuance level for Option C, relying on the current reward curve with a reduced base reward factor. The dashed grey curve indicates a potential setting for the graduated approach. The full possible reduction that retains sufficient economic security is a halving of issuance, indicated by the grey curve.</p>\n<h3><a name=\"h-34-comparison-of-options-a-c-20\" class=\"anchor\" href=\"https://ethresear.ch#h-34-comparison-of-options-a-c-20\"></a>3.4 Comparison of Options A-C</h3>\n<p>Figure 9 shows the three outlined options, both for a full reduction and a graduated approach. With the full reduction, all curves coincide around <span class=\"math\">D=2^{25}</span> ETH (circle), above which issuance for Option A declines, B levels off, and C continues increasing. With the graduated approach, all curves (dashed) coincide around the current quantity of stake of $D=31$M ETH (star), after which they diverge in a similar pattern. It can be noted that the graduated approach of both Option A and (narrowly) Option B gives a lower issuance if all ETH is staked than the full reduction in Option C.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/7/7714d22c1103e37bcc3a44b88c22b54a48e70800.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/7714d22c1103e37bcc3a44b88c22b54a48e70800\" title=\"Figure 9\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/7/7714d22c1103e37bcc3a44b88c22b54a48e70800_2_690x398.jpeg\" alt=\"Figure 9\" data-base62-sha1=\"gZrqwdUfDvV1y4h71QvyyFhfur6\" width=\"690\" height=\"398\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/7/7714d22c1103e37bcc3a44b88c22b54a48e70800_2_690x398.jpeg, https://ethresear.ch/uploads/default/optimized/2X/7/7714d22c1103e37bcc3a44b88c22b54a48e70800_2_1035x597.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/7/7714d22c1103e37bcc3a44b88c22b54a48e70800_2_1380x796.jpeg 2x\" data-dominant-color=\"F9FAF8\"></a></div><p></p>\n<p><strong>Figure 9.</strong> Comparison of issuance levels for Options A-C, with the graduated approach depicted by dashed curves. The three options provide the same issuance at around <span class=\"math\">2^{25}</span> ETH staked for the full reduction (circle) and at 31M ETH for the graduated step (star), above which issuance for Option A falls, B flatlines, and C continues rising.</p>\n<p>Figure 10 shows the staking yield (at the present level of REV) for the same three options. The black double-sided arrow illustrates that the initial reduction in staking yield at the present quantity staked would be under 1 %. The blue double-sided arrows capture the equilibrium shift in yield under the same hypothetical supply curves as previously, demonstrating that the hypothetical reduction in yield would be under 0.5 % for the first graduated step.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/1/1a61871de21e5de1c966aab7c66c8c8167d15e5b.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/1a61871de21e5de1c966aab7c66c8c8167d15e5b\" title=\"Figure 10\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/1/1a61871de21e5de1c966aab7c66c8c8167d15e5b_2_690x413.jpeg\" alt=\"Figure 10\" data-base62-sha1=\"3LnmJ81mkYC2sHYTHO0bfXKXkSf\" width=\"690\" height=\"413\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/1/1a61871de21e5de1c966aab7c66c8c8167d15e5b_2_690x413.jpeg, https://ethresear.ch/uploads/default/optimized/2X/1/1a61871de21e5de1c966aab7c66c8c8167d15e5b_2_1035x619.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/1/1a61871de21e5de1c966aab7c66c8c8167d15e5b_2_1380x826.jpeg 2x\" data-dominant-color=\"F8F9F8\"></a></div><p></p>\n<p><strong>Figure 10.</strong> Comparison of staking yield (at the present level of REV) for Options A-C, with the graduated approach illustrated by dashed curves, and the current reward curve in black. A shift in the equilibrium from the black square to the circle is indicated as a guideline, but there is uncertainty regarding both the supply curve and future REV. The initial reduction in the graduated approach is under 1 % (black arrows) and under 0.5 % at hypothetical equilibria (blue arrows).</p>\n<h2><a name=\"h-4-trade-offs-and-priors-21\" class=\"anchor\" href=\"https://ethresear.ch#h-4-trade-offs-and-priors-21\"></a>4. Trade-offs and priors</h2>\n<h3><a name=\"h-41-variability-for-solo-stakers-22\" class=\"anchor\" href=\"https://ethresear.ch#h-41-variability-for-solo-stakers-22\"></a>4.1 Variability for solo stakers</h3>\n<p>If issuance is moderated, a larger part of rewards will stem from REV, and variability in staking rewards may therefore increase. This affects solo stakers negatively since they cannot effortlessly rely on pooling for smoothing out variability. A simulation of reward variability was performed with current data for REV, resulting in, e.g., the CDFs previously plotted, but also mappings of annualized standard deviations (SDs) in yield across <span class=\"math\">y</span> and <span class=\"math\">D</span>. Recognizing that stakers may be more <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-43-variability-with-fixed-supply-and-varied-demand-15\">more sensitive to variance at lower staking yields</a>, the SDs can be divided by the square root of the expected yield, <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-45-two-dimensional-mappings-of-solo-rewards-variability-17\">referred to</a> as the SSD. This is a measure that attempts to capture degradation from variability. Figure 11 plots the SSD across quantity staked for the different reward curves of this study (lower is better). As evident, the SSD will rise marginally for Option A as the quantity staked increases. This increase is considered acceptable, on the basis that it is reasonable to sacrifice some variability to keep the quantity staked down. When considered in isolation, it is perhaps even desirable to allow the SSD to become a little worse at higher quantities staked than at lower quantities. But having a lower SSD at any <span class=\"math\">D</span> is of course preferable.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/1/1e1ace36a876ee07ece8e74b760333d408a56c0d.png\" data-download-href=\"https://ethresear.ch/uploads/default/1e1ace36a876ee07ece8e74b760333d408a56c0d\" title=\"Figure 11\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/1/1e1ace36a876ee07ece8e74b760333d408a56c0d_2_689x382.png\" alt=\"Figure 11\" data-base62-sha1=\"4ijKMZ0nSwPDqzYHxK6dW4GB5cp\" width=\"689\" height=\"382\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/1/1e1ace36a876ee07ece8e74b760333d408a56c0d_2_689x382.png, https://ethresear.ch/uploads/default/optimized/2X/1/1e1ace36a876ee07ece8e74b760333d408a56c0d_2_1033x573.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/1/1e1ace36a876ee07ece8e74b760333d408a56c0d_2_1378x764.png 2x\"></a></div><p></p>\n<p><strong>Figure 11.</strong> The SSD for the different reward curves (lower is better) across deposit size.</p>\n<h3><a name=\"h-42-equilibrium-yield-and-the-proportion-of-solo-stakers-23\" class=\"anchor\" href=\"https://ethresear.ch#h-42-equilibrium-yield-and-the-proportion-of-solo-stakers-23\"></a>4.2 Equilibrium yield and the proportion of solo stakers</h3>\n<h4><a name=\"h-421-overview-24\" class=\"anchor\" href=\"https://ethresear.ch#h-421-overview-24\"></a>4.2.1 Overview</h4>\n<p>Ethereum wants to retain solo stakers, certainly at least when measured as a proportion of all stakers. The anticipated outcome of a reduction in issuance level is that less ETH will be staked by both delegating and solo stakers, relative to the outcome when issuance is not reduced (<a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448/12#the-complex-conundrum-of-retaining-solo-stakers-1\">1</a>, <a href=\"https://ethereum-magicians.org/t/electra-issuance-curve-adjustment-proposal/18825/11\">2</a>). Using Figure 10 as an example, it is not clear whether the proportion of solo stakers is lower at a hypothetical equilibrium of around 33M ETH staked and a staking yield of 2.34 % than at around 50M ETH staked and a staking yield of 2.94 %. A concern is if there might be a staking yield below which solo stakers in particular would drop off due to the relatively higher fixed costs associated with solo staking. If solo stakers leave en masse below a yield of 2.5 %, then a staking yield of 2.34 % at 33M ETH staked will give a lower proportion than when the yield is 2.94 % at 50M ETH staked. This is of course something to take seriously. Economies of scale are hard to design away in a decentralized blockchain.</p>\n<p>There are however also some arguments as to why a more restrictive reward curve could give a higher or at least similar proportion of solo stakers (see also Section 2.2):</p>\n<ul>\n<li>Dominant SSPs have <a href=\"https://twitter.com/weboftrees/status/1710719496971862072\">better economies of scale at higher quantities staked</a>, increasing their cost advantage over solo stakers.</li>\n<li>Likewise, the <a href=\"https://x.com/weboftrees/status/1710712326117097785\">positive network externality of the money function</a> grows with quantity staked, and with it the competitive advantage of dominant LST-issuing SSPs.</li>\n<li>Furthermore, the PAP associated with an LST may seem less risky if everyone else also uses the LST, with the expectation that the social layer will waver on its commitment to the intended consensus process in the event of a failure.</li>\n<li>Risks could very well price out delegating stakers earlier than solo stakers as the yield falls. If a large enough subset of potential delegators believe that there is a 1 % risk of failure over a year for the LST they wish to hold, then favorable economies of scale or liquidity can be insufficient as a competitive edge. Self-custody is undeniably <a href=\"https://x.com/nero_eth/status/1766354381543292985\">important</a> to a relevant proportion of ETH token holders; this factor should not be overlooked when evaluating the staking supply side.</li>\n<li>Token holders with enough ETH and the technical ability to solo stake are not necessarily abundant, implying a <a href=\"https://twitter.com/weboftrees/status/1710723555476803643\">soft upper bound</a> on the solo staked quantity. It can be argued that if this pool is more or less depleted, then relatively <a href=\"https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751#supply-side-give-me-eth-and-have-my-stake-4\">few of the new stakers</a> will be solo stakers as the supply curve falls and <span class=\"math\">D</span> increases under the current policy. Still, concerning this particular argument, it should be remembered that if the yield is reduced substantially to halt the increase in <span class=\"math\">D</span>, there is no guarantee of retaining a larger proportion of solo stakers in the long run. This ultimately depends on the finer-grained distribution of reservation yields among them.</li>\n</ul>\n<p>Issuance policy should be focused on long-term objectives and not rely on short-term remedies. This also relates to the presently rather valuable solo staking airdrops, if they can be expected to cease. Yet, Ethereum’s evolving consensus mechanism may look different in ten years, with different requirements for staking anyway—there may even be different classes of validators (<a href=\"https://ethresear.ch/t/unbundling-staking-towards-rainbow-staking/18683\">1</a>, <a href=\"https://notes.ethereum.org/bW2PeHdwRWmeYjCgCJJdVA\">2</a>)—so circumstances of the present and in the near-term future cannot be completely ignored. An additional nuance can also be added to the fourth bullet point: some solo stakers stake quite a bit more than 32 ETH, and these may be the most resilient to low yields. Finally note that solo stakers who do not own their hardware may still enjoy some external economies of scale; home stakers on the contrary are directly affected by hardware costs.</p>\n<p>At a fundamental level, the conjecture is that the relative <a href=\"https://x.com/weboftrees/status/1710723376233304391\">distribution of reservation yields may differ between different classes of stakers</a>. This then leads to different proportions of solo stakers under different issuance policies. But whether one policy is better than the other in this respect cannot be ascertained and may also vary over the forthcoming decade. This research topic is certain to receive further attention (<a href=\"https://ethereum-magicians.org/t/electra-issuance-curve-adjustment-proposal/18825/17\">1</a>, <a href=\"https://ethresear.ch/t/initial-analysis-of-stake-distribution/19014\">2</a>).</p>\n<h4><a name=\"h-422-priors-regarding-the-supply-curve-and-rev-25\" class=\"anchor\" href=\"https://ethresear.ch#h-422-priors-regarding-the-supply-curve-and-rev-25\"></a>4.2.2 Priors regarding the supply curve and REV</h4>\n<p>Studying the yield offered at 110M ETH staked in the CDF of staking yield for Option A in Figure 5 may raise particular concerns. The expected staking yield there is only 0.65 %. At a token price of around $3000, a 32 ETH stake would then only generate an expected monthly income just above $50. The “guaranteed” attestation yield not deriving from block proposal or sync-committee duties is only half of that. A decline in the ETH token price could further reduce the income from staking. However, it should be noted that a reasonable prior for the supply curve is that the yield must be quite a bit higher than 0.65 % for 110M ETH to get staked. The only time an equilibrium can be expected at 110M ETH staked is then if the REV increases significantly, pushing up the staking yield to required levels. One easily overlooked benefit of a lower issuance yield at high quantities staked is thus that it pre-emptively counters a higher REV (the staking yield will never go below the marginal staker’s reservation yield under equilibrium). Concerns may of course also be raised around the fact that the quantity of stake is allowed to expand to 110M ETH in the first place, given the good arguments against it. Why offer any yield at all? Besides care for solo stakers, important reasons pertaining to consensus incentives at the present are provided in Section 5.2.</p>\n<h4><a name=\"h-423-low-yield-scenario-26\" class=\"anchor\" href=\"https://ethresear.ch#h-423-low-yield-scenario-26\"></a>4.2.3 Low-yield scenario</h4>\n<p>While it seems probable that a high quantity staked under the proposed reward curve (Option A) would be associated with an elevated level of REV, thus maintaining a slightly higher staking yield, it can still be valuable to examine the alternative scenario. What if the supply curve indeed falls substantially over the next decade to a very low level? Option A gives a staking yield to 1 % at around 74.6M ETH staked under the current level of REV. Option B gives a staking yield of 1.16 % there, and Option C sets it to 1.37 %. But the equilibrium would of course shift to a higher quantity staked for Option B and C, reducing the yield a bit in the process. A hypothetical outcome for Options A-C in a scenario with an equilibrium yield of 1 % for Option A (green circle) is shown in Figure 12. It is of course challenging to speculate on what the supply curve might look like in this unlikely scenario, but it may still be helpful to plot one possible outcome to aid the understanding (it is formed as in the <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-21-supply-and-demand-9\">previous study</a> using <span class=\"math\">k=1</span> and <span class=\"math\">c_2 = 0.001</span>). Option B then gives a yield of 1.09 % at an equilibrium of around 80M ETH staked (orange circle) and Option C gives 1.24 % at around 87M ETH staked (grey circle). Which outcome is better for Ethereum?</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/e/e9411dcd70ebd79f60873997e3ba872f788e4fcf.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/e9411dcd70ebd79f60873997e3ba872f788e4fcf\" title=\"Figure 12\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/e/e9411dcd70ebd79f60873997e3ba872f788e4fcf_2_690x408.jpeg\" alt=\"Figure 12\" data-base62-sha1=\"xhsLeIksj4LMN68EeIcMIxowtRB\" width=\"690\" height=\"408\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/e/e9411dcd70ebd79f60873997e3ba872f788e4fcf_2_690x408.jpeg, https://ethresear.ch/uploads/default/optimized/2X/e/e9411dcd70ebd79f60873997e3ba872f788e4fcf_2_1035x612.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/e/e9411dcd70ebd79f60873997e3ba872f788e4fcf_2_1380x816.jpeg 2x\" data-dominant-color=\"F9F9F9\"></a></div><p></p>\n<p><strong>Figure 12.</strong> Hypothetical equilibria in a scenario with a low supply curve that intersects 1 % staking yield of Option A under the current level of REV.</p>\n<p>If many solo stakers have a reservation yield between 1.24 % and 1 %—in this particular scenario where the majority is staked with reservation yields below 1 %—then they could be disproportionately more likely to drop off. This goes back to the trade-off between the relatively higher fixed costs for solo stakers and the benefits of LSTs at high quantities staked: the notion that dominant SSPs can offer lower fees, better LST money, and lower risk due to emerging moral hazard.</p>\n<p>Ethereum can be designed to enforce an equilibrium at any point along the supply curve (but must be <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-3-consensus-incentives-11\">attentive</a> to the level of <span class=\"math\">y_i</span> relative to <span class=\"math\">y_v</span>). Would enforcing an equilibrium at the blue star be preferable under this supply curve? It corresponds to 30M ETH staked and a total staking yield of around 0.37 %. At present, <span class=\"math\">y_v</span> is 1 %  at 30M ETH staked, so a staking fee would need to be introduced and taken out every epoch. This would require making solo stakers <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-42-effect-of-pooling-14\">lose money every epoch</a>, on the slim chance that they may be assigned to propose a block. That outcome is rather unappealing. But what about after introducing <a href=\"https://ethresear.ch/t/execution-tickets/17944\">MEV burn</a>, would it be desirable at that point? There are several arguments in favor of it (e.g., Section 2), but it could also render solo staking rather disadvantageous due to the relatively higher fixed costs. The effect on the <a href=\"https://notes.ethereum.org/@mikeneuder/set-theoretic-ethereum\">composition</a> of the staking set is harder to predict when enforcing a low quantity staked in such a manner at this point (refer also the next subsection). If the equilibrium yield is 2.6 % at 30M ETH staked, as with the candidate reward curve, then the outcome is far less controversial. An upward shift to the equilibrium quantity of stake—from the blue star to the green circle—<a href=\"https://x.com/weboftrees/status/1754941832595693910\">acts as a “safety valve”</a>, which helps Ethereum neutralize the less desirable properties of a low quantity of stake under a lower supply curve. The consensus mechanism essentially absorbs all delegating stakers with low reservation yields, just to push up the yield enough to allow solo stakers to still operate a 32 ETH validator at a profit. Regardless of whether this is desirable or not, for the sake of users, the upward shift must <em>only</em> take place if strictly required. This is something that the reward curve facilitates.</p>\n<p>It can be interesting to note the difference between the proposed Option A and the current reward curve also under this hypothetical supply curve. The staking yield at the black square in Figure 12 is around 2 % and the incentives to stake thus much higher, drawing 105M ETH into staking. The downsides are the same as previously discussed. Everyone is forced to stake to not see their savings eroded. The issuance level at the black square is around 1708k ETH per year, i.e., almost four times higher than under Option A. Therefore, even though the yield increases by almost 1 %, the supply inflation rate increases even more. Thus, remarkably, stakers are still left worse off under the current reward curve than under Option A, and everyone loses in terms of <span class=\"math\">u'</span>. This follows from the fact that the gain in <span class=\"math\">y_i</span> does not materially surpass the loss from the increased issuance rate <span class=\"math\">i=y_id</span>, where <span class=\"math\">d</span> is the deposit ratio <span class=\"math\">d=D/S</span>. Thus, for an increase in issuance to be profitable at high deposit ratios, the supply curve must slope almost vertically upwards, as further explored in Section 4.4 and Figure 14.</p>\n<p>In the example in Figure 12 with a low supply curve, changing from Option C (grey circle) to Option A (green circle) reduces issuance by around 330k ETH, from around 775k ETH down to 446k ETH. Around 42 % of that reduction corresponds to decreased implied costs <span class=\"math\">Y'_c</span>. The change in utility for ETH holders is shown in Figure 13. Everyone would be better off at the equilibrium of Option A than Option C (disregarding frictions). However, as previously mentioned, this does not mean that solo stakers will keep staking. They could be even better off de-staking, if their reservation yield sits in between 1 % and 1.24 %. From a macro perspective, the overall protocol (including the ETH token holders) could then be worse off if the proportion of solo stakers falls too much. The macro perspective is further interwoven with user utility in Section 6.4.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/c/cf2b713b6293317030e37780d8a7a9498544beb0.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/cf2b713b6293317030e37780d8a7a9498544beb0\" title=\"Figure 13\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/c/cf2b713b6293317030e37780d8a7a9498544beb0_2_690x463.jpeg\" alt=\"Figure 13\" data-base62-sha1=\"tyHUuixYlqw8Xkdr7vSXFVX5Vrq\" width=\"690\" height=\"463\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/c/cf2b713b6293317030e37780d8a7a9498544beb0_2_690x463.jpeg, https://ethresear.ch/uploads/default/optimized/2X/c/cf2b713b6293317030e37780d8a7a9498544beb0_2_1035x694.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/c/cf2b713b6293317030e37780d8a7a9498544beb0_2_1380x926.jpeg 2x\" data-dominant-color=\"F7F6F6\"></a></div><p></p>\n<p><strong>Figure 13.</strong> Isolating the change in utility <span class=\"math\">u'</span> when changing from Option C to Option A under a low supply curve. Stakers are subjected to a reduction in yield <span class=\"math\">y</span>, but the reduction in the inflation rate <span class=\"math\">s</span> is larger, so they are left slightly better off. De-stakers incur no further loss in utility once <span class=\"math\">y</span> falls below their reservation yield. They benefit together with non-stakers from the reduction in <span class=\"math\">s</span>.</p>\n<h3><a name=\"h-43-equilibrium-yield-and-the-broader-composition-of-the-staking-set-27\" class=\"anchor\" href=\"https://ethresear.ch#h-43-equilibrium-yield-and-the-broader-composition-of-the-staking-set-27\"></a>4.3 Equilibrium yield and the broader composition of the staking set</h3>\n<p>The relationship between issuance level and diversity in SSPs also entails a <a href=\"https://twitter.com/weboftrees/status/1710723555476803643\">trade-off</a> between economies of scale and factors such as the positive network externality of the money function. The fact that a positive yield is offered across the full staking range helps alleviate concerns that a major SSP with a structural advantage such as a centralized exchange (CEX)—perhaps leveraging some hypothetical staked ETF—can push up their proportion of the stake to critical levels. At the same time, the risks of centralization around such entities should not be disproportionately emphasized over other risk scenarios. There are already today SSPs attaining critical proportions of the stake by leveraging network externalities onchain, ostensibly foregoing profits in the pursuit of monopolization. This type of structural advantage grows with higher issuance, and stifling that before native ETH possibly is in the minority relative to one LST must be considered as a net positive to the community. When weighing these trade-offs, an issuance level that invariably forces any outcome—whether a very low or very high quantity staked—does not seem desirable. Just as when discussing solo staking, if the supply curve indeed is very low, then it seems acceptable to let the deposit size grow a bit above a level sufficient for security so that some staking yield still exists. If the supply curve turns out to be rather high, then a low deposit size is perfectly fine. No SSP can reasonably outcompete all others if there is an equilibrium at a 2.6 % staking yield and 30M ETH staked (as with Option A).</p>\n<p>Each SSP, reaching for a specific market segment, incurs unique costs besides the cost of running staking nodes, with a wide variety ranging from compliance to software. Indeed, CEXes have somewhat of a local monopoly on their customers-as-delegators, and the opportunity cost of keeping fees competitive with any onchain option is presumably so high that it does not represent the profit-maximizing strategy. What is clear is that competition for delegated stake will unfold across diverse market segments, and perfect competition is not a reasonable assumption. Seeing that the cost of operating a node will not be made prohibitively high relative to the yield, the candidate reward curve (or any of the other options) does not explicitly force a low quantity staked, ultimately allowing variety in preferences and circumstances between delegators to take a more central role in shaping the composition of the staking set. There is therefore no compelling argument as to why any one SSP will overtake others under Option A, B, or C. This is especially pertinent when adopting a graduated approach and evaluating the outcome before proceeding further. There is also already the risk of centralization being more likely under the current reward curve, in this case leveraged by some emerging dominant LST as the money of Ethereum.</p>\n<h3><a name=\"h-44-the-isoproportion-map-28\" class=\"anchor\" href=\"https://ethresear.ch#h-44-the-isoproportion-map-28\"></a>4.4 The isoproportion map</h3>\n<p>Figure 14 maps <span class=\"math\">y_p</span> across <span class=\"math\">y</span> and <span class=\"math\">D</span> under the given <span class=\"math\">b</span> and <span class=\"math\">y_v</span>, using the same hypothetical low supply curve as in Figure 12. The thin black lines can be characterized as “isoproportion” lines across which <span class=\"math\">y_p</span> remains constant. If the slope of the supply curve is steeper than the associated isoproportion line at some specific equilibrium, stakers benefit from an increase in issuance (<span class=\"math\">u'&gt;0</span>); if it is flatter, they are worse off. In microeconomics, a “production possibility curve” can be compared with isoprofit lines to optimize a production set. To the staker, the supply curve becomes a “proportion possibility curve” (the PPC of staking economics), capturing how stakers’ attainable proportion of the circulating supply varies with issuance policy. The dashed blue-white isoproportion line intersects the equilibrium of the current reward curve (square). Since the supply curve in blue (the PPC) has a flatter slope at equilibrium, stakers are worse off from an increase in issuance, and thus benefit from a decrease in issuance. They are therefore better off (higher <span class=\"math\">y_p</span>) at the equilibrium under the proposed reward curve (green circle). At the cross, the PPC reaches its maximum <span class=\"math\">y_p</span>, tangent to the corresponding isoproportion line. This is the optimal equilibrium in terms of <span class=\"math\">y_p</span> from the stakers’ perspective. Everyone else would of course still gain from a reduction in issuance, so in terms of <span class=\"math\">u'</span>, they would certainly prefer an equilibrium at 20M ETH staked (triangle), where stakers are just as well off as at the starting point at the square.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/d/d65b213a7d11568ac572b57cae83a0a25e2a2ec8.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/d65b213a7d11568ac572b57cae83a0a25e2a2ec8\" title=\"Figure 14\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/d/d65b213a7d11568ac572b57cae83a0a25e2a2ec8_2_690x446.jpeg\" alt=\"Figure 14\" data-base62-sha1=\"uAhqfqMgL9qn3Oh5RbdCxoaFbFm\" width=\"690\" height=\"446\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/d/d65b213a7d11568ac572b57cae83a0a25e2a2ec8_2_690x446.jpeg, https://ethresear.ch/uploads/default/optimized/2X/d/d65b213a7d11568ac572b57cae83a0a25e2a2ec8_2_1035x669.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/d/d65b213a7d11568ac572b57cae83a0a25e2a2ec8_2_1380x892.jpeg 2x\" data-dominant-color=\"D68893\"></a></div><p></p>\n<p><strong>Figure 14.</strong> Isoproportion map illustrating the condition for a positive utility change <span class=\"math\">u'</span> for stakers under a change in issuance policy. A change from the current reward curve (black) to the proposed reward curve (Option A) in green is profitable, as the blue-white isoproportion line has a steeper slope than the hypothetical supply curve. The maximum attainable <span class=\"math\">y_p</span> is at the cross, with stakers nominally indifferent between the equilibrium at the square and the triangle.</p>\n<p>The isoproportion lines for any specific <span class=\"math\">y_p</span> follow the equation</p>\n<div class=\"math\">\ny = \\frac{y_p-y_py_vd-y_vd-y_pb-b}{1-d-y_pd},\n</div>\n<p>which thus implies the basic condition for profitability that can be imposed on the supply curve under any equilibrium (e.g., via derivation and analysis of elasticities). If the (inverse) supply curve follows this equation (along any specified <span class=\"math\">y_p</span>), stakers will be indifferent to a change in issuance across the whole deposit ratio range.</p>\n<h2><a name=\"h-5-security-considerations-29\" class=\"anchor\" href=\"https://ethresear.ch#h-5-security-considerations-29\"></a>5. Security considerations</h2>\n<h3><a name=\"h-51-economic-security-30\" class=\"anchor\" href=\"https://ethresear.ch#h-51-economic-security-30\"></a>5.1 Economic security</h3>\n<p>The quantity of stake must be sufficiently high to retain economic security. The almost 14 million ETH at The Merge is the lowest deposit size that has secured Ethereum in the past; a revealed preference to some extent of a quantity that the community found sufficiently secure at the time. Deposit sizes that have been suggested as sufficient for economic security in the past are <a href=\"https://www.reddit.com/r/ethereum/comments/191kke6/comment/kh7do9k/\">15M ETH</a> (12.5 % of the stake), <a href=\"https://x.com/weboftrees/status/1710717547811655955\">24M ETH</a> (20 %), and <a href=\"https://www.reddit.com/r/ethereum/comments/191kke6/comment/kh79gh1\">30M ETH</a> (25 %). Table 2 denotes the cost of 1/3, 1/2 and 2/3 of the stake at a token price of $3000 under relevant deposit sizes, using the condition at The Merge for the lowest example.</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th>1/3</th>\n<th style=\"text-align:center\">1/2</th>\n<th style=\"text-align:center\">2/3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><strong>14M</strong></td>\n<td>$14B</td>\n<td style=\"text-align:center\">$21B</td>\n<td style=\"text-align:center\">$28B</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>24M</strong></td>\n<td>$24B</td>\n<td style=\"text-align:center\">$36B</td>\n<td style=\"text-align:center\">$48B</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>30M</strong></td>\n<td>$30B</td>\n<td style=\"text-align:center\">$45B</td>\n<td style=\"text-align:center\">$60B</td>\n</tr>\n</tbody>\n</table>\n</div><p><strong>Table 2.</strong> Value at stake at a token price of $3000 across various relevant deposit sizes.</p>\n<p>Table 3 shows the yield that the protocol will provide at the previously discussed deposit sizes, given the current level of REV. In addition, the outcome if the REV was to completely vanish is also provided. Note that if Ethereum ever adopts MEV burn and there is a clear need for it, the associated hard fork can apply dedicated adjustments for re-calibrating issuance. On the other hand, should the supply curve fall over the next few years and there is an agreement within the community, then the introduction of MEV burn can be conceived as the last step in the moderation of the quantity staked.</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\"></th>\n<th style=\"text-align:center\">A: <span class=\"math\">y_i</span></th>\n<th style=\"text-align:center\"><span class=\"math\">y_i+y_v</span></th>\n<th style=\"text-align:center\">B: <span class=\"math\">y_i</span></th>\n<th style=\"text-align:center\"><span class=\"math\">y_i+y_v</span></th>\n<th style=\"text-align:center\">C: <span class=\"math\">y_i</span></th>\n<th style=\"text-align:center\"><span class=\"math\">y_i+y_v</span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"><strong>14M</strong></td>\n<td style=\"text-align:center\">3.14 %</td>\n<td style=\"text-align:center\">5.28 %</td>\n<td style=\"text-align:center\">2.83 %</td>\n<td style=\"text-align:center\">4.97 %</td>\n<td style=\"text-align:center\">2.22 %</td>\n<td style=\"text-align:center\">4.37 %</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong>24M</strong></td>\n<td style=\"text-align:center\">1.98 %</td>\n<td style=\"text-align:center\">3.23 %</td>\n<td style=\"text-align:center\">1.88 %</td>\n<td style=\"text-align:center\">3.13 %</td>\n<td style=\"text-align:center\">1.70 %</td>\n<td style=\"text-align:center\">2.95 %</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong>30M</strong></td>\n<td style=\"text-align:center\">1.60 %</td>\n<td style=\"text-align:center\">2.60 %</td>\n<td style=\"text-align:center\">1.58 %</td>\n<td style=\"text-align:center\">2.58 %</td>\n<td style=\"text-align:center\">1.52 %</td>\n<td style=\"text-align:center\">2.52 %</td>\n</tr>\n</tbody>\n</table>\n</div><p><strong>Table 3.</strong> Staking yield at various relevant deposit sizes under the full reduction in issuance.</p>\n<p>The first column indicates that the quantity staked will stay at healthy levels under Option A, even without REV. It seems reasonable to assume that at least 24M ETH would still be staked at a 2 % yield. Certainly, 14M ETH would still be staked if a 3.1 % yield is offered. The lower yield under Option C can be noted, and is one of the reasons why a base reward factor below <span class=\"math\">F=32</span> is undesirable from a security perspective for this option. Table 4 instead shows the outcome under a graduated approach.</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\"></th>\n<th style=\"text-align:center\">Ag: <span class=\"math\">y_i</span></th>\n<th style=\"text-align:center\"><span class=\"math\">y_i+y_v</span></th>\n<th style=\"text-align:center\">Bg: <span class=\"math\">y_i</span></th>\n<th style=\"text-align:center\"><span class=\"math\">y_i+y_v</span></th>\n<th style=\"text-align:center\">Cg: <span class=\"math\">y_i</span></th>\n<th style=\"text-align:center\"><span class=\"math\">y_i+y_v</span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"><strong>14M</strong></td>\n<td style=\"text-align:center\">3.68 %</td>\n<td style=\"text-align:center\">5.82 %</td>\n<td style=\"text-align:center\">3.53 %</td>\n<td style=\"text-align:center\">5.67 %</td>\n<td style=\"text-align:center\">3.06 %</td>\n<td style=\"text-align:center\">5.20 %</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong>24M</strong></td>\n<td style=\"text-align:center\">2.50 %</td>\n<td style=\"text-align:center\">3.75 %</td>\n<td style=\"text-align:center\">2.46 %</td>\n<td style=\"text-align:center\">3.70 %</td>\n<td style=\"text-align:center\">2.33 %</td>\n<td style=\"text-align:center\">3.58 %</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong>30M</strong></td>\n<td style=\"text-align:center\">2.10 %</td>\n<td style=\"text-align:center\">3.10 %</td>\n<td style=\"text-align:center\">2.10 %</td>\n<td style=\"text-align:center\">3.10 %</td>\n<td style=\"text-align:center\">2.09 %</td>\n<td style=\"text-align:center\">3.09 %</td>\n</tr>\n</tbody>\n</table>\n</div><p><strong>Table 4.</strong> Staking yield at various relevant deposit sizes under a graduated approach.</p>\n<h3><a name=\"h-52-consensus-incentives-31\" class=\"anchor\" href=\"https://ethresear.ch#h-52-consensus-incentives-31\"></a>5.2 Consensus incentives</h3>\n<p>A reduction in issuance will alter the balance between the various roles that consensus participants are assigned to. In particular, the proposer will attain a larger proportion of all rewards, and this may <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-3-consensus-incentives-11\">threaten consensus</a> stability. If the yield provided for attestation duties <span class=\"math\">y_a</span> becomes a small proportion, <span class=\"math\">y_a/y</span>, of the staking yield, stakers can be expected to pursue irregular and adverse activities when profit-maximizing, and the consensus process may break down. They may as an example even <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448/2\">stop attesting entirely to avoid the risk of slashing</a>. The proposed reward curve has been designed to give attesters at least half of the rewards at any deposit size under the current level of REV, as illustrated in Figure 15. Should the REV rise substantially, the penalty for a missed target vote (and potentially source vote) can be <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448/2\">raised</a>, to give proper incentives for performing attester duties correctly.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/b/b2ece383e5859173dcd215d2c254e1b0f45dfb5d.png\" data-download-href=\"https://ethresear.ch/uploads/default/b2ece383e5859173dcd215d2c254e1b0f45dfb5d\" title=\"Figure 15\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/b/b2ece383e5859173dcd215d2c254e1b0f45dfb5d_2_690x432.png\" alt=\"Figure 15\" data-base62-sha1=\"pwQwkFvl5FwUw7ZWfoP2HsxkVAh\" width=\"690\" height=\"432\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/b/b2ece383e5859173dcd215d2c254e1b0f45dfb5d_2_690x432.png, https://ethresear.ch/uploads/default/optimized/2X/b/b2ece383e5859173dcd215d2c254e1b0f45dfb5d_2_1035x648.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/b/b2ece383e5859173dcd215d2c254e1b0f45dfb5d_2_1380x864.png 2x\" data-dominant-color=\"FAFAF9\"></a></div><p></p>\n<p><strong>Figure 15.</strong> Proportion of the staking yield provided for attestations (<span class=\"math\">y_a/y</span>) under 300k ETH REV/year for the analyzed options (graduated approach in dashed lines).</p>\n<h3><a name=\"h-53-discouragement-attacks-32\" class=\"anchor\" href=\"https://ethresear.ch#h-53-discouragement-attacks-32\"></a>5.3 Discouragement attacks</h3>\n<p>A discouragement attack is a malicious action against honest consensus participants of a blockchain, potentially at a cost for the attacker, to profit from the reduced competition for the remaining rewards. The traditional scenario <a href=\"https://github.com/ethereum/research/blob/09d9f34042262c8fb436171786ed6c62e1f57247/papers/discouragement/discouragement.pdf\">outlined by Buterin</a> involves a majority attack, but there are currently a few possible <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448/11\">minority discouragement attacks</a> against Ethereum. These include the censorship of sync-committee attestations, withheld attestations during an inactivity leak, and censorship of the head and potentially source vote (picking up stray votes in subsequent slots). Withheld attestations during an inactivity leak are a special case, because the attack is directly profitable. Since the victim loses out on seven times more ETH than the attacker and also receives a penalty of equal size as its loss, censorship of sync-committee attestations has a griefing factor of <span class=\"math\">G=14</span>.</p>\n<p>The “<span class=\"math\">p</span>-elasticity”—capturing the negated inverse point-wise yield-elasticity of demand across a reward curve—is a relevant macro measure for examining its susceptibility to discouragement attacks. If the <span class=\"math\">p</span>-elasticity is high, then attacks become (more) profitable. Specifically, for a small epsilon attack, the condition for profitability is</p>\n<div class=\"math\">\na+Ga-1 &gt; \\frac{q}{p},\n</div>\n<p>where <span class=\"math\">G</span> is the griefing factor, <span class=\"math\">a</span> the proportion of stake held by an attacker, and <span class=\"math\">q</span> the point-wise inverse yield-elasticity of supply. This simplified expression does not say anything about the level of the profits, which may very well be small. The margin of this post is too narrow for a complete exposition of this complex topic, which is forthcoming. The purpose of this simplification is to encapsulate that as <span class=\"math\">p</span> rises, the required griefing factor or proportion of stake held by an attacker falls. The <span class=\"math\">p_i</span>-elasticity computed only across issuance yield can be determined by relating the percentage change in deposit size <span class=\"math\">\\Delta D/D</span> to a percentage change in issuance yield across the demand curve <span class=\"math\">\\Delta y_i/y_i</span></p>\n<div class=\"math\">\np_i = -\\frac{\\Delta y_i/y_i}{\\Delta D/D}.\n</div>\n<p>The <span class=\"math\">p_i</span>-elasticity can be used as a reference point, because if it is modest, then the <span class=\"math\">p</span>-elasticity will be modest regardless of the size of the REV. Figure 16 plots <span class=\"math\">p_i</span>-elasticity across deposit size. For Option A, the <span class=\"math\">p_i</span>-elasticity rises smoothly, reaching a maximum slightly below 1.3 at 120M ETH staked. Lower is better from the perspective of discouragement attacks and cartelization attacks (described in Section 5.4), but a bit above 1 should be perfectly acceptable). Option B has <span class=\"math\">p_i&lt;1</span> since issuance never falls and Option C is identical to the current reward curve across deposit size. Note that for reward curves where the yield goes negative, the <span class=\"math\">p_i</span>-elasticity (and eventually the <span class=\"math\">p</span>-elasticity if the yield continues falling) goes to infinity (see <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-54-additional-properties-under-consideration-22\">Figure 40 in a previous post</a> showing the <span class=\"math\">p_i</span>-elasticity for targeting approaches). The equilibrium <span class=\"math\">p</span>-elasticity, and the profitability of discouragement attacks, will however also always depend on the supply curve.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/8/887ae886834a019a14ec9ba8f95528f0788b8063.png\" data-download-href=\"https://ethresear.ch/uploads/default/887ae886834a019a14ec9ba8f95528f0788b8063\" title=\"Figure 16\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/8/887ae886834a019a14ec9ba8f95528f0788b8063_2_690x418.png\" alt=\"Figure 16\" data-base62-sha1=\"jtmgtKHR0sO1pkCHUK07VcLKq55\" width=\"690\" height=\"418\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/8/887ae886834a019a14ec9ba8f95528f0788b8063_2_690x418.png, https://ethresear.ch/uploads/default/optimized/2X/8/887ae886834a019a14ec9ba8f95528f0788b8063_2_1035x627.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/8/887ae886834a019a14ec9ba8f95528f0788b8063_2_1380x836.png 2x\"></a></div><p></p>\n<p><strong>Figure 16.</strong> The point-wise negated inverse yield-elasticity of demand (<span class=\"math\">p_i</span>-elasticity) for the three reward curves examined in this post (lower is better).</p>\n<p>The best defense against a minority discouragement attack is to have a well-balanced consensus mechanism. Ethereum should therefore in the future fix the current incentives that allow for infinite profit margins during an inactivity leak and for attacks with <span class=\"math\">G=14</span>. The second best defense, particularly useful against larger attackers (including majorities), is some form of social intervention. The prospect of social intervention can make the risk-reward ratio rather unfavorable for discouragement attacks. The immediate gain is rather low (in fact negative in most cases), and profits vary with frictions in the decision to stake or unstake. Finally, note that the prospect of being able to rely on social intervention in the first place could also depend on quantity staked, as outlined in Section 2.2, making the problem multi-dimensional. To balance the various aspects discussed in this subsection, it is desirable with a reward curve that gives appropriate guarantees for a sufficient quantity staked without providing excessive levels of issuance, and then reduces the incentive to stake across a broad range, for example by gradually attenuating issuance. This was one of the design rationales for the candidate reward curve, Option A.</p>\n<h3><a name=\"h-54-cartelization-attacks-33\" class=\"anchor\" href=\"https://ethresear.ch#h-54-cartelization-attacks-33\"></a>5.4 Cartelization attacks</h3>\n<p>Cartelization attacks are hypothetical constructs related to and overlapping with discouragement attacks, that can differ in rationalization and execution. They will here be presented briefly. A cartelization attack consists of SSPs working together to try to reduce quantity staked both among themselves and others. When issuance increases with a reduction in quantity staked (<span class=\"math\">p&gt;1</span>), all stakers could try to agree to reduce their stake, and everyone would be better off. This framing can provide a suitable backdrop for convincing the social layer to not interfere when a staking cartel tries to inhibit Ethereum’s permissionlessness. Ideally for the cartel, it would be able to rely on some permissioned revenue outside of the consensus mechanism that a majority can extract to incentivize initial participation. However, if insufficient (in particular once the yield has risen), the cartel could resort to more sinister actions within the consensus mechanism. The actions need not completely break consensus formation to be effective. Modest discouragement attacks against “strikebreakers” to ensure that such entities “stop reducing everyone’s rewards” could be sufficient.</p>\n<p>If the reward curve is too deliberate about targeting some specific quantity staked, but still provides high issuance at lower quantities, then stakers may face a situation where cartelization and a reduction in quantity staked could lead to multiple times higher rewards for everyone. Option A has been designed to incorporate a sufficiently small issuance reduction as <span class=\"math\">D</span> rises, so that the rationale for executing a cartelization attack is minimal/non-existent.</p>\n<h2><a name=\"h-6-conclusion-and-discussion-34\" class=\"anchor\" href=\"https://ethresear.ch#h-6-conclusion-and-discussion-34\"></a>6. Conclusion and discussion</h2>\n<h3><a name=\"h-61-factors-influencing-the-optimal-shape-of-the-reward-curve-35\" class=\"anchor\" href=\"https://ethresear.ch#h-61-factors-influencing-the-optimal-shape-of-the-reward-curve-35\"></a>6.1 Factors influencing the optimal shape of the reward curve</h3>\n<p>Ethereum must weigh many factors when deciding on a reward curve. A low issuance level:</p>\n<ul>\n<li>improves aggregate utility by not compelling users to incur unnecessary costs. This is of fundamental importance, and it is the reason for why every ETH holder can benefit from a reduced issuance level.</li>\n<li>keeps the quantity staked moderate. This reduces:\n<ul>\n<li>the risks posed by a compromised social layer,</li>\n<li>network externalities that entrench dominant SSPs,</li>\n<li>economies of scale of dominant SSPs.</li>\n</ul>\n</li>\n</ul>\n<p>However, an excessively low issuance level might sideline solo (home) stakers down the line, if the yield is reduced so much that staking on a reasonably efficient setup becomes unprofitable. Retention of present solo stakers will hinge on the distribution of reservation yields of solo stakers, accounting for scenarios where initial exogenous incentives for solo staking (e.g., airdrops) have dried out and initial hardware investments have reached their depreciation horizon.</p>\n<p>From a security perspective, it is beneficial to keep the yield rather high at low quantities staked to ensure a sufficient quantity of stake and economic security. But to protect against discouragement attacks and cartelization attacks, it is also beneficial if issuance does not then fall too sharply at any point, i.e., it is best if the <span class=\"math\">p_i</span>-elasticity is kept rather moderate. This therefore also becomes a balancing act, where it is best that issuance is not too high and not too low at any deposit size.</p>\n<p>Additional complexities arise from MEV captured by block proposers:</p>\n<ul>\n<li>It raises the expected yield, so a lower issuance is required to enforce the same quantity staked.</li>\n<li>But issuance cannot be reduced too much without compromising consensus incentives under the current specification. Resolutions such as a staking fee could further debilitate solo staking, relative to delegated staking.</li>\n<li>It raises variability in the expected equilibrium yield at a macro level, complicating the effort to achieve any specific desirable equilibrium.</li>\n<li>It raises variability in the expected yield of individual solo stakers, leading to some utility degradation if the issuance is set too low.</li>\n</ul>\n<h3><a name=\"h-62-the-optimal-reward-curve-36\" class=\"anchor\" href=\"https://ethresear.ch#h-62-the-optimal-reward-curve-36\"></a>6.2 The optimal reward curve</h3>\n<h4><a name=\"h-621-option-a-the-best-alternative-for-ethereum-37\" class=\"anchor\" href=\"https://ethresear.ch#h-621-option-a-the-best-alternative-for-ethereum-37\"></a>6.2.1 Option A – the best alternative for Ethereum</h4>\n<p>The candidate reward curve, Option A, was designed to effectively moderate issuance while allowing Ethereum to retain proper consensus incentives and keep solo-staking variability acceptable. Section 2.1 highlighted the undeniable benefits of minimum viable issuance—the reduction in implied costs to Ethereum’s users and the associated aggregate utility gain, which may very well make everyone better off under equilibrium. Section 2.2 further strengthened the case by deliberating on the clear benefits of a neutral social layer that can ensure that Ethereum operates under the intended consensus process. Option A will best attain these benefits, and it was in a <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448\">previous study</a> selected over even stricter issuance policies because of its balanced approach to various relevant trade-offs. It is not clear which issuance level that will maximize the proportion of solo stakers (and the outcome can vary with time scale considered), with arguments in both directions reviewed in Section 4.2. Importantly, a low quantity of stake is not strictly forced, ensuring that the cost of running a staking node will not be prohibitively high relative to the yield. This ultimately allows variety in preferences and circumstances between stakers to take a more central role in shaping the composition of the staking set. The proposed reward curve also ensures economic security while being sufficiently resilient against discouragement attacks and cartelization attacks, retaining an intact social layer by a gradual attenuation of issuance with modest <span class=\"math\">p_i</span>-elasticities.</p>\n<p>Another benefit of Option A is its ease of interpretation: a single adjustable variable defines both the peak issuance point as well as the point where issuance is halved relative to the current reward curve. As noted in Section 2.3., compromising the predictive capacity of economic agents undermines welfare. Therefore, a graduated reduction of issuance is preferable when possible, allowing for gradual adaptation. This also accommodates reevaluation against uncertain future stake growth and solo-staking retention scenarios.</p>\n<p><em>In conclusion, the author believes that a holistic approach to issuance policy makes Option A—the candidate reward curve—the most suitable reward curve for Ethereum:</em></p>\n<div class=\"math\">\ny_i=\\frac{cF}{\\sqrt{D}(1+D/k)}.\n</div>\n<p>A graduated approach of <span class=\"math\">k=2^{26}</span> is preferable for the next hard fork, with a full reduction constituting <span class=\"math\">k=2^{25}</span>. The exact setting of the graduated approach should be determined based on circumstances at the time of the decision, and alternatives such as setting <span class=\"math\">k</span> to <span class=\"math\">2^{25}</span>, <span class=\"math\">2^{27}</span> or <span class=\"math\">10^{8}</span> at the next hard fork cannot definitely be ruled out at this point.</p>\n<h4><a name=\"h-622-option-c-the-backup-38\" class=\"anchor\" href=\"https://ethresear.ch#h-622-option-c-the-backup-38\"></a>6.2.2 Option C – the backup</h4>\n<p>The second best option is to reduce the base reward factor, i.e., Option C. Two things can be noted: (1) the consensus specification change is even more minimal, (2) the guaranteed yield for (solo) stakers is higher. Of course, to many, (2) would instead be considered a drawback, since it will lead to a higher equilibrium quantity of stake. Even if the endgame is Option A, the graduated first step can still be taken with Option C. But it would certainly be preferable to proceed with Option A at this point, giving the community a clear path and roadmap. Option B, perhaps also with <span class=\"math\">F=64</span>, is a compromise between A and C, and can be called upon if agreement cannot be made between them.</p>\n<h3><a name=\"h-63-the-unknown-endgame-39\" class=\"anchor\" href=\"https://ethresear.ch#h-63-the-unknown-endgame-39\"></a>6.3 The unknown endgame</h3>\n<p>The optimal “endgame” of Ethereum staking economics is at this point unknown, and will remain so, probably for at least another decade. It depends on many aspects that have currently not been settled, one of them being the specification of the endgame consensus mechanism itself.</p>\n<p>There are however reasons to believe that the proposed reward curve can be viable for a long time, perhaps forever. For example, consider the scenario where the community feels that it is important to temper the quantity staked. The setting can then be kept at <span class=\"math\">k=2^{25}</span> when instigating MEV burn (<span class=\"math\">k=2^{26}</span> would bring the staking yield back closer to pre-MEV burn levels). At <span class=\"math\">k=2^{25}</span>, the staking yield would indeed become rather low (see the lime-colored curve in Figure 33 of <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-52-exploration-of-alternative-reward-curves-20\">this post</a>). As an example, the issuance yield is 0.77 % at 60M ETH staked. When accounting for risks (slashing, smart contracts, governance, etc.), and other costs of staking, it seems rather unlikely that the quantity staked would be pushed above 60M ETH at such a low yield. Yet, even if this turns out to hold true, it should not be interpreted as arguing that the proposed reward curve necessarily <em>must</em> remain in place indefinitely—just that it seems rather suitable as an endgame and offers clear improvement relative to the current reward curve. The optimal quantity staked ultimately depends on the shape of the supply curve (representing possible attainable yield–quantity combinations), and how the relative importance of the various trade-offs that Ethereum balances will evolve in the future.</p>\n<p>There is one thing that certainly remains for the endgame even after adopting the proposed reward curve. The long-run staking equilibrium under reward curves that adapt to <span class=\"math\">D</span> instead of <span class=\"math\">d</span> is ultimately also influenced by the <a href=\"https://ethresear.ch/t/circulating-supply-equilibrium-for-ethereum-and-minimum-viable-issuance-during-the-proof-of-stake-era/10954\">circulating supply equilibrium</a>, since the circulating supply will <a href=\"https://www.youtube.com/watch?v=LtEMabS0Oas&amp;t=1187s\">drift</a> to <a href=\"https://twitter.com/weboftrees/status/1710725744651825281\">balance</a> supply, demand, and protocol income. Therefore, Ethereum should transition to using <span class=\"math\">d</span> in the equation for the reward curve, which can simply be done by <a href=\"https://x.com/weboftrees/status/1710728179260731715\">swapping in the circulating supply</a>, once it <a href=\"https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751#how-to-set-the-target-in-relative-staking-ratio-instead-of-absolute-fixed-eth-amount-terms-20\">begins to be tracked</a>.</p>\n<p>The benefits of not issuing more tokens than what is <a href=\"https://notes.ethereum.org/@anderselowsson/MinimumViableIssuance\">strictly needed for security</a> are indeed clear and substantial. Schwarz-Shilling and Dietrich <a href=\"https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751\">present benefits and argue</a> for an endgame of targeting a low quantity staked, potentially through <a href=\"https://notes.ethereum.org/@vbuterin/single_slot_finality#Economic-capping-of-total-deposits\">economic capping</a> (a yield that goes to negative infinity when too much ETH is staked). When it comes to solo staking, the analysis looks at a hypothetical <a href=\"https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751#implications-of-targeting-14\">scenario</a> where the reservation yield is 1.5 % at 30M ETH staked and 2 % at 120M ETH staked. Under a flat supply curve with high reservation yields, stricter targeting (a steeper reward curve) at low quantities staked indeed seems rather straightforward. Solo staking will with such supply curves always be viable, regardless of the shape of the reward curve. Under the supply curve of Figure 12, a strict targeting approach could instead force outcomes that may be unfavorable to the composition of the staking set. Priors regarding the possible shapes of the supply curve here matter. Discouragement attacks and cartelization attacks become more <a href=\"https://notes.ethereum.org/@vbuterin/single_slot_finality#Economic-capping-of-total-deposits\">viable with targeting approaches</a> where the equilibrium <span class=\"math\">p</span>-elasticity is pushed up too high. To solo stakers, uncertainty itself regarding if the yield will go negative may make them hold off on investing in hardware. Indeed, certainty pertaining to either yield or quantity staked is always substituted via the shape of the reward curve. A flatter demand curve will smooth out fluctuations in yield, and can therefore decrease the equilibrium yield, ceteris paribus (at the same quantity staked), since economic agents may be willing to substitute lower variability for lower returns. But a steeper demand curve instead gives smaller fluctuations in quantity staked, and better assurances concerning its maximum level. The author has gradually changed position, from favoring strict deposit ratio targeting in <a href=\"https://ethresear.ch/t/circulating-supply-equilibrium-for-ethereum-and-minimum-viable-issuance-during-the-proof-of-stake-era/10954\">2021</a>, to gradually focusing on targeting a desirable <a href=\"https://x.com/weboftrees/status/1585641193982918656\">deposit ratio range</a> (where the range of plausible equilibria can be rather broad to balance the trade-offs that exist), and settling on the proposed reward curve as the correct mechanism for balancing current circumstances pertaining to MEV, with the implementation of MEV burn allowing the same reward curve to transition into a viable endgame policy. Yet, it is important to remain open to changes when new information comes in over the next decade, updating priors, and reflecting on evolving trade-offs.</p>\n<p>Finally note that there is no direct monetary purpose for staking when the <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-21-supply-and-demand-9\">endogenous yield is 0</a> or negative. Therefore, a negative issuance yield should only ever be needed when MEV or, e.g., rewards for <a href=\"https://notes.ethereum.org/WLuNFaliQiqw7Zhd-7AnmQ\">preconfirmations</a> befall the proposer. Thus, in all current scenarios with a negative issuance yield, unpooled solo stakers would be forced to accept negative payouts each epoch while waiting on proposer assignment, while delegating stakers could reap small positive rewards at a regular basis. When it comes to negative issuance yields, the most interesting option would be very gradual transitions at particularly undesirable levels of stake, e.g., subtracting <span class=\"math\">(D/k)^p</span> from current constructions for <span class=\"math\">y_i</span>, with <span class=\"math\">p</span> set to 0.5, 1 or 2. Reward curves asymptotically approaching 0 are in this case however perhaps more viable as an option. The optimal shape will ultimately depend on how the consensus mechanism itself develops (e.g., the role of solo stakers). Constructions with an infinitely negative issuance yield (which may technically be unreachable due to the churn limit) will however bring even more uncertainty to stakers and may bring unneeded complexity into the protocol design space.</p>\n<p>Adding the dimension of <em>time</em> to the dimension of <em>quantity</em> is also worthy of consideration. Under a time–quantity policy, the reward curve is kept moderate in terms of elasticity, but the whole curve is allowed to <a href=\"https://x.com/weboftrees/status/1710728448543523219\">gradually adapt</a> to changes in the supply curve, implied by the equilibrium. Thus, under the proposed reward curve, <span class=\"math\">k</span> drifts when <span class=\"math\">d</span> ventures off desirable levels, with effects taking place at a time scale of decades. This would allow Ethereum to account for the supply curve when setting the reward curve. The mechanism would smooth out fluctuations in yield, and the delayed adjustment could make some discouragement attacks and cartelization attacks less attractive. Certain limits could be placed on how small <span class=\"math\">k</span> can become with this strategy, to avoid idiosyncratic outcomes. There is also a distinction here between automatically increasing the yield (opening up for discouragement attacks) and decreasing it (opening up for the familiar issues associated with low yields). Interestingly, while the protocol may not be able to determine who is at fault in a discouragement attack, it can still determine that it is in fact under attack, providing an avenue for safeguards through conditional logic.</p>\n<h3><a name=\"h-64-the-proportion-that-matters-to-eth-holders-40\" class=\"anchor\" href=\"https://ethresear.ch#h-64-the-proportion-that-matters-to-eth-holders-40\"></a>6.4 The proportion that matters to ETH holders</h3>\n<p>The insights presented in Sections 2.1, 2.2, and 4 have implications for Ethereum’s users, irrespective of if they hold significant amounts of ETH. Having a viable social layer—uncorrupted by any SSP—ultimately matters to credible neutrality. Having solo stakers matters, and having many viable options for delegated staking matters. These things also matter specifically to the ETH token holder. It is ultimately not the “proportional yield”, but rather the proportion of the world economy powered by Ethereum that will affect the ETH token holder the most, including the staker. If an issuance policy degrades Ethereum, there is little point in having maximized <span class=\"math\">y_p</span>, certainly not to users that do not hold ETH, and not even to ETH token holders, because the native token of a blockchain that has been degraded is not as valuable. The <em>real</em> “real yield” incorporates the change in value of the underlying ETH—including any staking yield—relative to a relevant consumer price index. However, it just so happens that it is indeed very useful to have sound native money in an economic system. And it so happens that the best sound money will not encumber its users to research the reliability of various SSPs, track staking income and see it taxed, or risk being wiped out in a slashing event or other failure.</p>\n<h3><a name=\"h-65-a-note-on-bounded-rationality-and-yield-41\" class=\"anchor\" href=\"https://ethresear.ch#h-65-a-note-on-bounded-rationality-and-yield-41\"></a>6.5 A note on bounded rationality and yield</h3>\n<p>The notion of <a href=\"https://en.wikipedia.org/wiki/Behavioral_economics#Bounded_rationality\">bounded rationality</a> helps to explain why some users may prefer a scenario with lower <span class=\"math\">y_p</span>, where the yield is raised but the supply inflation rate (which users have a harder time to track) rises even more. Behavioral economics can be helpful in this way for explaining why some economic agents act irrationally, perhaps for lack of education or time to deliberate on a topic. Recently, a twisted argument relying on bounded rationality has appeared in the writings of several representatives of SSPs or investors thereof. In essence, proponents suggest that Ethereum should harness people’s bounded understanding of yield, and keep it high to trick users into thinking that they gain something, when they do in fact not. Instead of educating people on these matters, user utility should be degraded for some short-term gain, which is unlikely to materialize. Such strategies must be avoided.</p>\n<h3><a name=\"h-66-next-step-42\" class=\"anchor\" href=\"https://ethresear.ch#h-66-next-step-42\"></a>6.6 Next step</h3>\n<p>The author hopes that this post has convinced the community of the benefits of adjusting the issuance policy, and is available for any questions you may have. The post will act as the long-form reference for the EIP on the proposed reward curve—Option A, which is to be made available next week. A graduated approach seems favorable, with a decision to be made at the time of inclusion. The most important thing right now is to take a step in the right direction.</p>\n            <p><small>3 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171\">Read full topic</a></p>","link":"https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171","pubDate":"Mon, 01 Apr 2024 08:39:26 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19171"},"source":{"@url":"https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171.rss","#text":"Reward curve with tempered issuance: EIP research post"},"filter":false},{"title":"Reduced Attestation Format for gossiping attestations","dc:creator":"Giulio2002","category":"Networking","description":"<p>At the current state, even after Deneb and the introduction of blobs, Attestations are responsible for 90% of the traffic on the Consensus Layer’s gossip network. Right now, the <code>Attestation</code> propagation format is the same as the one used during internal operation within the Consensus Client. which is as follows:</p>\n<pre><code class=\"lang-python\">class Attestation(Container):\n    aggregation_bits: Bitlist[MAX_VALIDATORS_PER_COMMITTEE]\n    data: AttestationData\n    signature: BLSSignature\n</code></pre>\n<p>where AttestationData is just:</p>\n<pre><code class=\"lang-python\">class AttestationData(Container):\n    slot: Slot\n    index: CommitteeIndex\n    # LMD GHOST vote\n    beacon_block_root: Root\n    # FFG vote\n    source: Checkpoint\n    target: Checkpoint\n```.\n\nThis format totals a maximum of 256 bytes for each attestation (assuming that `len(aggregation_bits)=32`).\n\nHowever, it could be possible to reduce this size by at least a 15ish% by defining a different format for `AttestationData`, which could be as follows:\n\n```python\nclass NetworkAttestationData(Container):\n    slot: Slot\n    index: CommitteeIndex\n    # LMD GHOST vote\n    beacon_block_root: Root\n    # FFG vote\n    source_digest: BytesVector8\n    target_digest: BytesVector8\n</code></pre>\n<p>where source_digest = source. HashTreeRoot () [0:8] and target_digest = target. HashTreeRoot () [0:8]. This format requires clients to keep track of an internal map of possible <code>CheckpointDigest</code> (<code>BytesVector8</code>) as they sync, and maps them to their equivalent <code>Checkpoint</code>. This reduction would save 64 bytes per attestation which is equivalent to &gt;25ish% of a single attestation. If this is also applied to <code>aggregates and proofs</code>, it should also decrease the bandwidth for the average node as well. Potentially, bandwidth-wise, something like this can allow for a doubling of the current Blob per block. A consideration to make is that perhaps <code>BytesVector8</code> as a digest size might open up for possible collision attacks. However, an attacker needs to do so in the 12 seconds of slot time to create a collision with 2 recent <code>CheckpointDigests</code>. Probably <code>16</code> is a better size for something like this (but I am not a cryptographer). this would also require that at each block, each consensus client would need to generate some digests for some potential future checkpoints as well, e.g., generates 4 checkpoints for block root <code>0xabcd</code> spanning 4 epochs into the future.</p>\n            <p><small>2 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/reduced-attestation-format-for-gossiping-attestations/19157\">Read full topic</a></p>","link":"https://ethresear.ch/t/reduced-attestation-format-for-gossiping-attestations/19157","pubDate":"Sun, 31 Mar 2024 08:18:28 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19157"},"source":{"@url":"https://ethresear.ch/t/reduced-attestation-format-for-gossiping-attestations/19157.rss","#text":"Reduced Attestation Format for gossiping attestations"},"filter":false},{"title":"Blob Preconfirmations with Inclusion Lists to Mitigate Blob Contention and Censorship","dc:creator":"chrmatt","category":"Economics","description":"<p>Thanks to my collaborators <a class=\"mention\" href=\"https://ethresear.ch/u/murat\">@murat</a>, <a class=\"mention\" href=\"https://ethresear.ch/u/ckartik\">@ckartik</a>, <a class=\"mention\" href=\"https://ethresear.ch/u/evan-kim2028\">@Evan-Kim2028</a>, and <a class=\"mention\" href=\"https://ethresear.ch/u/bemagri\">@bemagri</a>. Further thanks to the ETHGlobal London community for early feedback and in particular Nethermind for awarding a prize at the hackathon for our credible preconfirmation leader auction.</p>\n<h1><a name=\"introduction-1\" class=\"anchor\" href=\"https://ethresear.ch#introduction-1\"></a>Introduction</h1>\n<p>In this post, we describe an out-of-protocol mechanism for blob inclusion preconfirmations. It allows preconfirmation providers to bid in an auction to become the leader for the subsequent slot. The auction winner can then accept bids on blob inclusions and issue preconfirmations to the bidders. The preconfirmed blobs then act as an inclusion list that needs to be respected by all participants. The goals of this design are to enable L2 sequencers and other entities relying on blob inclusion to gain certainty about their inclusion, prevent censorship, and possibly mitigate latency issues of blocks with many blobs in the future. Our out-of-protocol design can be implemented right now and serve as a starting point to measure and understand blob inclusion lists in practice until they are available natively in the Ethereum protocol.</p>\n<p>Given recent issues with blob contention [1,2], we believe it is timely to implement an out-of-protocol solution as the one described here.</p>\n<p>We next provide an overview of the entities involved in the protocol and then describe the protocol in more detail. We then provide more details on the payment and slashing mechanism. We conclude with discussions on the <span class=\"math\">N+1</span> slot design, the leader auction, and how to handle premature blob inclusion.</p>\n<h1><a name=\"actors-2\" class=\"anchor\" href=\"https://ethresear.ch#actors-2\"></a>Actors</h1>\n<p>We assume there is a sidechain (with a faster block time than L1) that is used to settle payments among the protocol participants. This chain in the following is called <em>mev-commit chain</em>.</p>\n<p>The following actors participate in the protocol:</p>\n<ul>\n<li>L1 proposers. We assume a subset of L1 validators opt-in to participate in the protocol and stake a collateral on the mev-commit chain.</li>\n<li>Relays. The opted-in proposers need to exclusively work with relays that participate in the protocol and only forward blocks from builders that also participate in the protocol.</li>\n<li>Preconfirmation providers. These are the actors bidding to become the blob preconfirmation leaders and issue the blob inclusion lists. They could be the proposers themselves, builders, or relays. For this write-up, we assume the relays play the role of preconfirmation providers.</li>\n<li>Preconfirmation bidders. The entities who want blobs to be included in a block, such as L2 sequencers.</li>\n<li>Builders. The builders need to be aware of the blob inclusion lists and include the corresponding blobs in their blocks.</li>\n</ul>\n<h1><a name=\"protocol-description-3\" class=\"anchor\" href=\"https://ethresear.ch#protocol-description-3\"></a>Protocol Description</h1>\n<p>We next describe the protocol execution in more detail. Let <span class=\"math\">N</span> be the current L1 slot and consider preconfirmation bidders want to include blobs in a block by slot <span class=\"math\">N+1</span> (see below for a justification of the one-slot delay).</p>\n<p>The protocol description is divided into three steps. The first is done during slot <span class=\"math\">N</span>, the second at the beginning of slot <span class=\"math\">N+1</span>, and the last one during the rest of slot <span class=\"math\">N+1</span>. See also the figures below for graphical overviews of the corresponding steps.</p>\n<h2><a name=\"protocol-execution-in-slot-n-4\" class=\"anchor\" href=\"https://ethresear.ch#protocol-execution-in-slot-n-4\"></a>Protocol Execution in Slot <span class=\"math\">N</span></h2>\n<ol>\n<li>During L1‘s slot <span class=\"math\">N</span>, preconfirmation bidders send their bids to the providers. A bid contains the hash of the KZG commitment of the blob, the bid amount, and the target slot <span class=\"math\">N+1</span>, and is signed by the bidder.</li>\n<li>Preconfirmation providers collect preconfirmation bids during L1 slot <span class=\"math\">N</span>.</li>\n<li>The relays further submit leader bids for slot <span class=\"math\">N+1</span> to the leader auction.</li>\n</ol>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/7/78d6d4a75c61c8924502b41d72fa6adfa2853f0f.png\" data-download-href=\"https://ethresear.ch/uploads/default/78d6d4a75c61c8924502b41d72fa6adfa2853f0f\" title=\"Fig. 1: Protocol overview for Slot N.\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/7/78d6d4a75c61c8924502b41d72fa6adfa2853f0f_2_689x304.png\" alt=\"Fig. 1: Protocol overview for Slot N.\" data-base62-sha1=\"heZzht0aKd1q7ORqmY08PTei1MX\" width=\"689\" height=\"304\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/7/78d6d4a75c61c8924502b41d72fa6adfa2853f0f_2_689x304.png, https://ethresear.ch/uploads/default/optimized/2X/7/78d6d4a75c61c8924502b41d72fa6adfa2853f0f_2_1033x456.png 1.5x, https://ethresear.ch/uploads/default/original/2X/7/78d6d4a75c61c8924502b41d72fa6adfa2853f0f.png 2x\" data-dominant-color=\"FAFAFA\"></a></div><p></p>\n<h2><a name=\"protocol-execution-at-the-beginning-of-slot-n1-5\" class=\"anchor\" href=\"https://ethresear.ch#protocol-execution-at-the-beginning-of-slot-n1-5\"></a>Protocol Execution at the Beginning of Slot <span class=\"math\">N+1</span></h2>\n<ol>\n<li>At the beginning of slot <span class=\"math\">N+1</span>, the leader auction declares the provider with the highest bid as the leader for slot <span class=\"math\">N+1</span> by publishing the leader together with the auction price on the mev-commit chain. The auction further settles the payment from the leader to the proposer.</li>\n<li>The elected leader can now issue preconfirmations. Note that the leader has already received all preconfirmation bids for that slot and, therefore, can issue all preconfirmations immediately. They do so by publishing a blob inclusion list on the mev-commit chain. This list is final at this point and includes all bids including the blob KZG commitment hashes the leader commits to. To avoid timing issues in the next step, we can require the inclusion list to be published sufficiently early, e.g., within the first 6 seconds of the slot (and ignore lists published too late).</li>\n</ol>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/4/4ed8dfebb950c5c7877ade27b2d878d918a04cb0.png\" data-download-href=\"https://ethresear.ch/uploads/default/4ed8dfebb950c5c7877ade27b2d878d918a04cb0\" title=\"Fig. 2: Protocol overview for beginning of slot N+1.\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/4/4ed8dfebb950c5c7877ade27b2d878d918a04cb0_2_690x486.png\" alt=\"Fig. 2: Protocol overview for beginning of slot N+1.\" data-base62-sha1=\"bfvTsKG1Mhs0oMXI0ur6XlFdMYw\" width=\"690\" height=\"486\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/4/4ed8dfebb950c5c7877ade27b2d878d918a04cb0_2_690x486.png, https://ethresear.ch/uploads/default/original/2X/4/4ed8dfebb950c5c7877ade27b2d878d918a04cb0.png 1.5x, https://ethresear.ch/uploads/default/original/2X/4/4ed8dfebb950c5c7877ade27b2d878d918a04cb0.png 2x\" data-dominant-color=\"F8F9FA\"></a></div><p></p>\n<h2><a name=\"protocol-execution-during-slot-n1-6\" class=\"anchor\" href=\"https://ethresear.ch#protocol-execution-during-slot-n1-6\"></a>Protocol Execution During Slot <span class=\"math\">N+1</span></h2>\n<ol>\n<li>The builders receive from the mev-commit chain the inclusion list of blobs that are mandated to be in the block. Then, the builders build a block containing those blobs (in an order of their choice) and send it to the relays. Note that the builder can also include additional blobs if they choose to do so.</li>\n<li>The relays only forward to the L1 proposer the blocks from builders that opted-in to the protocol. The relays can also optionally verify that the blocks respect the blob inclusion list (or simply trust the builders in case of optimistic relays).</li>\n<li>The proposer signs a block header received from a relay (this can be done optimistically without checking for blob inclusion).</li>\n</ol>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/5/5498d249fa6e64846cf1c2a555e5b2c93a003873.png\" data-download-href=\"https://ethresear.ch/uploads/default/5498d249fa6e64846cf1c2a555e5b2c93a003873\" title=\"Fig. 3: Protocol overview for slot N+1.\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/5/5498d249fa6e64846cf1c2a555e5b2c93a003873_2_690x330.png\" alt=\"Fig. 3: Protocol overview for slot N+1.\" data-base62-sha1=\"c4nwBJAuDWGKL6ny48ygchh73YD\" width=\"690\" height=\"330\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/5/5498d249fa6e64846cf1c2a555e5b2c93a003873_2_690x330.png, https://ethresear.ch/uploads/default/optimized/2X/5/5498d249fa6e64846cf1c2a555e5b2c93a003873_2_1035x495.png 1.5x, https://ethresear.ch/uploads/default/original/2X/5/5498d249fa6e64846cf1c2a555e5b2c93a003873.png 2x\" data-dominant-color=\"F8F9FA\"></a></div><p></p>\n<h2><a name=\"l1-monitoring-7\" class=\"anchor\" href=\"https://ethresear.ch#l1-monitoring-7\"></a>L1 Monitoring</h2>\n<ol>\n<li>An oracle monitors the L1 chain for blocks and reports to the mev-commit chain when a block is proposed by an opted-in proposer, who the proposer is, and whether this block (or a previous block) contained the blobs from the inclusion list. The oracle may want to wait for L1 finality to avoid reorg issues.</li>\n<li>If the oracle reports a blob inclusion list violation, slashing is executed (see below for details).</li>\n</ol>\n<h2><a name=\"payments-and-slashing-8\" class=\"anchor\" href=\"https://ethresear.ch#payments-and-slashing-8\"></a>Payments and Slashing</h2>\n<p>We consider the following rules for payments and slashing.</p>\n<p>The following two types of payments are always executed (even if blobs are not included, slots get missed, etc.):</p>\n<ol>\n<li>The preconfirmation leader pays the amount for winning the leader auction to the L1 proposer (regardless of whether any preconfirmations are issued; this is because they participated in the auction and won the rights to become the leader).</li>\n<li>For all blobs in the inclusion list, the preconfirmation bidders pay the corresponding amount to the preconfirmation providers (regardless of whether the blob actually gets included; bidders may get reimbursed via slashing of proposers as described below).</li>\n</ol>\n<p>If the oracle reports that a blob from an inclusion list is not included in an L1 block in slot <span class=\"math\">N+1</span> or earlier, we need to execute slashing. There are different scenarios that can lead to a blob from the inclusion list not being included on L1:</p>\n<ol>\n<li>An opted-in relay sends a block not containing the blob.</li>\n<li>The proposer includes a block from an external relay.</li>\n<li>The proposer proposes their own block.</li>\n<li>The proposer misses the slot.</li>\n<li>The block gets proposed but then orphaned in a reorg.</li>\n</ol>\n<p>In the first case, the relay or the builder violated the protocol, while in cases 2, 3, and 4, it is the proposer’s fault. Without additional mechanisms, the proposer cannot prove which relay has sent what block. We therefore always slash the proposer. If it was indeed the relay’s fault, the relay’s reputation with the proposer gets “slashed” in the same way as the relay sending invalid blocks. If in turn the relay has received that block from an opted-in builder, that builder’s reputation with the relay is “slashed” and they can settle the dispute out-of-protocol with existing mechanisms.</p>\n<p>The last case is special since it may not be the fault of anyone involved and constitutes a general reorg risk the proposers need to consider. The slashing amount therefore needs to be set such that proposers still can make profit overall.</p>\n<p>The slashed amount is distributed to the preconfirmation bidders proportionally to their bid amounts since they are the ones harmed by the protocol violation.</p>\n<h1><a name=\"further-details-and-discussions-9\" class=\"anchor\" href=\"https://ethresear.ch#further-details-and-discussions-9\"></a>Further Details and Discussions</h1>\n<h2><a name=\"n1-slot-design-10\" class=\"anchor\" href=\"https://ethresear.ch#n1-slot-design-10\"></a><span class=\"math\">N+1</span> Slot Design</h2>\n<p>In our protocol, preconfirmation bidders bid in slot <span class=\"math\">N</span> for blob inclusion in slot <span class=\"math\">N+1</span>. This means that there is an expected one-slot delay between bidding and blob inclusion. While this would be unacceptable for time-sensitive transactions, e.g., for mev extraction, blob inclusion is substantially less time-sensitive.</p>\n<p>We further note that this next-slot inclusion list design is also used in EIP-7547 [3]. The lack of an out-of-protocol next-slot design and consequently the lack of real-world data from such designs has recently been criticized in the context of L1 inclusion lists [4]. Hence, the availability of this via mev-commit can be used to gather data about the efficacy of next-slot inclusion lists and can serve the Ethereum Foundation to make an effective decision based on the obtained empirical data.</p>\n<p>The purpose of the delay is to ensure that at the time the block builders want to build the block, the blob inclusion list is available to them. Furthermore, this allows the preconfirmation leader of slot <span class=\"math\">N+1</span> to preconfirm blobs at the beginning of the slot. By doing so, all participants know which blobs are going to be included in the block of slot <span class=\"math\">N+1</span> way ahead of the end time of that slot. In particular, the L1 validators could in the future use this information to pre-fetch the blob data at this time, thereby mitigating reorg risks due to long blob propagation (see, e.g., [5]). Having a predetermined leader also means that once the leader issues a preconfirmation, all participants can immediately rely on the blob inclusion, which allows for synchronous composability.</p>\n<h2><a name=\"leader-auction-11\" class=\"anchor\" href=\"https://ethresear.ch#leader-auction-11\"></a>Leader Auction</h2>\n<p>The auction to determine the preconfirmation leaders should ideally be credible, i.e., not require a trusted auctioneer. There are different ways to achieve this, and the precise implementation is orthogonal to our preconfirmation design. One option using SUAVE was explored by the Primev team, which has built a prototype auction at ETHGlobal London and was awarded a prize from Nethermind [6]. Another option is to use cryptographic tools such as timed commitments to realize the auction [7]. As an intermediate solution, the auction can also be run by a trusted actor.</p>\n<h2><a name=\"premature-inclusion-of-blobs-12\" class=\"anchor\" href=\"https://ethresear.ch#premature-inclusion-of-blobs-12\"></a>Premature Inclusion of Blobs</h2>\n<p>Leaders can issue preconfirmations for blobs to be included in slot <span class=\"math\">N+1</span>. However, before slot <span class=\"math\">N+1</span>, some builder (possibly outside of our protocol) may have already included this blob in a block that is now part of L1. In such a case, all payments are executed as described above and no slashing takes place. The preconfirmation bidders got their blobs included even earlier than expected and nobody else is harmed in this scenario.</p>\n<h1><a name=\"conclusion-13\" class=\"anchor\" href=\"https://ethresear.ch#conclusion-13\"></a>Conclusion</h1>\n<p>We have presented an out-of-protocol mechanism for blob inclusion lists. As long as inclusion lists are not available as part of the Ethereum protocol, we believe our solution offers a viable solution to the current issues with blob contention. Since it is a next-slot design, findings from this solution can further help to improve the understanding of such designs, which are also considered for L1 inclusion lists.</p>\n<p>Currently, mev-commit for transaction preconfirmations is live on the Holesky testnet and we are excited to add an implementation of the design discussed here. We are looking forward to feedback to further refine and improve this system.</p>\n<h1><a name=\"references-14\" class=\"anchor\" href=\"https://ethresear.ch#references-14\"></a>References</h1>\n<p>[1] <a href=\"https://twitter.com/bertkellerman/status/1773031698222989623?s=46\" rel=\"noopener nofollow ugc\">https://twitter.com/bertkellerman/status/1773031698222989623?s=46</a><br>\n[2] <a href=\"https://twitter.com/mcutler/status/1773033173573628009\" rel=\"noopener nofollow ugc\">https://twitter.com/mcutler/status/1773033173573628009</a><br>\n[3] <a href=\"https://github.com/ethereum/EIPs/blob/30fec793f3cb6769cb44d2d0daa5238451f67c48/EIPS/eip-7547.md\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">EIPs/EIPS/eip-7547.md at 30fec793f3cb6769cb44d2d0daa5238451f67c48 · ethereum/EIPs · GitHub</a><br>\n[4] <a href=\"https://notes.ethereum.org/@mikeneuder/the-case-for-ilectra\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">The Case for ILECTRA - HackMD</a><br>\n[5] <a href=\"https://ethresear.ch/t/validator-timing-game-post-eip4844/18129\" class=\"inline-onebox\">Validator Timing Game Post EIP4844</a><br>\n[6] <a href=\"https://ethglobal.com/showcase/blobpreconf-auction-qfdco\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">BlobPreconf Auction | ETHGlobal</a><br>\n[7] <a href=\"https://eprint.iacr.org/2023/1336\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Riggs: Decentralized Sealed-Bid Auctions</a></p>\n            <p><small>8 posts - 4 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/blob-preconfirmations-with-inclusion-lists-to-mitigate-blob-contention-and-censorship/19150\">Read full topic</a></p>","link":"https://ethresear.ch/t/blob-preconfirmations-with-inclusion-lists-to-mitigate-blob-contention-and-censorship/19150","pubDate":"Fri, 29 Mar 2024 11:36:38 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19150"},"source":{"@url":"https://ethresear.ch/t/blob-preconfirmations-with-inclusion-lists-to-mitigate-blob-contention-and-censorship/19150.rss","#text":"Blob Preconfirmations with Inclusion Lists to Mitigate Blob Contention and Censorship"},"filter":false},{"title":"Introducing Accrual-Based Recurring Payments for Decentralized Platforms","dc:creator":"grybniak","category":"EVM","description":"<p><img src=\"https://ethresear.ch/uploads/default/original/2X/1/1928a38c42bccb97e4d24ca767786d6cd62d55d9.jpeg\" alt=\"Image1\" data-base62-sha1=\"3Az0jF2NUUJqPM3esOAlDkJ9NfH\" width=\"634\" height=\"369\"></p>\n<h1><a name=\"key-takeaways-1\" class=\"anchor\" href=\"https://ethresear.ch#key-takeaways-1\"></a>Key Takeaways:</h1>\n<ol>\n<li>The proposed solution introduces a payment system based on accrual accounting, reflecting revenue and expenses before actual payment transfers.</li>\n<li>Advantages of the presented solution include convenience, instant access to payment revenues, regular debentures, and significantly lower transaction fees.</li>\n<li>The lazy evaluation approach is employed to efficiently process a large number of transactions and minimize computing power usage.</li>\n<li>The solution utilizes smart contracts on Ethereum or Ethereum-compatible networks to implement the new token standard.</li>\n<li>The implementation of the solution follows the currently developing IEEE standard for recurring transactions on distributed ledger technologies.</li>\n</ol>\n<p>Automatic recurring payments have become a critical revenue stream for businesses in almost every sector, providing a reliable incremental cash flow to support business processes. Some real-world use cases are displayed in the following figure.</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/2X/f/f9102e6e07fba56330da01693b7849466106ba60.png\" alt=\"Image3\" data-base62-sha1=\"zxjyD6ngDCyX9uI078Q948FiChG\" width=\"681\" height=\"391\"></p>\n<p>However, customer payments made on centralized platforms are subject to data breaches, and automated payments can easily be disrupted due to insufficient funds, expired debit or credit cards, and delays imposed by centralized banks. High transaction fees can take a bite out of payments, amounting to a significant reduction in revenues.</p>\n<p>For consumers, the convenience of automatic payments is offset by risks to sensitive data, making them vulnerable to identity theft and other mischief, and subjecting them to unauthorized sale of data to advertisers and other entities.</p>\n<p>While decentralized smart-contract payment systems do exist, they rely on a cash-based accounting system that is inadequate for executing ongoing recurring payments.</p>\n<p>Learn how accrual-based accounting can offer a solution that eliminates or significantly reduces the drawbacks of existing decentralized payment systems via smart contracts on the blockchain/Web 3.0 networks.</p>\n<h1><a name=\"challenges-of-recurring-payments-2\" class=\"anchor\" href=\"https://ethresear.ch#challenges-of-recurring-payments-2\"></a>Challenges of Recurring Payments</h1>\n<p>Subscription-based business models have been thriving globally for years, as exemplified by familiar centralized platforms such as streaming services, mobile service providers, and a variety of other platforms with automated recurring monthly fees.</p>\n<p>However, in decentralized public networks, a system for ongoing recurring payments has not been well thought out. Crypto wallet users currently face challenges when it comes to remitting and receiving recurring payments – they lack convenient, fast, and efficient payment methods, and they often involve transaction fees.</p>\n<p>But imagine if your recurring decentralized payments were made automatically, with minimal transaction fees, saving you time and money. We present a solution that has the potential to significantly impact decentralized transactions.</p>\n<p>Recurring transactions can be defined in terms of payment frequency (weekly, biweekly, monthly, etc.), fixed amounts, the prolongation of payments, the number of persons involved (individual, family, corporate subscriptions), a list of provided services, termination rules, and other conditions.</p>\n<p>However, problems arise when a customer does not have enough available funds to make a timely payment. In that case, we need to take into account the possibility of:</p>\n<ul>\n<li>debt formation​ – a necessary procedure to keep payments up-to-date and to charge late fees for late payments;</li>\n<li>debt repayment;​</li>\n<li>early termination​.</li>\n</ul>\n<h1><a name=\"the-status-quo-vs-our-approach-3\" class=\"anchor\" href=\"https://ethresear.ch#the-status-quo-vs-our-approach-3\"></a>The Status Quo vs Our Approach</h1>\n<h2><a name=\"current-options-for-decentralized-transactions-4\" class=\"anchor\" href=\"https://ethresear.ch#current-options-for-decentralized-transactions-4\"></a>Current options for decentralized transactions</h2>\n<p>Let’s recall that a <em>smart contract</em> is a self-executing contract with the terms of the agreement directly written into code. It is typically built on a blockchain platform and allows for the automation of transactions and agreements without the need for intermediaries.</p>\n<p>Smart contracts are executed by nodes. The process of running a smart contract involves the following steps:</p>\n<ol>\n<li>Choosing a suitable blockchain platform that supports smart contracts.</li>\n<li>Writing code for the smart contract using a programming language that is compatible with the chosen blockchain platform.</li>\n<li>Compiling the contract using the appropriate compiler for the chosen programming language. This step generates the bytecode that can be executed on the blockchain.</li>\n<li>Deploying the compiled smart contract onto the blockchain. This process involves creating a transaction that contains the bytecode and sending it to the network for inclusion in a block.</li>\n<li>Interacting with the contract: once the smart contract is deployed, it becomes a part of the blockchain and can be accessed and interacted with by users. Users can send transactions to the contract, triggering the execution of the predefined code.</li>\n<li>Executing the contract. When a transaction is sent to the smart contract, the code within the contract is executed on the blockchain nodes. The contract’s logic is automatically enforced, and the contract performs the actions specified in the code.</li>\n<li>Verifying and validating the contract. The decentralized nature of blockchain ensures that every node in the network verifies and validates the execution of the smart contract.</li>\n</ol>\n<p>The problem with popular smart contract platforms is that they are not designed to accept ongoing recurring payments. Decentralized public networks use a cash-based accounting system for payments, whose main advantages are ease of tracking and accurate reflection of account balances. The cash method of accounting records transactions whenever cash is received or paid. This approach is typical for small businesses with a relatively small turnover of funds and a small customer base.</p>\n<p>There currently exist two types of payment solutions:</p>\n<ul>\n<li>Custodial solutions – these solutions depend on the crypto-wallet owner depositing funds into a smart contract account, making it possible for the funds to be freely used without the consent of their true owner. This method creates an opportunity to establish a system for regular or on-demand payments.</li>\n<li>Non-custodial solutions – here, the customer sends a batch of transactions to the supplier, who stores them off-chain. For each billing period, the supplier selects and submits a single transaction to the Ethereum blockchain or an EVM-compatible network. A smart contract verifies the transaction’s validity and initiates the payment. While this approach does not reduce transaction fees, it offers the convenience of fully automated crypto wallets.</li>\n</ul>\n<h2><a name=\"our-proposed-solution-5\" class=\"anchor\" href=\"https://ethresear.ch#our-proposed-solution-5\"></a>Our proposed solution</h2>\n<p>An alternative to a cash-based accounting system is the accrual system. Accrual accounting has long been used by traditional financial institutions, but it has not yet been used in decentralized networks. We propose an accrual-based system that operates in a non-custodial manner. In our proposed system, revenue and expenses appear whenever a product or service is delivered to a customer, but before the payment amount is actually transferred. Our proposed smart contract on Ethereum or Ethereum-compatible networks provides a new token standard.</p>\n<p>Advantages, especially for enterprises and state authorities as well as social interaction purposes, include:</p>\n<ul>\n<li>convenience and ease of use;</li>\n<li>instant access to payment revenues;</li>\n<li>regular debentures and recurring payments;</li>\n<li>significantly lower transaction fees.</li>\n</ul>\n<h1><a name=\"our-high-level-design-6\" class=\"anchor\" href=\"https://ethresear.ch#our-high-level-design-6\"></a>Our High-Level Design</h1>\n<h2><a name=\"conventional-blockchain-financial-information-flow-7\" class=\"anchor\" href=\"https://ethresear.ch#conventional-blockchain-financial-information-flow-7\"></a>Conventional blockchain financial information flow</h2>\n<p>Here is how financial information is typically processed on the blockchain:</p>\n<ol>\n<li>Financial and other transactions flow from users to nodes on the network, forming a common transaction pool.</li>\n<li>Following a specific consensus protocol, transactions from the pool end up in blockchain blocks. In effect, the blockchain acts as a payment transaction ledger.</li>\n<li>Each node receives a new block and sequentially applies transactions from the block to its version of the blockchain state.</li>\n<li>The blockchain state records the amount of funds in the accounts of all users.</li>\n</ol>\n<p>In blockchain transactions, nodes perform calculations and remember the results only when transactions from the next block are applied. Until there is a new block, the nodes do nothing with the blockchain state. Put simply, the blockchain state is a ledger storing the number of tokens in the accounts of all users.</p>\n<h2><a name=\"how-our-solution-differs-8\" class=\"anchor\" href=\"https://ethresear.ch#how-our-solution-differs-8\"></a>How our solution differs</h2>\n<p>Our proposed approach to making regular payments does not change the blockchain scheme described above. As before, the amounts in the accounts are updated only when transactions are processed. Even if a regular payment is made once every second, the system does not update the account amounts with the same frequency.</p>\n<p>For example: If Alice pays Bob one token per second, the system does not process the payments every second. The status of both Alice’s and Bob’s accounts are recalculated all at once (so-called <em>lazy evaluation</em> or call-by-need method), for a given period, at the moment when any transaction from Bob or Alice requires knowledge of their account status.</p>\n<p>In other words, to process regular payments, our solution adds a recursive algorithm for finding and accounting for all regular payments associated with the participating accounts, at the moment of transaction processing. All linked regular payments are applied in their correct chronological order. This coincides with the concept of lazy computation in programming – the value of a variable is not computed until the variable is used.</p>\n<h2><a name=\"activity-diagram-9\" class=\"anchor\" href=\"https://ethresear.ch#activity-diagram-9\"></a>Activity Diagram</h2>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/d/d98593b3774f8abb6e9c19293051e966347ddf46.png\" data-download-href=\"https://ethresear.ch/uploads/default/d98593b3774f8abb6e9c19293051e966347ddf46\" title=\"Image2\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/d/d98593b3774f8abb6e9c19293051e966347ddf46_2_356x500.png\" alt=\"Image2\" data-base62-sha1=\"v2hNv2G03aA3FMDYyspezHv3dwq\" width=\"356\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/d/d98593b3774f8abb6e9c19293051e966347ddf46_2_356x500.png, https://ethresear.ch/uploads/default/optimized/2X/d/d98593b3774f8abb6e9c19293051e966347ddf46_2_534x750.png 1.5x, https://ethresear.ch/uploads/default/original/2X/d/d98593b3774f8abb6e9c19293051e966347ddf46.png 2x\" data-dominant-color=\"E8EAED\"></a></div><p></p>\n<h1><a name=\"payment-logic-design-10\" class=\"anchor\" href=\"https://ethresear.ch#payment-logic-design-10\"></a>Payment Logic Design</h1>\n<p>Unlike unsecured transfer transactions that frequently result in payment delays, our proposed solution enables regular payments without freezing funds. Payments that are nearing their due date, but for which there are insufficient available funds, are called “short-term payment commitments,” which are sent to the debt queue. When funds are insufficient, suppliers can decide whether to extend credit or terminate the contract. To terminate, they must send a termination transaction, to prevent a default to granting credit.</p>\n<p>In the same way, customers may send a transaction to terminate their subscription and stop future payments.</p>\n<p>To implement this system, certain elements must be considered:</p>\n<ul>\n<li>the final balance after completed payments;</li>\n<li>a provision for partial payments;</li>\n<li>a procedure for late payments.</li>\n</ul>\n<p>The table represents an example where transactions on a customer’s account gradually pay off her debt. A capital letter indicates the payee and an asterisk* marks the separable transaction – in this case, it is the first in the queue. When funds appear in the account, the account is reviewed and the oldest debt is paid first. Subsequent debts remain in the account queue and are paid when funds appear, from oldest to newest, until all are paid in full.</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>Debt queue</th>\n<th>Funds added to Alice’s account</th>\n<th>Alice’s account status</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>В*(20), С(100), B(1), C(2)</td>\n<td>5</td>\n<td>0</td>\n</tr>\n<tr>\n<td>В*(15), С(100), B(1), C(2)</td>\n<td>17</td>\n<td>1</td>\n</tr>\n<tr>\n<td>С(100), C(2)</td>\n<td>50</td>\n<td>49</td>\n</tr>\n<tr>\n<td>С(100)</td>\n<td>50</td>\n<td>99</td>\n</tr>\n<tr>\n<td>С(100)</td>\n<td>10</td>\n<td>9</td>\n</tr>\n</tbody>\n</table>\n</div><p>Let’s look at the first row. Alice adds 5 tokens to her account, and they are applied to her oldest debt: В*(20). In the second row, we see that Alice is able to pay off her first debt, and also the third one. At that time, she cannot pay off the second debt, so it remains the same. In subsequent rows, we see that Alice is able to pay off all her debts over time.</p>\n<h1><a name=\"implementation-and-future-work-11\" class=\"anchor\" href=\"https://ethresear.ch#implementation-and-future-work-11\"></a>Implementation and Future Work</h1>\n<p>The Recurring Transactions on the Distributed Ledger Technologies (DLTs) Working Group for the development of a <a href=\"https://standards.ieee.org/ieee/3228/11069/\" rel=\"noopener nofollow ugc\">token standard (project P3228)</a> has been approved by the IEEE Computer Society/Blockchain and Distributed Ledgers (C/BDL) Standards Committee. The purpose of this standard is to provide a resource for implementing blockchain and distributed ledger-based recurring payment methods in relevant industries, including public services, banking, finance, insurance, real estate, commercial payments, payrolls, and online services.</p>\n<p>Our proposed payment and token accounting solution is implemented as a smart contract on the Ethereum network, or on any system with a compatible virtual machine. Two smart contract versions were developed for backward compatibility with ERC-20 and ERC-777 standards, respectively. The tokens issued on their basis have successfully demonstrated the declared properties.</p>\n<p>Our idea <a href=\"https://doi.org/10.1109/iGETblockchain56591.2022.10087077\" rel=\"noopener nofollow ugc\">was presented</a> at the 2022 IEEE 1st Global Emerging Technology Blockchain Forum: Blockchain &amp; Beyond (iGETblockchain), in Irvine, CA, USA.</p>\n<p>Future functionality of our solution may expand to include:</p>\n<ul>\n<li>suspension of subscriptions;</li>\n<li>accrual of late fees on debts;</li>\n<li>recalculation of payments based on fiat exchange rates;</li>\n<li>creation of “oracles” to enable changes in the payment amount during the subscription period;</li>\n<li>confirmation of work performed;</li>\n<li>creation of web applications.</li>\n</ul>\n<p>Decentralized solutions like the one we propose can expand the scope of opportunities and services provided to users of cryptocurrency platforms, without sacrificing important advantages such as transparency and security.</p>\n<p><em>Learn more about</em> <a href=\"https://github.com/waterfall-network/recurring-payment-contract\" rel=\"noopener nofollow ugc\"><em>recurring payment contracts</em></a><em>.</em></p>\n            <p><small>3 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/introducing-accrual-based-recurring-payments-for-decentralized-platforms/19147\">Read full topic</a></p>","link":"https://ethresear.ch/t/introducing-accrual-based-recurring-payments-for-decentralized-platforms/19147","pubDate":"Fri, 29 Mar 2024 06:49:33 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19147"},"source":{"@url":"https://ethresear.ch/t/introducing-accrual-based-recurring-payments-for-decentralized-platforms/19147.rss","#text":"Introducing Accrual-Based Recurring Payments for Decentralized Platforms"},"filter":false},{"title":"Supporting decentralized staking through more anti-correlation incentives","dc:creator":"vbuterin","category":"Proof-of-Stake","description":"<p><em>Content note: preliminary research. Would love to see independent replication attempts.</em></p>\n<p><em>Code: <a href=\"https://github.com/ethereum/research/tree/master/correlation_analysis\">https://github.com/ethereum/research/tree/master/correlation_analysis</a></em></p>\n<p>One tactic for incentivizing better decentralization in a protocol is to <em>penalize correlations</em>. That is, if one actor misbehaves (including accidentally), the penalty that they receive would be greater the more other actors (as measured by total ETH) misbehave at the same time as them. The theory is that if you are a single large actor, any mistakes that you make would be more likely to be replicated across all “identities” that you control, even if you split your coins up among many nominally-separate accounts.</p>\n<p>This technique is already employed in Ethereum <a href=\"https://github.com/ethereum/annotated-spec/blob/master/phase0/beacon-chain.md#aside-anti-correlation-penalties-in-eth2\">slashing (and arguably inactivity leak) mechanics</a>. However, edge-case incentives that only arise in a highly exceptional attack situation that may never arise in practice are perhaps not sufficient for incentivizing decentralization.</p>\n<p><strong>This post proposes to extend a similar sort of anti-correlation incentive to more “mundane” failures, such as missing an attestation</strong>, that nearly all validators make at least occasionally. The theory is that larger stakers, including both wealthy individuals and staking pools, are going to run many validators on the same internet connection or even on the same physical computer, and this will cause disproportionate correlated failures. Such stakers <em>could</em> always make an independent physical setup for each node, but if they end up doing so, it would mean that we have completely eliminated economies of scale in staking.</p>\n<h2><a name=\"sanity-check-are-errors-by-different-validators-in-the-same-cluster-actually-more-likely-to-correlate-with-each-other-1\" class=\"anchor\" href=\"https://ethresear.ch#sanity-check-are-errors-by-different-validators-in-the-same-cluster-actually-more-likely-to-correlate-with-each-other-1\"></a>Sanity check: are errors by different validators in the same “cluster” actually more likely to correlate with each other?</h2>\n<p>We can check this by combining two datasets: (i) <strong>attestation data</strong> from some recent epochs showing which validators were supposed to have attested, and which validators actually did attest, during each slot, and (ii) data mapping validator IDs to publicly-known <strong>clusters</strong> that contain many validators (eg. “Lido”, “Coinbase”, “Vitalik Buterin”). You can find a dump of the former <a href=\"https://data.ethpandaops.io/efresearch/attesters.txt\">here</a>, <a href=\"https://data.ethpandaops.io/efresearch/immediate_attesters.txt\">here</a> and <a href=\"https://data.ethpandaops.io/efresearch/committees.txt\">here</a>, and the latter <a href=\"https://data.ethpandaops.io/efresearch/clusters.csv\">here</a>.</p>\n<p>We then run a script that computes the total number of <strong>co-failures</strong>: instances of two validators within the same cluster being assigned to attest during the same slot, and failing in that slot.</p>\n<p>We also compute <strong>expected co-failures</strong>: the number of co-failures that “should have happened” if failures were fully the result of random chance.</p>\n<p>For example, suppose that there are ten validators with one cluster of size 4 and the others independent, and three validators fail: two within that cluster, and one outside it.</p>\n<br>\n<p><img src=\"https://ethresear.ch/uploads/default/original/2X/e/e103e9e08c8e2aedd5d5fa0e004b8429f5cb2af2.png\" alt=\"\" data-base62-sha1=\"w6zOzplJ28Kqbpxs8yvJvje00hk\" role=\"presentation\" width=\"610\" height=\"102\"></p>\n<br>\n<p>There is one co-failure here: the second and fourth validators within the first cluster. If all four validators in that clusters had failed, there would be <em>six</em> co-failures, one for each six possible pairs.</p>\n<p>But how many co-failures “should there” have been? This is a tricky philosophical question. A few ways to answer:</p>\n<ul>\n<li>For each failure, assume that the number of co-failures equals the failure rate across the other validators in that slot times the number of validators in that cluster, and halve it to compensate for double-counting. For the above example, this gives <span class=\"math\">\\frac{2}{3}</span>.</li>\n<li>Calculate the global failure rate, square it, and then multiply that by <span class=\"math\">\\frac{n * (n-1)}{2}</span> for each cluster. This gives <span class=\"math\">(\\frac{3}{10})^2 * 6 = 0.54</span>.</li>\n<li>Randomly redistribute each validator’s failures among their entire history.</li>\n</ul>\n<p>Each method is not perfect. The first two methods fail to take into account different clusters having different quality setups. Meanwhile, the last method fails to take into account correlations arising from different slots having different <em>inherent difficulties</em>: for example, slot <a href=\"https://beaconcha.in/slot/8103681\">8103681</a> has a very large number of attestations that don’t get included within a single slot, possibly because the block was published unusually late.</p>\n<br>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/c/c34994d5a8fe647e304a91b98a72361eeb532c48.png\" data-download-href=\"https://ethresear.ch/uploads/default/c34994d5a8fe647e304a91b98a72361eeb532c48\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/original/2X/c/c34994d5a8fe647e304a91b98a72361eeb532c48.png\" alt=\"\" data-base62-sha1=\"rRAKKRSTWmSK8UQ24Qpk612CFug\" role=\"presentation\" width=\"690\" height=\"177\" data-dominant-color=\"422038\"></a></div><p></p>\n<p><em>See the “10216 ssfumbles” in this python output.</em></p>\n<br>\n<p>I ended up implementing three approaches: the first two approaches above, and a more sophisticated approach where I compare “actual co-failures” with “fake co-failures”: failures where each cluster member is replaced with a (pseudo-) random validator that has a similar failure rate.</p>\n<p>I also explicitly separate out <strong>fumbles</strong> and <strong>misses</strong>. I define these terms as follows:</p>\n<ul>\n<li><strong>Fumble</strong>: when a validator misses an attestation during the current epoch, but attested correctly during the previous epoch</li>\n<li><strong>Miss</strong>: when a validator misses an attestation during the current epoch and also missed during the previous epoch</li>\n</ul>\n<p>The goal is to separate the two very different phenomena of (i) network hiccups during normal operation, and (ii) going offline or having longer-term glitches.</p>\n<p>I also simultaneously do this analysis for two datasets: <strong>max-deadline</strong> and <strong>single-slot-deadline</strong>. The first dataset treats a validator as having failed in an epoch only if an attestation was never included at all. The second dataset treats a validator as having failed if the attestation does not get included <em>within a single slot</em>.</p>\n<p>Here are my results for the first two methods of computing expected co-failures. SSfumbles and SSmisses here refer to fumbles and misses using the single-slot dataset.</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Fumbles</th>\n<th>Misses</th>\n<th>SSfumbles</th>\n<th>SSmisses</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Expected (algo 1)</td>\n<td>8602090</td>\n<td>1695490</td>\n<td>604902393</td>\n<td>2637879</td>\n</tr>\n<tr>\n<td>Expected (algo 2)</td>\n<td>937232</td>\n<td>4372279</td>\n<td>26744848</td>\n<td>4733344</td>\n</tr>\n<tr>\n<td>Actual</td>\n<td>15481500</td>\n<td>7584178</td>\n<td>678853421</td>\n<td>8564344</td>\n</tr>\n</tbody>\n</table>\n</div><p>For the first method, the <code>Actual</code> row is different, because a more restricted dataset is used for efficiency:</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Fumbles</th>\n<th>Misses</th>\n<th>SSfumbles</th>\n<th>SSmisses</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Fake clusters</td>\n<td>8366846</td>\n<td>6006136</td>\n<td>556852940</td>\n<td>5841712</td>\n</tr>\n<tr>\n<td>Actual</td>\n<td>14868318</td>\n<td>6451930</td>\n<td>624818332</td>\n<td>6578668</td>\n</tr>\n</tbody>\n</table>\n</div><p>The “expected” and “fake clusters” columns show how many co-failures within clusters there “should have been”, if clusters were uncorrelated, based on the techniques described above. The “actual” columns show how many co-failures there actually were. Uniformly, we see strong evidence of “excess correlated failures” within clusters: two validators in the same cluster are significantly more likely to miss attestations at the same time than two validators in different clusters.</p>\n<h2><a name=\"how-might-we-apply-this-to-penalty-rules-2\" class=\"anchor\" href=\"https://ethresear.ch#how-might-we-apply-this-to-penalty-rules-2\"></a>How might we apply this to penalty rules?</h2>\n<p>I propose a simple strawman: in each slot, let <code>p</code> be the current number of missed slots divided by the average for the last 32 slots. That is, <span class=\"math\">p[i] = \n\\frac{misses[i]}{\\sum_{j=i-32}^{i-1}\\ misses[j]}</span>. Cap it: <span class=\"math\">p \\leftarrow min(p, 4)</span>. Penalties for attestations of that slot should be proportional to <span class=\"math\">p</span>. That is, <strong>the penalty for not attesting at a slot should be proportional to how many validators fail in that slot <em>compared to other recent slots</em></strong>.</p>\n<p>This mechanism has a nice property that it’s not easily attackable: there isn’t a case where failing <em>decreases</em> your penalties, and manipulating the average enough to have an impact requires making a large number of failures yourself.</p>\n<p>Now, let us try actually running it. Here are the total penalties for big clusters, medium clusters, small clusters and all validators (including non-clustered) for four penalty schemes:</p>\n<ul>\n<li><strong><code>basic</code></strong>: Penalize one point per miss (ie. similar to status quo)</li>\n<li><strong><code>basic_ss</code></strong>: the same but requiring single-slot inclusion to not count as a miss</li>\n<li><strong><code>excess</code></strong>: penalize <code>p</code> points with <code>p</code> calculated as above</li>\n<li><strong><code>excess_ss</code></strong>: penalize <code>p</code> points with <code>p</code> calculated as above, requiring single-slot inclusion to not count as a miss</li>\n</ul>\n<p>Here is the output:</p>\n<pre><code class=\"lang-auto\">                   basic          basic_ss       excess         excess_ss    \nbig                0.69           2.06           2.73           7.96           \nmedium             0.61           3.00           2.42           11.54         \nsmall              0.98           2.41           3.81           8.77           \nall                0.90           2.44           3.54           9.30\n</code></pre>\n<p>With the “basic” schemes, big has a ~1.4x advantage over small (~1.2x in the single-slot dataset). With the “excess” schemes, this drops to ~1.3x (~1.1x in the single-slot dataset). With multiple other iterations of this, using slightly different datasets, <strong>the excess penalty scheme uniformly shrinks the advantage of “the big guy” over “the little guy”</strong>.</p>\n<h2><a name=\"whats-going-on-3\" class=\"anchor\" href=\"https://ethresear.ch#whats-going-on-3\"></a>What’s going on?</h2>\n<p>The number of failures per slot is small: it’s usually in the low dozens. This is much smaller than pretty much any “large staker”. In fact, it’s smaller than the number of validators that a large staker would have active <em>in a single slot</em> (ie. 1/32 of their total stock). If a large staker runs many nodes on the same physical computer or internet connection, then any failures will plausibly affect all of their validators.</p>\n<p>What this means is: when a large validator has an attestation inclusion failure, they single-handedly move the current slot’s failure rate, which then in turn increases their penalty. Small validators do not do this.</p>\n<p>In principle, a big staker can get around this penalty scheme by putting each validator on a separate internet connection. But this sacrifices the economies-of-scale advantage that a big staker has in being able to reuse the same physical infrastructure.</p>\n<h2><a name=\"topics-for-further-analysis-4\" class=\"anchor\" href=\"https://ethresear.ch#topics-for-further-analysis-4\"></a>Topics for further analysis</h2>\n<ul>\n<li>Find other strategies to confirm the size of this effect where validators in the same cluster are unusually likely to have attestation failures at the same time</li>\n<li>Try to find the ideal (but still simple, so as to not overfit and not be exploitable) reward/penalty scheme to minimize the average big validator’s advantage over little validators.</li>\n<li>Try to prove safety properties about this class of incentive schemes, ideally identify a “region of design space” within which risks of weird attacks (eg. strategically going offline at specific times to manipulate the average) are too expensive to be worth it</li>\n<li>Cluster by geography. This could determine whether or not this mechanism also creates an incentive to geographically decentralize.</li>\n<li>Cluster by (execution and beacon) client software. This could determine whether or not this mechanism also creates an incentive to use minority clients.</li>\n</ul>\n<h2><a name=\"mini-faq-5\" class=\"anchor\" href=\"https://ethresear.ch#mini-faq-5\"></a>Mini-FAQ</h2>\n<p><strong>Q</strong>: But wouldn’t this just lead to staking pools architecturally decentralizing their infra without politically decentralizing themselves, and isn’t the latter what we care about more at this point?</p>\n<p><strong>A</strong>: If they do, then that increases the cost of their operations, making solo staking relatively more competitive. The goal is not to single-handedly force solo staking, the goal is to make the economic part of the incentives more balanced. Political decentralization seems very hard or impossible to incentivize in-protocol; for that I think we will just have to count on social pressure, starknet-like airdrops, etc. But if economic incentives can be tweaked to favor architectural decentralization, that makes things easier for politically decentralized projects (which cannot avoid being architecturally decentralized) to get off the ground.</p>\n<p><strong>Q</strong>: Wouldn’t this hurt the “middle-size stakers” (wealthy individuals who are not big exchanges/pools) the most, and encourage them to move to pools?</p>\n<p><strong>A</strong>: In the table above, the “small” section refers to stakers with 10-300 validators, ie. 320-9600 ETH. That includes most wealthy people. And as we can see, those stakers suffer significantly higher penalties than pools today, and the simulation shows how the proposed adjusted reward scheme would equalize things between precisely those validators and the really big ones. Mathematically speaking, someone with 100 validator slots would only have 3 per slot, so they would not be greatly affecting the penalty factor for a round; only validators that go far above that would be.</p>\n<p><strong>Q</strong>: Post-MAXEB, won’t big stakers get around this by consolidating all their ETH into one validator?</p>\n<p><strong>A</strong>: The proportional penalty formula would count total amount of ETH, not number of validator IDs, so 4000 staked ETH that acts the same way would be treated the same if it’s split between 1 validator or 2 or 125.</p>\n<p><strong>Q</strong>: Won’t adding even more incentives to be online create further pressure to optimize and hence centralize, regardless of the details?</p>\n<p><strong>A</strong>:The parameters can be set so that on average, the size of the incentive to be online is the same as it is today.</p>\n            <p><small>13 posts - 9 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/supporting-decentralized-staking-through-more-anti-correlation-incentives/19116\">Read full topic</a></p>","link":"https://ethresear.ch/t/supporting-decentralized-staking-through-more-anti-correlation-incentives/19116","pubDate":"Tue, 26 Mar 2024 23:23:16 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19116"},"source":{"@url":"https://ethresear.ch/t/supporting-decentralized-staking-through-more-anti-correlation-incentives/19116.rss","#text":"Supporting decentralized staking through more anti-correlation incentives"},"filter":false},{"title":"[RFC] [DRAFT] Anoma as the universal intent machine for Ethereum","dc:creator":"cwgoes","category":"Architecture","description":"<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/2/269f3eb4c2a042598c3ffc03d97de6e966192830.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/269f3eb4c2a042598c3ffc03d97de6e966192830\" title=\"Ethereum intent network\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/2/269f3eb4c2a042598c3ffc03d97de6e966192830_2_500x500.jpeg\" alt=\"Ethereum intent network\" data-base62-sha1=\"5vFk7LbWxdZIoq2pIpcr7DMHwvC\" width=\"500\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/2/269f3eb4c2a042598c3ffc03d97de6e966192830_2_500x500.jpeg, https://ethresear.ch/uploads/default/optimized/2X/2/269f3eb4c2a042598c3ffc03d97de6e966192830_2_750x750.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/2/269f3eb4c2a042598c3ffc03d97de6e966192830_2_1000x1000.jpeg 2x\" data-dominant-color=\"523670\"></a></div><p></p>\n<p><em>Thanks to <a class=\"mention\" href=\"https://ethresear.ch/u/0xapriori\">@0xapriori</a>, <a class=\"mention\" href=\"https://ethresear.ch/u/adrianbrink\">@adrianbrink</a>, <a class=\"mention\" href=\"https://ethresear.ch/u/awasunyin\">@awasunyin</a>, and <a class=\"mention\" href=\"https://ethresear.ch/u/0xemperor\">@0xemperor</a> for reviewing this post. This is an active draft and further feedback (both solicited and unsolicited) is very welcome. All opinions and errors within are my own.</em></p>\n<p><strong>One-line summary</strong></p>\n<p>Anoma brings a universal intent machine to Ethereum, allowing developers to write applications in terms of intents, which can be ordered, solved, and settled anywhere in the Ethereum ecosystem.</p>\n<p><strong>tl; dr summary</strong></p>\n<ul>\n<li>An <em>intent</em> is a commitment to user preferences and constraints over the space of possible state transitions.</li>\n<li>The Anoma protocol brings a universal intent machine to Ethereum, allowing developers to write applications in terms of intents instead of transactions.</li>\n<li>Anoma is an <em>interface</em>, not an <em>intermediary</em> - it’s not another MEV redirection device.</li>\n<li>Anoma provides a universal intent standard which does not constrain what kinds of intents can be expressed, but allows for built-in state, network, and application interoperability.</li>\n<li>Intents and applications written with Anoma can be ordered, solved, and settled anywhere - on the Ethereum main chain, on EVM and non-EVM rollups, on Eigenlayer AVSs, on Cosmos chains, Solana, or any sufficiently programmable state machine.</li>\n<li>Anoma provides four key affordances: <em>permissionless intent infrastructure</em>, <em>intent-level composability</em>, <em>information flow control</em>, and <em>heterogeneous trust</em> - using three mechanisms: the <em>resource machine</em>, the <em>heterogeneous trust node architecture</em>, and <em>languages for explicit service commitments</em>.</li>\n<li>Anoma is compatible with any topology the Ethereum network chooses.</li>\n</ul>\n<h2><a name=\"table-of-contents-1\" class=\"anchor\" href=\"https://ethresear.ch#table-of-contents-1\"></a>Table of Contents</h2>\n<ul>\n<li><a href=\"https://ethresear.ch#introduction-2\">Introduction</a>\n<ul>\n<li><a href=\"https://ethresear.ch#motivations-3\">Motivations</a></li>\n<li><a href=\"https://ethresear.ch#personal-interlude-4\">Personal interlude</a></li>\n<li><a href=\"https://ethresear.ch#some-definitions-5\">Some definitions</a></li>\n<li><a href=\"https://ethresear.ch#interfaces-not-intermediaries-6\">Interfaces not intermediaries</a></li>\n<li><a href=\"https://ethresear.ch#where-are-we-at-7\">Where are we at?</a></li>\n<li><a href=\"https://ethresear.ch#what-does-a-universal-intent-machine-mean-for-ethereum-8\">What does a “universal intent machine” mean for Ethereum?</a></li>\n<li><a href=\"https://ethresear.ch#why-use-anoma-9\">Why use Anoma?</a></li>\n</ul>\n</li>\n<li><a href=\"https://ethresear.ch#intent-centric-applications-with-anoma-and-ethereum-10\">Intent-centric applications with Anoma and Ethereum</a>\n<ul>\n<li><a href=\"https://ethresear.ch#architecture-11\">Architecture</a>\n<ul>\n<li><a href=\"https://ethresear.ch#affordances-12\">Affordances</a>\n<ul>\n<li><a href=\"https://ethresear.ch#permissionless-intent-infrastructure-13\">Permissionless intent infrastructure</a></li>\n<li><a href=\"https://ethresear.ch#intent-level-composability-14\">Intent-level composability</a></li>\n<li><a href=\"https://ethresear.ch#information-flow-control-15\">Information flow control</a></li>\n<li><a href=\"https://ethresear.ch#heterogeneous-trust-16\">Heterogeneous trust</a></li>\n</ul>\n</li>\n<li><a href=\"https://ethresear.ch#mechanisms-17\">Mechanisms</a>\n<ul>\n<li><a href=\"https://ethresear.ch#resource-machine-18\">Resource machine</a></li>\n<li><a href=\"https://ethresear.ch#heterogeneous-trust-node-architecture-20\">Heterogeneous trust node architecture</a></li>\n<li><a href=\"https://ethresear.ch#service-commitment-languages-21\">Service commitment languages</a></li>\n</ul>\n</li>\n<li><a href=\"https://ethresear.ch#example-applications-22\">Example applications</a>\n<ul>\n<li><a href=\"https://ethresear.ch#multichat-23\">Multichat</a></li>\n<li><a href=\"https://ethresear.ch#public-signal-24\">Public Signal</a></li>\n<li><a href=\"https://ethresear.ch#scale-free-money-25\">Scale-free money</a></li>\n<li><a href=\"https://ethresear.ch#promise-graph-26\">Promise Graph</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"https://ethresear.ch#topology-27\">Topology</a>\n<ul>\n<li><a href=\"https://ethresear.ch#rollups-and-l2s-28\">Rollups and L2s</a></li>\n<li><a href=\"https://ethresear.ch#plasma-29\">Plasma</a></li>\n<li><a href=\"https://ethresear.ch#eigenlayer-and-service-providers-30\">EigenLayer and service providers</a></li>\n<li><a href=\"https://ethresear.ch#topology-implementation-requirements-31\">Topology implementation requirements</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"https://ethresear.ch#challenges-and-tradeoffs-32\">Challenges and tradeoffs</a></li>\n<li><a href=\"https://ethresear.ch#request-for-comment-33\">Request for comment</a></li>\n</ul>\n<h2><a name=\"introduction-2\" class=\"anchor\" href=\"https://ethresear.ch#introduction-2\"></a>Introduction</h2>\n<p>In this post, I will describe how Anoma can provide a universal intent machine for Ethereum - both the Ethereum base chain and the Ethereum ecosystem of rollups, searchers/solvers, and other network participants. What <em>is</em> a universal intent machine, you might ask? In what follows I shall attempt to clearly define this concept and what it means for the Ethereum ecosystem - but first I want to explain my motivations, tell you a bit about my personal history with Ethereum, and clarify some essential background context.</p>\n<h3><a name=\"motivations-3\" class=\"anchor\" href=\"https://ethresear.ch#motivations-3\"></a>Motivations</h3>\n<p>I have three key motivations in writing this piece. First, I want to establish a conceptual and discursive bridge to the Ethereum research community, which Anoma has historically operated somewhat separately from. I think that this separation was necessary in order for us to independently develop useful concepts, but the time has now come to reunite. Second, I want to clearly articulate to potential developers and users of applications built with Anoma what affordances Anoma can provide and why. I do this in order to set accurate expectations for what I think the technology - and often, any intent technology - can and cannot do, so that developers and users can make informed choices. Third, I want to reach out to Ethereum application developers in order to better understand their needs, see which of our ideas might be most helpful, and map out who specifically might be interested in collaborating on what.</p>\n<h3><a name=\"personal-interlude-4\" class=\"anchor\" href=\"https://ethresear.ch#personal-interlude-4\"></a>Personal interlude</h3>\n<p><em>Forgive me a bit of self-indulgence here, but I think it will help contextualize the history.</em></p>\n<p>My blockchain development journey actually started with Ethereum. In the summer of 2017, I started playing around with the system, and I was looking for a good starter Solidity development project to learn how smart contracts worked. These were the early days of decentralized exchanges (pre-Uniswap). At the time, I briefly used a since-defunct exchange called EtherDelta, and I thought it was a shame that someone had gone to all the trouble to build a complicated smart contract, orderbook server, and web frontend to support only ERC20 fungible tokens, when - to me - a key benefit of Ethereum’s fully-featured VM was the ability to generalize and support any asset representation a user wanted. I thought the best way to test this hypothesis was to try to implement it myself, so <a href=\"https://github.com/ProjectWyvern/wyvern-ethereum/commit/32af652eda2aedab82749c5c5479bc375f382a1b\" rel=\"noopener nofollow ugc\">I did</a> - and I decided to call it “Wyvern”.</p>\n<p>Wyvern was really a proto-intent system - and the whitepaper I wrote at the time <a href=\"https://github.com/ProjectWyvern/wyvern-protocol/blob/master/build/whitepaper.pdf\" rel=\"noopener nofollow ugc\">uses the word</a> - but I didn’t have a clear mathematical concept in mind then, nor an understanding of what a generalized intent system would really entail. Nevertheless, working on Wyvern taught me quite a bit about how Ethereum and the EVM worked, and also that they might not be the best fit for intents - or, at least, didn’t seem to have been designed for them. Around the time I published the first version of Wyvern, <a href=\"https://www.crunchbase.com/organization/opensea\" rel=\"noopener nofollow ugc\">a startup company</a> was founded by Devin Finzer and Alex Atallah called OpenSea. OpenSea - aiming to create a marketplace for non-fungible collectibles, and thus in need of a more generalized exchange protocol - became the first (and more-or-less only) user of Wyvern, which they used <a href=\"https://opensea.io/blog/articles/introducing-seaport-protocol\" rel=\"noopener nofollow ugc\">until 2022</a>. I didn’t do much work on Wyvern after 2018 - but luckily, since it was already generalized, OpenSea could add features such as batch sales and multi-asset payments without needing any changes to the core protocol.</p>\n<blockquote>\n<p>Aside: Believe it or not, the Wyvern contracts are <a href=\"https://www.paradigm.xyz/2024/03/how-to-raise-the-gas-limit-1\" rel=\"noopener nofollow ugc\">apparently still #1 in Ethereum DEX state consumption</a>. In retrospect, I really wish I had optimized my Solidity code. (I also looked for historical gas consumption data, but didn’t easily find it)</p>\n</blockquote>\n<p>After building Wyvern, I wanted to explore other aspects of blockchain systems design - particularly interoperability - and I was lucky enough to land a position at the (now defunct) Tendermint company, where I worked on the <a href=\"https://cosmos.network\" rel=\"noopener nofollow ugc\">Cosmos</a> project, specifically IBC, until the launch of IBC <a href=\"https://cointelegraph.com/news/cosmos-launches-stargate-paving-the-way-for-interoperable-defi-applications\" rel=\"noopener nofollow ugc\">in 2021</a>. After Wyvern and IBC, I felt like I had a few key pieces of the protocol puzzle, but not the whole picture - so I decided to co-found Anoma with <a class=\"mention\" href=\"https://ethresear.ch/u/awasunyin\">@awasunyin</a> and <a class=\"mention\" href=\"https://ethresear.ch/u/adrianbrink\">@adrianbrink</a> to figure out if I could piece together the rest. Anoma started with <a href=\"https://anoma.net/vision-paper.pdf\" rel=\"noopener nofollow ugc\">a vision</a>, but little idea of how exactly to implement it. This is the primary reason why we didn’t engage with the Ethereum ecosystem earlier on - we didn’t have a clear idea of what we wanted to do, and we didn’t know what Anoma could offer. Through years of research and design - most of it not by myself but rather by the brilliant and compassionate folks in the Anoma research ecosystem - we’ve come to a much better understanding of what Anoma is, and we now have a much better idea of what Anoma can bring to the Ethereum ecosystem. I’m happy to be coming back.</p>\n<h3><a name=\"some-definitions-5\" class=\"anchor\" href=\"https://ethresear.ch#some-definitions-5\"></a>Some definitions</h3>\n<p>Before discussing Anoma and Ethereum, I want to clarify what I mean by those words. In common language, words for blockchain networks, such as “Ethereum”, “Cosmos”, and “Anoma”, are commonly used to bundle together six distinct components:</p>\n<ul>\n<li>A <em>protocol</em>, defining what transactions do - in Ethereum’s case, the EVM.</li>\n<li>A specific <em>security model</em>, defining how blocks are produced - in Ethereum’s case, Gasper.</li>\n<li>A <em>network</em> or <em>ecosystem</em> of connected machines (physical and virtual) - in Ethereum’s case, the Ethereum ecosystem, including the base chain validators, rollups/L2s, bridges, off-chain services, etc.</li>\n<li>A <em>history</em> (all blocks since genesis)</li>\n<li>A nominal <em>asset</em> - in Ethereum’s case, ETH.</li>\n<li>A <em>community</em> of people who self-identify as members - in Ethereum’s case, the Ethereum community.</li>\n</ul>\n<p>In this post - which is addressed to the Ethereum <em>community</em> - I shall be talking <em>only</em> about possible relationships between the Ethereum and Anoma <em>protocols</em> and <em>networks</em> - so when I use the word “Ethereum” or “Anoma”, I mean only these components. Possible relationships between <em>histories</em>, <em>assets</em>, and <em>security models</em> are a fascinating topic, but one which I think had better be covered in a separate post.</p>\n<h3><a name=\"interfaces-not-intermediaries-6\" class=\"anchor\" href=\"https://ethresear.ch#interfaces-not-intermediaries-6\"></a>Interfaces not intermediaries</h3>\n<p>I also want to clarify that Anoma aims to provide a universal intent machine <em>interface</em> for applications - <em>not</em> an intent <em>intermediary</em>. What’s the difference? An <em>interface</em>, as I use the word here, is simply a protocol which translates, or represents, one semantics - in this case, declarative intent semantics - in terms of another - in this case, imperative ordering, compute, and storage semantics of underlying machines. TCP/IP, for example, is an interface - TCP translates the semantics of declarative ordered packet delivery into imperative send, retry, and window management semantics of underlying network hardware. An <em>intermediary</em>, on the other hand, is a particular network participant (possibly a chain or network) through which data (such as intents) flow. Many bridges in the Ethereum ecosystem - multisignature and chain alike - are intermediaries, as are, for example, banks in the US banking system which simply reissue dollars. <em>Interfaces</em> are simply code - freely copied, implemented anywhere, and usable by everybody. <em>Intermediaries</em>, however, are actors in a network - sometimes valuable ones - but their privileged positions often allow to extract rents, introduce additional security assumptions, or add unnecessary latency and complexity. The Anoma protocol is an interface - not an intermediary. There is no “Anoma chain” to which you must send intents.</p>\n<h3><a name=\"where-are-we-at-7\" class=\"anchor\" href=\"https://ethresear.ch#where-are-we-at-7\"></a>Where are we at?</h3>\n<p>The final (I promise!) clarifying note: Anoma is in active development and not yet finished. Ongoing research will continue to evolve the live system. We’ve finished enough research (indexed <a href=\"https://art.anoma.net\" rel=\"noopener nofollow ugc\">here</a>) that the overall design is pretty clear, but some details and priorities in implementation and deployment are not yet fixed. We think the design is at a stage which is possible to articulate and communicate, and we want to get feedback on it from Ethereum’s perspective (hence this post). The first full implementation of Anoma (in Elixir!) is also in the works, and will be open source soon.</p>\n<h3><a name=\"what-does-a-universal-intent-machine-mean-for-ethereum-8\" class=\"anchor\" href=\"https://ethresear.ch#what-does-a-universal-intent-machine-mean-for-ethereum-8\"></a>What does a “universal intent machine” mean for Ethereum?</h3>\n<p>At a high level, three things:</p>\n<ol>\n<li>Using Anoma, developers can write applications in terms of intents and distributed intent machines, instead of transactions and specific state machines. These intents can request particular network actors to perform roles such as computational search, data storage, intent matching, and transaction ordering, and they can disclose specific information to selected parties chosen either initially by the user authoring the intent, or programmatically during the intent matching and settlement process.</li>\n<li>Anoma provides a universal standard for intent and application formats which does not constrain what kinds of intents can be expressed (beyond the fundamental constraints - e.g. intents must be computable functions), but allows for state, network, and application interoperability. Broadly, Anoma standardizes what it takes to verify that an intent has been satisfied - but not how the solution is computed.</li>\n<li>These intents and applications written with Anoma can be ordered, solved, and settled anywhere - on the Ethereum main chain, on EVM and non-EVM rollups, on Eigenlayer AVSs, on Cosmos chains, Solana, etc. - anywhere an Anoma protocol adapter (which the post will cover in more detail later) is deployed.</li>\n</ol>\n<p>The rest of this post will expand on the details here - but first, I want to motivate why we think this kind of universal intent machine standard may be compelling.</p>\n<h3><a name=\"why-use-anoma-9\" class=\"anchor\" href=\"https://ethresear.ch#why-use-anoma-9\"></a>Why use Anoma?</h3>\n<p>In my view, a universal intent machine standard has three key benefits: intent-level and intent machine <em>composability</em>, application <em>portability</em>, and the availability of permissionless <em>intent infrastructure</em>. I’ll expect upon these aspects further in what follows, but in brief:</p>\n<ul>\n<li>Intent-level composability allows applications to compose interactions at the intent level, not just the transaction level. This unifies liquidity (as much as possible given heterogeneous preferences) and allows users to precisely select which decisions they would like to make themselves, which decisions they would like to delegate to specific network operators, and what constraints they would like to be enforced on those delegated decisions.</li>\n<li>Intent machine composability allows users to treat a distributed system of many computers, chains, and networks, as if it were a single intent machine, while that intent machine is internally composed of many smaller or special-purpose intent machines networked together.</li>\n<li>Application portability allows applications to move freely across concurrency and security domains without additional development work or protocol incompatibility barriers. Applications written for Anoma can treat the entire Anoma network as a virtualized state space - they do not need to be deployed separately to different chains, integrate different bridges, or pick any specific security model.</li>\n<li>Permissionless intent infrastructure allows applications to make use of existing nodes and chains running the Anoma protocol. Of course, what nodes choose to do is up to them - but a standardized protocol allows application developers to think only about their application, make use of services provided by existing node operators, and not spend any time building complicated, custom off-chain infrastructure - and it allows node operators to focus purely on the services they want to provide and the conditions under which they want to provide them, without needing to care (or even know) which applications are using those services.</li>\n</ul>\n<p>Now, I’d like to hope that maybe you’re interested (or, if not, you probably wouldn’t be reading this line of text anyways). What do intent-centric applications actually look like?</p>\n<h2><a name=\"intent-centric-applications-with-anoma-and-ethereum-10\" class=\"anchor\" href=\"https://ethresear.ch#intent-centric-applications-with-anoma-and-ethereum-10\"></a>Intent-centric applications with Anoma and Ethereum</h2>\n<blockquote>\n<p>For more information on what Anoma means by the word “intent”, see <a href=\"https://anoma.net/blog/abstract-intent-machines\" rel=\"noopener nofollow ugc\">this blog post</a>.</p>\n</blockquote>\n<p>I’m going to split this section into two parts: <em>architecture</em> and <em>topology</em>. Let me first define what I mean by those concepts. The <em>architecture</em> of a protocol is a mathematical specification of what the protocol is and does - typically, what (complex) pattern of messages will be sent in response to (complex) patterns of messages received. The <em>topology</em> of a network is the specific structure of connections - in the case of intent-based systems, connections induced by user choices of what intents to craft and where to send them. I find drawing a clear line between <em>architecture</em> and <em>topology</em> very helpful in designing and analyzing distributed, networked systems, for three reasons:</p>\n<ul>\n<li>The <em>architecture</em> and <em>topology</em> are always cleanly separable. Particular identities or connections can only be configuration parameters of a particular protocol - one could always copy the protocol (in design or in code), and change the particular parameters of who to connect to - thus keeping the same <em>architecture</em> but picking an arbitrarily different <em>topology</em>. More limited architectures may support only a certain subset of topologies, or may provide certain guarantees only for a certain subset of topologies - but the particular topology is always a matter of runtime configuration.</li>\n<li>The <em>architecture</em> and <em>topology</em> are chosen by different parties - the <em>architecture</em> of a particular protocol is chosen by whoever designed the protocol, while the <em>topology</em> of a network using that protocol is chosen (in a distributed fashion) by the users of that network making choices. Wearing my protocol designer hat, I consider it my responsibility to clearly define the <em>architecture</em> - but I have neither desire nor ability to influence the <em>topology</em>, which is a function of decisions made by network participants, themselves often influenced by cryptoeconomic parameters.</li>\n<li>Although the protocol designers may be subject to incentives of their own, the <em>architecture</em> of a particular protocol is fixed - in a sense, it is what defines the protocol - so one can easily analyze a particular architecture as a discrete, static mathematical object. The <em>topology</em> of a live network, however, changes all the time, and is subject to incentives both inside and outside the domain of what is legible to the protocol. In other words, <em>architectural definition</em> precedes <em>topological analysis</em> (using game theory, mechanism design, or similar). In order to understand how participants might use a language, one must first define the semantics which that language can express.</li>\n</ul>\n<h3><a name=\"architecture-11\" class=\"anchor\" href=\"https://ethresear.ch#architecture-11\"></a>Architecture</h3>\n<p>Let’s start with Anoma’s architecture. What kind of architecture does Anoma have, and what can this architecture provide? In this section, I will define the architecture in two levels: the <em>affordances</em> which Anoma’s architecture offers to applications, and the <em>mechanisms</em> with which Anoma provides these affordances. I will then detail a few <em>example applications</em> which I’m particularly excited about.</p>\n<blockquote>\n<p>Note: I have tried to select a level of abstraction which will provide a solid intuition for how Anoma works and why without getting lost in extraneous implementation details - but I certainly will not have selected optimally for all audiences (or even any), so please let me know where you would like more detail and which parts don’t make sense.</p>\n</blockquote>\n<h4><a name=\"affordances-12\" class=\"anchor\" href=\"https://ethresear.ch#affordances-12\"></a>Affordances</h4>\n<p>I’ve borrowed the word “affordance” from <a href=\"https://en.wikipedia.org/wiki/Affordance\" rel=\"noopener nofollow ugc\">cognitive psychology</a> - in this context it means, simply, what application developers can <em>do</em> with Anoma - what capabilities of application design and instantiation Anoma offers that they didn’t have before. In contrast to, for example, the <a href=\"https://barnabe.substack.com/p/seeing-like-a-protocol\" rel=\"noopener nofollow ugc\">perspective of the protocol</a>, affordances describe the perspective of the application developer. To applications, Anoma provides four key affordances: <em>permissionless intent infrastructure</em>, <em>intent-level composability</em>, <em>information flow control</em>, and <em>heterogeneous trust</em>. I will detail each of these in turn.</p>\n<h5><a name=\"permissionless-intent-infrastructure-13\" class=\"anchor\" href=\"https://ethresear.ch#permissionless-intent-infrastructure-13\"></a>Permissionless intent infrastructure</h5>\n<p><em>Permissionless intent infrastructure</em> means that - once Anoma is live - developers will be able to write complex intent-centric applications, which might need solvers, various intent pools, and multiple consensi, and deploy them to the Anoma network directly without needing to develop or operate any bespoke off-chain infrastructure of their own. The word “permissionless” can be a bit misleading - in any multi-user interaction, you’re interacting with <em>someone</em>, and they could always choose not to interact with you - but here I mean simply that Anoma provides intent infrastructure with no <em>specific</em> party (or parties) whose permission you must seek in order to use it.</p>\n<p>This is possible because Anoma nodes are topology-agnostic. Depending on the operator’s configuration and the intents received, they can act as consensus nodes, storage providers, solvers, gossip forwarders, or any other role - not just for one chain but for any number. For Anoma, topology is a matter of runtime configuration - often even negotiated over the network. What consensi users want shifts as demand shifts for particular applications and atomicity between particular partitions of state, and consensus providers must themselves shift to meet this demand.</p>\n<p>One concern with intent systems raised by, among others, <a href=\"https://www.paradigm.xyz/2023/06/intents\" rel=\"noopener nofollow ugc\">Paradigm and Flashbots</a>, is that intent systems could lead to centralization if specific parts of the intent lifecycle are performed by designated trusted parties (who would then have an undue influence). On the other hand, Chitra, Kulkarni, Pai, and Diamandis <a href=\"https://arxiv.org/abs/2403.02525\" rel=\"noopener nofollow ugc\">recently showed</a> that competitive solver markets could drive most solvers out and lead to oligopoly. Permissionless intent infrastructure does not change the mechanism design landscape, but it does remove all protocol barriers to decentralization and mechanism experimentation. The likelihood of designated trusted infrastructure or oligopoly is much lower when intents are built into the core protocol and nodes can be spun up on demand, and tracking intent satisfaction in the protocol allows for users to much more easily automatically switch between providers (or credibly threaten to) when their interests aren’t being sufficiently optimized for.</p>\n<h5><a name=\"intent-level-composability-14\" class=\"anchor\" href=\"https://ethresear.ch#intent-level-composability-14\"></a>Intent-level composability</h5>\n<p><em>Intent-level composability</em> means that application interactions can be composed at the intent level instead of at the transaction level. If applications use intents, but can be composed only with transactions, liquidity is fragmented between applications, and users cannot access the entire intent pool without writing an application intent aggregation interface on top of all the applications which provide liquidity of a relevant nature. Intent-level composability unifies this intent liquidity pool, such that users’ intents can be composed even with intents from another application the user has never heard of and never needs to think about - arbitrary composition is possible as long as the criteria in the user’s intent are satisfied.</p>\n<p>Intent-level composability also opens up efficiency improvements possible only when users can articulate precisely the nature and granularity of their preferences, such that their intents can be more flexibly composed - and more surplus returned back - than if the user had articulated more specific preferences than they actually have. Suppose that I want to swap USDC for ETH, and I’m happy to receive ETH on either the main chain, Optimism, or zkSync. With intent-level composability, my intent can be matched with the best offer available to settle on any of those chains - or perhaps even partially settled in multiple places, if I’m willing to accept the promise of a staked solver with liquidity in multiple places.</p>\n<h5><a name=\"information-flow-control-15\" class=\"anchor\" href=\"https://ethresear.ch#information-flow-control-15\"></a>Information flow control</h5>\n<p><em>Information flow control</em> means that developers of Anoma applications - and users of these applications - can reason precisely about what information actions taken in their application disclose to whom. Anoma provides information flow control at three distinct levels of the system:</p>\n<ul>\n<li><em>State-level information flow control</em> describes what can be seen by whom after transaction creation and execution. For example, a shielded transaction (as in <a href=\"https://z.cash\" rel=\"noopener nofollow ugc\">Zcash</a>) only reveals specific state changes and a proof that they satisfied required invariants, while a transparent transaction reveals all involved data to all observers.</li>\n<li><em>Intent-level information flow control</em> describes what can be seen by whom during intent solving, intent composition, and transaction creation. For example, a user may elect to disclose certain information to well-known solvers in order to help those solvers find valid matches quickly, but elect not to disclose that information to other solvers which they do not know.</li>\n<li><em>Network-level information flow control</em> describes what metadata going around the network can be seen by whom. For example, a user may elect to disclose certain physical network addresses and transport options - the Bluetooth ID of a phone, for example - only to a few well-known friends, lest the information revealed by the address render the user vulnerable to denial-of-service or deanonymization.</li>\n</ul>\n<p>Information flow control is a declarative specification of what should be disclosed to whom under which conditions. Desired information flow control properties constrain, but do not fix, the choice of specific cryptographic primitives such as regular encryption, succinct zero-knowledge proofs, or partially or fully homomorphic encryption - as long as the primitives chosen preserve the desired properties, the choice can be made on the basis of implementation availability, computational efficiency, and cryptographic interoperability. This is not a new concept - it is pretty well-covered in existing computer science research literature. In particular, the framework used by Anoma has been inspired and informed by the <a href=\"https://dl.acm.org/doi/pdf/10.1145/3453483.3454074\" rel=\"noopener nofollow ugc\">Viaduct paper</a>, although Anoma differs in providing a dynamic runtime instead of a one-shot compiler.</p>\n<h5><a name=\"heterogeneous-trust-16\" class=\"anchor\" href=\"https://ethresear.ch#heterogeneous-trust-16\"></a>Heterogeneous trust</h5>\n<p><em>Heterogeneous trust</em> means that Anoma application developers and users can make their own assumptions about the behavior of other participants on the network and their own choices about who to entrust with specific service-provisioning roles in the operation of their application, while reasoning about these assumptions and choices explicitly and detecting when their assumptions do not match their observations. In particular, most applications need three basic services: reliable <em>ordering</em> of intents and transactions related to the application, reliable <em>storage</em> of application data, and efficient <em>compute</em> of new application states and temporal statistics. Anoma makes all three of these services programmable:</p>\n<ul>\n<li>Using <em>programmable ordering</em>, users and developers can choose who will order their intents and transactions. These choices may be made independently for each intent, and conditionally delegated to third parties. For example, a user may request their intent to be ordered by A or B, and delegate the choice of whether it is in fact ordered by A or B to a solver, to be made on the basis of relative price.</li>\n<li>Using <em>programmable storage</em>, users and developers can choose who will store the data needed by their application. These choices may be made independently for each intent and each piece of data, conditionally delegated to third parties, and changed over time. For example, an application may choose to store recently-used data with the same nodes who will order its transactions, but move stale data to a cheaper long-term storage provider.</li>\n<li>Using <em>programmable compute</em>, users and developers can choose who will perform the computation required for their intents, transactions, and ongoing application usage. For example, an application may choose to pay low-latency solver nodes for the compute required to match intents, but pay higher-latency but cheaper-per-FLOP server farms for long-term statistical indexing.</li>\n</ul>\n<p>Heterogeneous trust is also a pre-existing concept in the academic research literature, from which we’ve been lucky to draw many ideas, such as <a href=\"https://arxiv.org/abs/2011.08253\" rel=\"noopener nofollow ugc\">Heterogeneous Paxos</a>. A lot of study still remains here, however - and I’m hopeful that blockchain projects (who desperately need this research) can step up a bit more to help organize and fund it.</p>\n<h4><a name=\"mechanisms-17\" class=\"anchor\" href=\"https://ethresear.ch#mechanisms-17\"></a>Mechanisms</h4>\n<p>How is all of this implemented? This post is already long enough without fully specifying exactly how Anoma works, but I will detail three key mechanisms here: the <em>resource machine</em>, which implements intents, the <em>heterogeneous trust node architecture</em>, which supports many networks with one piece of node software, and <em>languages for explicit service commitments</em>, which match user requests and operator offers for ordering, compute, and storage services.</p>\n<h5><a name=\"resource-machine-18\" class=\"anchor\" href=\"https://ethresear.ch#resource-machine-18\"></a>Resource machine</h5>\n<p>The Anoma <em>resource machine</em> implements intents without loss of generality. In relation to the affordances above, the resource machine provides state-level and intent-level information flow control, intent-level composability, and the programming framework for ordering, storage, and compute services required for heterogeneous trust. Heterogeneous trust also requires cross-domain state synchronization and verification, which the resource machine’s state architecture is designed to make simple and efficient.</p>\n<blockquote>\n<p>Note: I’ve chosen to simplify some of the definitions here for conceptual legibility of the key intuitions. For full details, please see the <a href=\"https://zenodo.org/records/10498991\" rel=\"noopener nofollow ugc\">resource machine ART report</a>.</p>\n</blockquote>\n<p>The basic concept of the resource machine is to organize state around <em>resources</em>. Resources are immutable: they can only be created and consumed exactly once. The current state of the system can be defined as the set of resources which have been created but not yet consumed. Each resource has an associated predicate called a <em>resource logic</em> that specifies the conditions under which the resource can be created and consumed. These conditions could include, for example, the creation or consumption of other resources, data such as a signature over a specific payload, or a proof that state elsewhere in the system satisfies a certain property. A transaction is considered valid only if the logics associated with all resources created and consumed in the transaction are satisfied, the transaction balances (enforcing e.g. linearity of currency), and no resources consumed in the transaction have been consumed before (no double-spends).</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/1/1a560ae66942cc03fa5434d35fa54aa5cb46b2ee.png\" data-download-href=\"https://ethresear.ch/uploads/default/1a560ae66942cc03fa5434d35fa54aa5cb46b2ee\" title=\"Resource logic examples\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/1/1a560ae66942cc03fa5434d35fa54aa5cb46b2ee_2_690x369.png\" alt=\"Resource logic examples\" data-base62-sha1=\"3KYL5VfPeSVSDYddpInofM2Lsp8\" width=\"690\" height=\"369\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/1/1a560ae66942cc03fa5434d35fa54aa5cb46b2ee_2_690x369.png, https://ethresear.ch/uploads/default/optimized/2X/1/1a560ae66942cc03fa5434d35fa54aa5cb46b2ee_2_1035x553.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/1/1a560ae66942cc03fa5434d35fa54aa5cb46b2ee_2_1380x738.png 2x\" data-dominant-color=\"EEEEEE\"></a></div><p></p>\n<p><em>Diagram credit Yulia Khalniyazova</em></p>\n<h6><a name=\"formal-definitions-19\" class=\"anchor\" href=\"https://ethresear.ch#formal-definitions-19\"></a>Formal definitions</h6>\n<p>A resource is a tuple <span class=\"math\">R = (l, label, q, v, nonce)</span> where:</p>\n<ul>\n<li><span class=\"math\">Resource = \\mathbb{F}_{l} \\times \\mathbb{F}_{label} \\times \\mathbb{F}_Q \\times \\mathbb{F}_{v} \\times \\mathbb{F}_{nonce}</span></li>\n<li><span class=\"math\">l: \\mathbb{F}_{l}</span> is a succinct representation of the predicate associated with the resource (resource logic)</li>\n<li><span class=\"math\">label: \\mathbb{F}_{label}</span> specifies the fungibility domain for the resource</li>\n<li><span class=\"math\">q: \\mathbb{F}_Q</span> is an number representing the quantity of the resource</li>\n<li><span class=\"math\">v: \\mathbb{F}_{v}</span> is the fungible data of the resource (data which does not affect fungibility)</li>\n<li><span class=\"math\">nonce: \\mathbb{F}_{nonce}</span> guarantees the uniqueness of the resource computable components</li>\n</ul>\n<p>From a resource <span class=\"math\">r</span> may be computed:</p>\n<ul>\n<li><span class=\"math\">r.kind = h_{kind}(r.l, r.label)</span></li>\n<li><span class=\"math\">r.\\Delta = h_{\\Delta}(r.kind, r.q)</span></li>\n</ul>\n<p>A transaction is a composite structure <span class=\"math\">TX = (rts, cms, nfs, \\Pi, \\Delta, extra)</span>, where:</p>\n<ul>\n<li><span class=\"math\">rts \\subseteq \\mathbb{F}_{rt}</span> is a set of roots of the commitment tree</li>\n<li><span class=\"math\">cms \\subseteq  \\mathbb{F}_{cm}</span> is a set of created resources’ commitments.</li>\n<li><span class=\"math\">nfs \\subseteq \\mathbb{F}_{nf}</span> is a set of consumed resources’ nullifiers.</li>\n<li><span class=\"math\">\\Pi: \\{ \\pi: ProofRecord\\}</span> is a set of proof records.</li>\n<li><span class=\"math\">\\Delta_{tx}: \\mathbb{F}_{\\Delta}</span> is computed from <span class=\"math\">\\Delta</span> parameters of created and consumed resources. It represents the total delta change induced by the transaction.</li>\n<li><span class=\"math\">extra: \\{(k, d): k \\in \\mathbb{F}_{key}, d \\subseteq \\mathbb{F}_{d}\\}</span> contains extra information requested by the logics of created and consumed resources</li>\n</ul>\n<p>A transaction is considered <em>valid</em> with respect to a previous state if and only if:</p>\n<ul>\n<li><span class=\"math\">rts</span> contains valid commitment tree roots that are correct inputs for the membership proofs</li>\n<li><span class=\"math\">nfs</span> contains valid nullifiers that correspond to consumed resources, and none of these nullifiers have been previously revealed</li>\n<li>input resources have valid resource logic proofs and the compliance proofs associated with them</li>\n<li>output resources have valid resource logic proofs and the compliance proofs associated with them</li>\n<li><span class=\"math\">\\Delta</span> is computed correctly</li>\n</ul>\n<p>A transaction is considered <em>balanced</em> if and only if <span class=\"math\">\\Delta = 0</span>.</p>\n<p>Applying a transaction to the state simply entails adding new commitments and nullifiers to their respective Merkle tree and set. <em>Transaction candidates</em> - functions which generate transactions - are provided access to the current state and can look up resources by kind, which allows for post-ordering state-dependent updates without conflicts.</p>\n<p>Compared to the EVM (and other blockchain VMs such as the SVM and Move), the resource machine unbundles three aspects which these VMs intertwine: the <em>state architecture</em>, the <em>instruction set</em>, and the <em>message-passing model</em>. The EVM, for example, specifies:</p>\n<ul>\n<li>A <em>state architecture</em> where state is read and written in 256-bit blocks, partitioned by smart contract addresses, and encoded into a Merkle-Patricia Trie.</li>\n<li>An <em>instruction set</em> for stack-based computations with a 256-bit native word type and an assortment of specially optimized Ethereum-related instructions (e.g. <code>ecrecover</code>).</li>\n<li>A <em>message-passing model</em> where one smart contract owns the execution frame at a time, and sending a message to another contract switches control of the execution frame to the message recipient.</li>\n</ul>\n<p>The resource machine, by contrast, specifies <em>only</em> the state architecture. Different instruction sets can be chosen by different operators as long as valid state updates are ultimately produced, and messages can be passed in whatever fashion is most efficient in each particular case. This unbundling is necessary for native intent support, as with intents, much of the computation (expressed in an instruction set) and communication (expressed by a message-passing model) happens <em>before</em> final transaction execution and state change verification. With the resource machine, the instruction set can be chosen by whomever is performing the computation in question, and the message-passing model can simply reflect the actual intent-matching topology.</p>\n<h5><a name=\"heterogeneous-trust-node-architecture-20\" class=\"anchor\" href=\"https://ethresear.ch#heterogeneous-trust-node-architecture-20\"></a>Heterogeneous trust node architecture</h5>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/f/f70d2a9b2dae614e776808b0562601a340536cd9.png\" data-download-href=\"https://ethresear.ch/uploads/default/f70d2a9b2dae614e776808b0562601a340536cd9\" title=\"P2P overlay diagram\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/f/f70d2a9b2dae614e776808b0562601a340536cd9_2_690x417.png\" alt=\"P2P overlay diagram\" data-base62-sha1=\"zfw8W6su6OCn4FyLdGHNC3qXapX\" width=\"690\" height=\"417\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/f/f70d2a9b2dae614e776808b0562601a340536cd9_2_690x417.png, https://ethresear.ch/uploads/default/optimized/2X/f/f70d2a9b2dae614e776808b0562601a340536cd9_2_1035x625.png 1.5x, https://ethresear.ch/uploads/default/original/2X/f/f70d2a9b2dae614e776808b0562601a340536cd9.png 2x\" data-dominant-color=\"DFDDE1\"></a></div><p></p>\n<p><em>Diagram credit Naqib Zarin</em></p>\n<p>Anoma’s <em>heterogeneous trust node architecture</em> weaves together the key ordering, storage, and compute functionalities needed to provide permissionless intent infrastructure into a single piece of node software. The base of the node stack is a generalized heterogeneous P2P network stack with support for network-level information flow control. The node architecture can be split into two components: the <em>networking machine</em>, which implements the heterogeneous P2P network stack and provides interfaces for storage and compute resource provisioning, and the <em>ordering machine</em>, which implements heterogeneous consensus (total ordering), heterogeneous mempools (partial ordering), and parallel transaction execution.</p>\n<p>The <em>networking machine</em> consists of a set of communicating sub-processes which are responsible for message-passing between nodes. Anoma’s network architecture, based on <a href=\"https://arxiv.org/abs/2306.16153\" rel=\"noopener nofollow ugc\">P2P Overlay Domains with Sovereignty</a>, assumes <em>heterogeneous P2P preferences</em>: different nodes want to broadcast and subscribe to different types of intents, participate in different domains of consensus, storage, and compute, and choose different bandwidth, latency, and resource usage tradeoffs in their network connection choices. The networking machine is designed to abstract this complexity and the ever-changing physical and overlay network topologies behind an interface which simply allows sending a message to any other node or topic in the network - the internal processes within the networking machine are responsible for using network metadata to figure out where it should go and how to route it there efficiently. Generic interfaces are also provided for provisioning and requesting storage and compute.</p>\n<p>The <em>ordering machine</em> consists of a set of communicating sub-processes which are responsible for receiving transactions, partially ordering those transactions in the mempool, coming to consensus over a total order for each logical consensus clock, executing the transactions, updating the state accordingly, and sending updated state to the appropriate recipients. Both the mempool and consensus are heterogeneous - implementing <a href=\"https://github.com/anoma/typhon/blob/heterogeneous-narwhal/pubs/HeterogeneousNarwhal.pdf\" rel=\"noopener nofollow ugc\">Heterogeneous Narwhal</a> and <a href=\"https://arxiv.org/abs/2011.08253\" rel=\"noopener nofollow ugc\">Heterogeneous Paxos</a>, respectively - and the execution engine based on <a href=\"https://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf\" rel=\"noopener nofollow ugc\">CalvinDB</a> is capable of n-processor parallel scale-out.</p>\n<p>Heterogeneous chains are operationally expensive if you have to run a separate node for each one - so Anoma’s node architecture supports <em>many networks with a single node</em>. The operator simply configures which networks they’d like to participate in and the node software takes care of all the bookkeeping. Processes within the ordering machine, for example, can be spun up or spun down automatically based on which consensi the operator is participating in and how much throughput each needs at the moment.</p>\n<p>A full paper on the heterogeneous trust node architecture is still in the works, but the curious reader may be interested in <a href=\"https://anoma.net/blog/heterogeneous-paxos-and-multi-chain-atomic-commits\" rel=\"noopener nofollow ugc\">more about multi-chain atomic commits</a> or <a href=\"https://anoma.net/blog/anomas-p2p-layer\" rel=\"noopener nofollow ugc\">a brief introduction to Anoma’s P2P layer</a>.</p>\n<h5><a name=\"languages-for-explicit-service-commitments-21\" class=\"anchor\" href=\"https://ethresear.ch#languages-for-explicit-service-commitments-21\"></a>Languages for explicit service commitments</h5>\n<p><em>Languages for explicit service commitments</em> provide a way for clients (requesting a service) and servers (providing a service) to automatically negotiate the terms of that service in a way which is legible to the network, so that records can be kept of services performed as promised - and promises broken. For example, service commitments could include</p>\n<ul>\n<li>“I will vote in consensus XYZ”</li>\n<li>“I will store your data D for T weeks”</li>\n<li>“I will spend N units of CPU time searching for an answer to problem P”</li>\n</ul>\n<p>Generally, these commitments will be comprised of aspects classifiable into one of two categories: <em>safety</em> - roughly, promising not to send any messages which violate a certain invariant, and <em>liveness</em> - roughly, promising to send responses promptly when queried. Initially, I think three kinds of services are particularly important in the context of distributed systems: <em>ordering</em> services, <em>storage</em> services, and <em>compute</em> services. I will expand a bit upon each of these in turn.</p>\n<p><em>Ordering services</em> are perhaps the most basic historical function of blockchains. Satoshi actually <a href=\"https://bitcoin.org/bitcoin.pdf\" rel=\"noopener nofollow ugc\">uses the term</a> “timestamping server”. In order for multiple observers to agree on the history of a particular piece of state, they must agree on a particular party whose local order of observations will determine the order in which events (transactions) are applied to that state. This party must attest to the order in which it has received events, from which we can derive the desired liveness property: signing updates to the event graph (blocks of transactions). Consistency for other observers requires consistency of ordering attestations, from which we can derive the desired safety property: never signing two conflicting updates. Other properties of interest here could include censorship resistance - perhaps promising to construct blocks in a certain fashion (connecting directly to <a href=\"https://ethresear.ch/t/unbundling-pbs-towards-protocol-enforced-proposer-commitments-pepc/13879\">PEPC</a>).</p>\n<p><em>Storage services</em> are recently in the spotlight thanks to the concept of “data availability”, which illuminates an important nuance: it is not enough for observers that data be <em>stored</em>, it must be <em>available</em> - they must be able to retrieve it. In order to make data available, though, one must store it. I think storage services need only a liveness property: responding with the requested data, perhaps only within a certain span of time from the initial request. Thanks to content-addressing, the recipients of that data can quickly check that the correct bits were provided. Storage markets are very heterogeneous - different data needs to be retrieved by different parties, at different frequencies, and stored with different level of redundancy and distribution.</p>\n<p><em>Compute services</em> are currently split across a few different areas: <em>searching</em> performed in the Ethereum MEV supply chain, <em>indexing</em> still typically performed by web2-style services, and <em>historical statistics</em> calculated on the backend by explorers or specialized providers. I think that these various specific services are variants of one basic unit: the provision of computational infrastructure to search for a solution satisfying a particular relation, which may include network history. Safety for compute means providing the correct result, and liveness means getting a result quickly and with high likelihood.</p>\n<p>We’re still in the design process for these service commitment languages, and I see a lot of potential for collaboration. In particular, several folks in the Ethereum research community have been exploring relevant concepts: <a href=\"https://ethresear.ch/t/unbundling-pbs-towards-protocol-enforced-proposer-commitments-pepc/13879\">protocol-enforced proposer commitments</a>, whereby Ethereum block proposers can make commitments which are enforced by the core protocol, and <a href=\"https://ethresear.ch/t/unbundling-staking-towards-rainbow-staking/18683\">rainbow staking</a>, where staking services are split into “heavy” and “light” categories (the names follow from expected node operator resource requirements). I expect that clear specification of various flavors of heavy and light services as described in the post, and negotiation of who is to perform those services, how they are to be compensated, and how other network actors can tell whether or not they are performing as promised will require some form of service commitment language, and PEPC would allow these commitments to be legible to and enforceable by the core protocol. <a href=\"https://markburgess.org/promises.html\" rel=\"noopener nofollow ugc\">Promise Theory</a> may also provide helpful theoretical ammunition. I hope to develop a better understanding of the Ethereum ecosystem’s requirements here and see whether it is possible to develop a common standard.</p>\n<h4><a name=\"example-applications-22\" class=\"anchor\" href=\"https://ethresear.ch#example-applications-22\"></a>Example applications</h4>\n<p>What kinds of applications can be built with these affordances and mechanisms? I’ll save a comprehensive survey for a separate discussion, but I want to list a few here which I’m particularly excited about.</p>\n<h5><a name=\"multichat-23\" class=\"anchor\" href=\"https://ethresear.ch#multichat-23\"></a>Multichat</h5>\n<p>Multichat is a distributed chat network without any specially designated server operators. Like Slack or Discord, multichat supports permissioned channels in complex topologies. Like Signal, multichat encrypts messages and metadata. Unlike Signal, Slack, or Discord, multichat can run offline, or in an isolated physical subnetwork, using direct connections between chat participants. Unlike Slack or Discord, multichat cleanly separates protocol, operators, and interfaces, allowing different interfaces to be designed for different user needs while retaining protocol compatibility, and allowing users to choose which operators they want to trust with what roles.</p>\n<p>For more details on this application concept, see <a href=\"https://research.anoma.net/t/dreaming-of-multichat-a-chat-application-built-with-anoma/515\" rel=\"noopener nofollow ugc\">this forum thread</a>.</p>\n<h5><a name=\"public-signal-24\" class=\"anchor\" href=\"https://ethresear.ch#public-signal-24\"></a>Public Signal</h5>\n<p>Public Signal is a double-sided version of Kickstarter implemented with intents. Kickstarter is a supply-side driven platform: a group or individual wishing to produce a particular good or service and in need of funding posts a description of their product or service and organizes a campaign to raise small donations/payments in order to cover their capital expenditure requirements and labor costs. Public Signal, by contrast, is <em>demand-side driven</em>: potential purchasers of a good or beneficiaries of a service describe their preferences and are gradually matched together, generating a demand signal which potential suppliers can use to decide what to produce. The demand-side approach is particularly appealing because it can generate an otherwise difficult-to-observe signal for <em>public</em> or <em>hybrid</em> goods.</p>\n<p>For more details on this application concept, see the <a href=\"https://anoma.net/blog/publicsignal\" rel=\"noopener nofollow ugc\">Public Signal blog post</a>.</p>\n<h5><a name=\"scale-free-money-25\" class=\"anchor\" href=\"https://ethresear.ch#scale-free-money-25\"></a>Scale-Free Money</h5>\n<p>Scale-free money is a hypothetical monetary system design which allows anyone to create arbitrary denominations of money at any time for any purpose and provides the necessary infrastructure to unbundle the three key functions of money (store of value, means of exchange, and unit of account) and allow for the choice of what money to use to be made differently by different network participants without compromising their ability to enact voluntary economic interchange. To me, scale-free money is the natural synthesis of the experimental practice of <em>decentralized cryptocurrencies</em>, the anthropological record of <em>heterogeneous credit</em>, and the cybernetic approach to <em>coordination systems design</em>.</p>\n<p>For more details on this application concept, see <a href=\"https://pluranimity.org/2022/09/26/towards-heterotopia/\" rel=\"noopener nofollow ugc\">this blog post</a>.</p>\n<h5><a name=\"promise-graph-26\" class=\"anchor\" href=\"https://ethresear.ch#promise-graph-26\"></a>Promise Graph</h5>\n<p>Promise Graph, inspired in part by the <a href=\"https://galois.com/wp-content/uploads/2016/06/CW-picmet-proceedings.pdf\" rel=\"noopener nofollow ugc\">Collaborative Web</a> model of Galois, and Informal Systems’ <a href=\"https://workflow.informal.systems/\" rel=\"noopener nofollow ugc\">Workflow</a>, is a language and structured accounting logic for making and managing promises in order to effectively synchronize causally-interwoven workstreams across a distributed organization and demonstrate to customers that the organization can keep its promises. Heliax has been dogfooding the promise graph system internally for awhile now, but it is designed in particular to facilitate coordination irrespective of, and even without knowledge of, organizational boundaries. Similar to how Anoma splits the architecture of an intent-centric system from the particular topology users choose, Promise Graph splits the architecture of promise-based coordination from the particular topology of promises necessary to achieve a particular aim.</p>\n<p>For more details on this application concept, see <a href=\"https://promisegrapheliax.click\" rel=\"noopener nofollow ugc\">this description of the system</a>.</p>\n<h3><a name=\"topology-27\" class=\"anchor\" href=\"https://ethresear.ch#topology-27\"></a>Topology</h3>\n<p>Now, let’s talk topology. How does Anoma relate to the topology of the Ethereum network today? As a refresher, by <em>topology</em>, I mean specific network connections, specific security relations, and specific service providers performing specific network roles. My understanding is that the Ethereum network is still figuring out what topology it wants to have, as evidenced by the fervent discussions around shared sequencing for rollups/L2s, EigenLayer, and rainbow staking. I think the existence of such discussions is evidence of a healthy network - a network with a static topology is a network which can no longer adapt to changes in the world outside.</p>\n<p>There are infinitely many possible topologies, and I don’t know of any finite framework to classify them - so here I will simply list a few topologies under discussion in the Ethereum ecosystem and explain at a high level how Anoma could be used with them. I will also discuss how Anoma could interface with specific specialized ordering, compute, and storage service providers which already exist, and how service commitments could be made using existing assets. Finally, I will detail two components which I believe sufficient to connect Anoma to any and all of these topologies.</p>\n<h4><a name=\"rollups-and-l2s-28\" class=\"anchor\" href=\"https://ethresear.ch#rollups-and-l2s-28\"></a>Rollups and L2s</h4>\n<p>For several years now, the Ethereum ecosystem has been following the rollup-centric roadmap, where different sequencers order transactions for different rollup chains, which then post proofs and some data to Ethereum. Most rollups use the EVM, although a few have developed distinct VMs with different properties designed for particular kinds of applications. Recent proposals have brought forth the idea that many rollups may wish to share a sequencer, which is subsequently referred to as a “shared sequencer”. The Ethereum main chain could in fact play this role of the shared sequencer.</p>\n<p>Anoma could be useful for rollups in two ways:</p>\n<ul>\n<li>The resource machine could be used as a rollup execution environment instead of or in addition to the EVM, providing the rollup’s users with intent-level and transaction-level information flow control, intent-level composability, and parallel transaction execution.</li>\n<li>The heterogeneous node architecture could be used to simply run the EVM, which would allow rollup operators to participate in a shared heterogeneous P2P network on which they could connect to other rollups, Ethereum main chain validators, searchers, and other intent infrastructure operators. Heterogeneous Paxos, in particular, may be interesting for rollups with frequent cross-traffic, because it <a href=\"https://anoma.net/blog/chimera-chains\" rel=\"noopener nofollow ugc\">generalizes shared sequencers</a>.</li>\n</ul>\n<h4><a name=\"plasma-29\" class=\"anchor\" href=\"https://ethresear.ch#plasma-29\"></a>Plasma</h4>\n<p>Plasma is the name for a family of Ethereum scaling solutions which require only posting deposit transactions, withdrawal transactions, and updated state Merkle roots to the main chain, while keeping all data and compute happening “within Plasma” off-chain. The basic concept assumes a block-producing operator who publishes state roots for each new block to the main chain and sends to individual users any state updates affecting them (e.g. tokens they’ve received). Should the operator misbehave, users can themselves publish proof of state they own to the main chain in order to withdraw it. In the original Plasma design, this forced withdrawal requires a 7-day challenge period, but the addition of validity proofs (where the operator proves that each new state root is the valid result of block execution) allows for withdrawals referring to the latest state root to be processed instantly. For a comprehensive summary of Plasma research and recent work, I recommend <a href=\"https://vitalik.eth.limo/general/2023/11/14/neoplasma.html\" rel=\"noopener nofollow ugc\">Vitalik’s blog post on the topic</a>.</p>\n<p>As discussed in that blog post, even with validity proofs, several challenges remain for Plasma:</p>\n<ul>\n<li><em>Dependency tracking</em>: The EVM does not attempt to limit or explicitly track state change dependencies, so (for example) ETH held in an account in block n + 1 could have come from anywhere block n. Exiting may then require publication of the entire history, which would incur prohibitive gas costs - and this worst-case exit game would likely entail limitations on the compute and storage bandwidth of Plasma, since it must be possible to replay everything on the main chain.</li>\n<li><em>Incentive-compatible generalization</em>: Plasma works well with state objects which have a clear economic owner, such as fungible or non-fungible tokens, but it’s not clear how to adapt the incentive model to objects without a clear economic owner such as a CDP, which a user might want to pretend to have forgotten the data for if the price of ETH goes below the DAI they’ve withdrawn.</li>\n<li><em>Developer-facing complexity</em>: Plasma application developers must reason about state ownership graphs and data storage/publication incentives, adding mental overhead and introducing whole new classes of potential bugs.</li>\n</ul>\n<p>What happens if we try to build a Plasma-like construction on top of the Resource Machine, instead of the EVM? Let’s call it <em>Resource Plasma</em>. In Resource Plasma, the operator executes transactions and publishes only the Merkle root of the commitment tree, the Merkle root of the nullifier set, and a proof of correct transaction execution to the main chain. Withdrawals function as follows:</p>\n<ul>\n<li>Withdrawals with a proof made against the latest nullifier set Merkle root can be processed immediately. They also reveal the nullifier of the resource being withdrawn (so that it can no longer be withdrawn again).</li>\n<li>Withdrawals against older Merkle roots require a delay period. When the withdrawal is requested, the Plasma contract starts a “double spend” challenge game, where anyone can submit a proof that the nullifier for the resource being withdrawn was included in a later nullifier set than the Merkle root used for the withdrawal request. After the challenge period, if no conflicting nullifier has been found, the withdrawal is processed.</li>\n</ul>\n<p>Thanks to the dual commitment-nullifier system used by the resource machine, client data storage requirements are minimal - the client need only store their resources, proofs that the commitments corresponding to those resources were included in the commitment Merkle tree root, and proofs that the nullifiers corresponding to those resources were not revealed at whatever height the client last synced at. Clients wanting to withdraw instantly must update these latter proofs, but have the option to exit if the operator withholds data (in this case, the latest nullifier set). Resource Plasma thus removes the need for separate <em>dependency tracking</em> - clients need not track dependencies, only the latest state relevant to them and proofs associated with it. <em>Developer-facing complexity</em> is also reduced because this state architecture, including commitments and nullifiers, is built into the resource machine and requires no additional effort on the part of developers.</p>\n<p><em>Incentive-compatible generalization</em> is a trickier beast. One option is to have each resource designate a particular highly-available party whose signature over an attestation that they have stored the resource data is required in order to construct a valid transaction. For example, in the CDP example, perhaps it would be suitable for a quorum of operators elected by MKR holders to store data associated with all CDPs. If the CDP is to be liquidated and the owner pretends not to have the data, this quorum could then reveal the data, allowing for the liquidation of the CDP. MKR holders have an incentive to store and publish this data in order to keep the system on which their value flows depend credible. In a version of MakerDAO without governance, it is less clear what to do in this scenario. Perhaps the data could also be sent to a party who might want to liquidate the CDP, or more generally one with the opposite economic incentive. Perhaps systems such as Maker really do need a designated party (which could be a dynamic, distributed operator set) whose job it is to store the data. The resource machine makes it easy to designate who should store what state, but it doesn’t change the game design problem.</p>\n<p>Resource Plasma is no panacea. Application developers must still reason about storage handoff, and users who want to be able to receive payments while offline must pick a highly available operator who they trust to temporarily store data on their behalf (or pay the main chain itself to do so). These limitations, however, are fundamental - they could not be alleviated by a different virtual machine - and other scaling solutions such as L2s or rollups will be faced with the same constraints. This is just a sketch of stirring the Plasma and Resource Machine models together, and seeing what emerges from the resulting conceptual melange. I think there may be some compelling options here, and I plan to explore this design space further in a separate post.</p>\n<h4><a name=\"eigenlayer-and-service-providers-30\" class=\"anchor\" href=\"https://ethresear.ch#eigenlayer-and-service-providers-30\"></a>EigenLayer and service providers</h4>\n<p>Many blockchains today - including, typically, those who self-identify as data availability layers, in the modular conceptual framework - provide both <em>ordering</em> and <em>storage</em> services. Ordering services are mostly distinguished by the validator set and consensus mechanism, the combination of which determine latency, costs, and security. Storage services are often distinguished by how long the storage is to be provided for - blob storage on Ethereum, Celestia, and EigenDA, for example, is short-term, while file storage on Filecoin or Arweave is (at least nominally) permanent. Storage services are also distinguished by operators, redundancy, and optimization for specific patterns of retrieval. Many applications will likely want to use multiple storage providers with different specialties with different purposes. <em>Compute</em> services today are mostly provided by off-chain actors, as it is not typically necessary to replicate compute (since the results can be verified). I understand Ethereum ecosystem searchers/builders, indexers, and statistical data providers to all be providing compute services of various specialized flavors.</p>\n<p><a href=\"https://www.eigenlayer.xyz/\" rel=\"noopener nofollow ugc\">EigenLayer</a> provides a set of smart contracts and coordination infrastructure with which Ethereum validators can make commitments to operate additional services (AVSs), and “restake” their existing ETH staking bonds to provide security for those services (in the sense that these bonds can be slashed if the services are not performed as promised). The services performed by these validators could include ordering, storage, and compute as described here.</p>\n<p>Anoma’s service commitment languages can help both users and operators - the demand and supply sides of this service provisioning market - detail what services they are willing to provide, navigate through the services on offer, bargain for a mutually acceptable price, detect if service commitment properties have been violated, and publish evidence of defection in a standard way that can be embedded into network history and preserved as a reputation signal for future participants.</p>\n<h4><a name=\"topology-implementation-requirements-31\" class=\"anchor\" href=\"https://ethresear.ch#topology-implementation-requirements-31\"></a>Topology implementation requirements</h4>\n<p>What would be required to support these varied possible topologies? Just two main ingredients, I think: an Anoma <em>protocol adapter</em> and an Anoma <em>node sidecar</em>. I will describe each of these in turn.</p>\n<p>The Anoma <em>protocol adapter</em> is a smart contract or set of smart contracts - primarily written for the EVM, but variants could be written for other VMs or execution environments - which emulate the resource machine and thus allow for execution of Anoma-formatted transactions, synchronization of Anoma-formatted state, and verification of state changes executed elsewhere on the Anoma network.<br>\nThere may be some efficiency loss in execution emulation, but there shouldn’t be too much: since the resource machine does not fix a particular instruction set or message-passing architecture, these transactions can simply use EVM bytecode to compute new resource values and the EVM’s message-passing architecture to pass messages between different contracts required for the computation. Verification with succinct ZKPs is constant-cost anyways, so no problems there. Should the Anoma application model prove safe and popular, the EVM could easily enshrine it with a few precompiles for native performance.</p>\n<blockquote>\n<p>Slight aside: solvers will get to have some fun in this model in a pro-public-good way: now they get to compete on optimizing JIT EVM compilers, the software created for which could probably aid other languages and systems trying to target the EVM.</p>\n</blockquote>\n<p>The Anoma <em>node sidecar</em> is a process which would run alongside and communicate bidirectionally with Ethereum execution &amp; consensus clients, rollup sequencer processes, solver/searcher algorithms, etc. The node sidecar process runs all the Anoma network protocols, allowing nodes running the sidecar to connect to other Anoma nodes, subscribe to intents which are of interest to them, and solve, order, and execute as desired. This sidecar would be fully opt-in and configurable per the node operator’s preferences in terms of who they want to connect to (or not), what services they want to offer, what intents they want to receive (or even send), and how they want to process or forward them. If the node operator solves two intents to create an EVM transaction with the protocol adapter, they can submit it directly to their local web3 HTTP API - minimum latency, maximum throughput!</p>\n<p>What would this intent dataflow look like visually? Something like this:</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/3/3294ba703a672efe0a0f167477ede160368535de.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/3294ba703a672efe0a0f167477ede160368535de\" title=\"Intent dataflow\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/3/3294ba703a672efe0a0f167477ede160368535de_2_689x485.jpeg\" alt=\"Intent dataflow\" data-base62-sha1=\"7dswo2fyFraOw4kejIqWoDw4Xgi\" width=\"689\" height=\"485\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/3/3294ba703a672efe0a0f167477ede160368535de_2_689x485.jpeg, https://ethresear.ch/uploads/default/optimized/2X/3/3294ba703a672efe0a0f167477ede160368535de_2_1033x727.jpeg 1.5x, https://ethresear.ch/uploads/default/original/2X/3/3294ba703a672efe0a0f167477ede160368535de.jpeg 2x\" data-dominant-color=\"EEEDEC\"></a></div><p></p>\n<p><em>Diagram credit <a class=\"mention\" href=\"https://ethresear.ch/u/0xapriori\">@0xapriori</a></em></p>\n<h2><a name=\"challenges-and-trade-offs-32\" class=\"anchor\" href=\"https://ethresear.ch#challenges-and-trade-offs-32\"></a>Challenges and trade-offs</h2>\n<p>Nothing new is without challenges or trade-offs. I don’t want to paint with too rosy a brush here - plenty remains to be figured out, and plenty could go wrong. I see three main challenges (and I’m sure there are many which I miss):</p>\n<ul>\n<li><em>Implementation risk</em>: Anoma, although mathematically characterized at a high-level, is not yet fully implemented. We’re working to reduce uncertainty and risk here by conducting and publishing more research, writing a formal model of the protocol stack in Isabelle/HOL, and developing an initial node implementation in Elixir (to be released soon) - but there could still be mistakes in our current understanding, and there are plenty of details we need to get right (which will always be the case with any new architecture)</li>\n<li><em>Hardware requirements</em>: compared to just running a vanilla Ethereum full node, running an Ethereum full node plus the Anoma sidecar will increase resource usage somewhat. Different node operators can pick and choose which kinds of extra messages they want to receive and process, which should help, but perhaps full node operators and solo stakers are already strained to the limit. An interesting option could be to enmesh the two protocols more closely together in order to support more points in the spectrum in between light nodes and full nodes (related to the rainbow staking concept).</li>\n<li><em>Developer education</em>: writing applications for Anoma is very different not only from writing applications for the EVM or other blockchains, but even from writing applications in imperative, computation-ordering-oriented languages at all - in a certain sense, it’s more akin to a kind of declarative object-oriented programming, where you define which kinds of objects exist (resource kinds), how they can change (resource logics), and what actions users can take with them (application actions). Personally, I find this model much easier to work with after getting used to it - it provides a lot of expressive power - but it will take developers some time to learn.</li>\n</ul>\n<h2><a name=\"request-for-comment-33\" class=\"anchor\" href=\"https://ethresear.ch#request-for-comment-33\"></a>Request for comment</h2>\n<p>Thank you for reading this far! These are my initial thoughts, and I’d love to know what you think. I have four specific questions - or respond with anything you like.</p>\n<ol>\n<li>Of what I describe here, what makes sense to you? What doesn’t make sense?</li>\n<li>What challenges, risks, or trade-offs do you see that I didn’t cover?</li>\n<li>What do you think would be most valuable to the Ethereum community for us to focus on?</li>\n<li>What would you - or a team or project you know - like to collaborate on? Who should we talk to?</li>\n</ol>\n<p>Cheers!</p>\n            <p><small>5 posts - 3 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/rfc-draft-anoma-as-the-universal-intent-machine-for-ethereum/19109\">Read full topic</a></p>","link":"https://ethresear.ch/t/rfc-draft-anoma-as-the-universal-intent-machine-for-ethereum/19109","pubDate":"Mon, 25 Mar 2024 22:13:49 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19109"},"source":{"@url":"https://ethresear.ch/t/rfc-draft-anoma-as-the-universal-intent-machine-for-ethereum/19109.rss","#text":"[RFC] [DRAFT] Anoma as the universal intent machine for Ethereum"},"filter":false},{"title":"Towards Scalable Ethereum Staking: The Imperative of Stateless Clients with Compact Proof Sizes","dc:creator":"sogolmalek","category":"Economics","description":"<p><strong>TL;DR:</strong> Ethereum’s scalability and security hinge on addressing the burgeoning state size issue and mitigating the risks posed by consensus bugs. Choosing less prevalent clients becomes crucial for stakers to minimize correlated failures. Transitioning to stateless clients with compact proof sizes offers a promising solution, decoupling historical state storage from transaction processing and enhancing scalability while preserving network integrity.</p>\n<p><strong>Introduction:</strong><br>\nTo address Ethereum’s scalability and security imperatives, a paradigm shift towards stateless clients with compact proof sizes is imperative. Stateless clients offer a pragmatic approach to alleviating the burden of state size on network participants by decoupling historical state storage from transaction processing.</p>\n<p>Furthermore,  picking the right Ethereum client is really important. If many validators using the same client mess up, the consequences get worse. So, people who stake Ethereum and use less popular clients reduce their risk of losing everything. But if they stick with the popular clients, they could lose everything if things go wrong. So, it’s important for responsible stakers to choose less common clients to protect against big problems in the system.</p>\n<p>The different Ethereum clients add another layer of difficulty, especially because of the constant worry about “consensus bugs.” These bugs, like the one that caused the “infinite money supply,” can mess up the network and make Ether less valuable. Although people are trying to make the clients stronger, there’s still a risk of everything going wrong because there’s no way to directly fix the problems caused by too much data.</p>\n<p>So, picking the right Ethereum client is a big deal for staking. If lots of validators using the same client make mistakes, the punishments get worse. That’s why people need to choose less popular clients to reduce the risks and keep the network running smoothly.</p>\n<p>Dankrad Feist has an <a href=\"https://dankradfeist.de/ethereum/2022/03/24/run-the-majority-client-at-your-own-peril.html\" rel=\"noopener nofollow ugc\">inisghtful article</a> explains the concerns in details.</p>\n<p><strong>Evolution Towards Stateless Clients with Compact Proof Sizes:</strong><br>\nAddressing Ethereum’s scalability and resilience imperatives demands a transformative shift towards stateless clients augmented by compact proof sizes. Stateless clients, by decoupling historical state data storage from transaction processing, offer a promising avenue to alleviate the burden of burgeoning state size on network participants.</p>\n<p>At the core of this transition lies the imperative to devise mechanisms for succinctly representing transaction validity, while minimizing computational overhead using Zero-Knowledge proofs. we recognize that the slashing protection history is a critical database that records a validator’s local signing history. This database plays a vital role in preventing validators from signing slashable messages by keeping track of previously signed messages. The failure to migrate this database during system upgrades or client switches puts validators at risk of duplicative actions, potentially leading to severe penalties under the slashing protocol.</p>\n<p>Our EIP-X proposal centers around Creating a peer-to-peer (P2P) network of light clients leveraging Zero-Knowledge Proofs (ZKPs) for the validation of Ethereum block headers, specifically the last finalized block at the beacon chain and  presents a novel approach to enhancing the scalability, security, and efficiency of the Ethereum network without changing the core structure. It can be considered as a PIC of Verkle tree plans. This proposal outlines the technical foundation and goals for implementing such a system, with an emphasis on the Ethereum Portal Network and its potential to support this innovative solution.</p>\n<p>Integrating Zero-Knowledge Proofs (ZKPs) can significantly enhance this framework by offering a more scalable and secure approach to managing and verifying the slashing protection history without compromising on privacy or the integrity of validators’ signing history. ZKPs can allow validators to prove that they have not signed any slashable messages in the past without revealing the specific details of the messages they have signed. This capability is particularly relevant in the context of stateless clients, where the emphasis is on minimizing the storage requirements and computational overhead on validators.</p>\n<p><strong>Integrating zk-SNARK proofs of witness could potentially save both gas and disk space:</strong></p>\n<p>There’s a significant reduction in gas costs, which can lead to lower transaction fees and improved efficiency on the network.<br>\nThe disk space savings, while smaller, contribute to reducing the storage requirements, which can be particularly beneficial for nodes with limited resources.<br>\nThis analysis doesn’t account for the computational cost and time of generating zk-SNARK proofs, which is significant (around 3 seconds and 106 MB of memory for generation). These factors would need to be considered when evaluating the overall benefits and trade-offs of implementing such a system.<br>\n<a href=\"https://eips.ethereum.org/EIPS/eip-2929\" rel=\"noopener nofollow ugc\">EIP-2929</a> introduces a  simpler gas cost schedule that directly charges for accessing  subtree  and an element within the subtree. however Its interesting to explore the pptential optimization of gas cost used and disc resources for ZKP of blockwitness in the meanwhile where Verkle tries are under development.</p>\n<p>Comparing with Verkle tries, which aim to optimize storage and access through a different approach, zk-SNARKs offer a complementary strategy by providing privacy and compression benefits. The choice between using Verkle tries and zk-SNARKs (or a combination of both) would depend on the specific requirements of the application, including the need for privacy, the amount of data to be stored, and the computational resources available for proof generation.</p>\n<p>Vitalik has a brilliant and insightful <a href=\"https://notes.ethereum.org/@vbuterin/witness_gas_cost_2#Simple-Summary\" rel=\"noopener nofollow ugc\">proposal</a> on redesign how witness gas costs would work in a more principled way that covers accounts, storage slots and contract code and the witness lengths of each one after a switch to Verkle trees.<br>\nI’ve used that insight to estiamte **potential improvement in gas cost and resrouce usage of ZKP **(snark) of witness for EIP-x proposal:</p>\n<p><strong>Step 1:</strong> Calculate Traditional Witness Gas Costs<br>\nAverage Gas Per Byte (Traditional): Given as 9.5 gas.<br>\nAverage Witness Size: Given as 200 bytes.<br>\nCalculate Gas Cost for Traditional Witness:<br>\nFormula: Average Gas Per Byte (Traditional) * Average Witness Size<br>\nCalculation: 9.5 gas/byte * 200 bytes = 1,900 gas</p>\n<p><strong>Step 2:</strong> Estimate zk-SNARKs Gas Costs<br>\nFor zk-SNARKs, the direct gas cost per byte wasn’t provided, so we used the same average gas per byte as the traditional method for comparison purposes.</p>\n<p>Assumed Average Gas Per Byte (zk-SNARKs): 9.5 gas (same as traditional for comparison).<br>\nzk-SNARK Proof Size: Given as 128 bytes.<br>\nCalculate Gas Cost for zk-SNARKs:<br>\nFormula: Average Gas Per Byte (zk-SNARKs) * zk-SNARK Proof Size<br>\nCalculation: 9.5 gas/byte * 128 bytes = 1,216 gas</p>\n<p><strong>Step 3:</strong> Calculate Gas Savings with zk-SNARKs<br>\nDifference in Gas Costs: Subtract the zk-SNARKs gas cost from the traditional witness gas cost.<br>\nCalculation: 1,900 gas (Traditional) - 1,216 gas (zk-SNARKs) = 684 gas saved<br>\nStep 4: Calculate Disk Space Savings<br>\nDifference in Disk Space Requirements: Subtract the zk-SNARK proof size from the average traditional witness size.<br>\nCalculation: 200 bytes (Traditional) - 128 bytes (zk-SNARKs) = 72 bytes saved</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/4/4f908d60e6a78d6ba84791bd463325d3c679036c.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/4f908d60e6a78d6ba84791bd463325d3c679036c\" title=\"Bildschirmfoto 2024-03-24 um 13.43.51\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/4/4f908d60e6a78d6ba84791bd463325d3c679036c_2_690x415.jpeg\" alt=\"Bildschirmfoto 2024-03-24 um 13.43.51\" data-base62-sha1=\"blRq9ZpXXQ0RPnkTscC3Ljgs4CE\" width=\"690\" height=\"415\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/4/4f908d60e6a78d6ba84791bd463325d3c679036c_2_690x415.jpeg, https://ethresear.ch/uploads/default/optimized/2X/4/4f908d60e6a78d6ba84791bd463325d3c679036c_2_1035x622.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/4/4f908d60e6a78d6ba84791bd463325d3c679036c_2_1380x830.jpeg 2x\" data-dominant-color=\"E9C1C1\"></a></div><p></p>\n<p>Here’s a summary of the comparison between traditional witness data and using zk-SNARKs, displayed in both a table and a chart:<br>\n</p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/b/b0033976833d63ca0a55b8da6eb6fdf840cc1ef1.png\" data-download-href=\"https://ethresear.ch/uploads/default/b0033976833d63ca0a55b8da6eb6fdf840cc1ef1\" title=\"Bildschirmfoto 2024-03-24 um 13.44.03\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/b/b0033976833d63ca0a55b8da6eb6fdf840cc1ef1_2_690x110.png\" alt=\"Bildschirmfoto 2024-03-24 um 13.44.03\" data-base62-sha1=\"p74WtAOYQBqGDj4neEP5QdjdFvz\" width=\"690\" height=\"110\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/b/b0033976833d63ca0a55b8da6eb6fdf840cc1ef1_2_690x110.png, https://ethresear.ch/uploads/default/optimized/2X/b/b0033976833d63ca0a55b8da6eb6fdf840cc1ef1_2_1035x165.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/b/b0033976833d63ca0a55b8da6eb6fdf840cc1ef1_2_1380x220.png 2x\" data-dominant-color=\"F3F3F3\"></a></div><p></p>\n<p>**Futhre Comprehension between  ZKP and Verkle trie: **<br>\nVerkle trees provide a more efficient method for generating and verifying proofs in scenarios involving a large number of operations, with their verification process being fast and cost-effective for adding additional proofs. zk-SNARKs, while offering strong privacy and succinct proofs, require more time for proof generation and incur a significant cost for verification. The choice between the two would depend on the specific application requirements, including the importance of proof generation time, verification costs, and the scalability needs for handling multiple proofs or commitments.To compare the computational and verification costs between Verkle trees and zk-SNARKs, we’ll outline the key metrics for each method based on the provided data. The comparison will focus on the proof generation time, verification time, and the verification cost where applicable.</p>\n<h3><a name=\"verkle-trees-1\" class=\"anchor\" href=\"https://ethresear.ch#verkle-trees-1\"></a>Verkle Trees</h3>\n<h4><a name=\"prover-cost-2\" class=\"anchor\" href=\"https://ethresear.ch#prover-cost-2\"></a>Prover Cost:</h4>\n<ul>\n<li><strong>Operations per Opening:</strong> 256 * 4 field operations.</li>\n<li><strong>Total for 25,000 Openings:</strong> Based on 5,000 proofs leading to 25,000 openings (an extrapolation from the data provided), the total operations would be significantly higher. However, the exact operations for 25,000 openings aren’t directly provided but can be inferred to require substantial computational effort.</li>\n<li><strong>Proof Generation Time:</strong> Approximately 750 milliseconds for 5,000 proofs.</li>\n</ul>\n<h4><a name=\"verifier-cost-3\" class=\"anchor\" href=\"https://ethresear.ch#verifier-cost-3\"></a>Verifier Cost:</h4>\n<ul>\n<li><strong>Multi-Scalar Multiplications (MSM) Size:</strong> For 5,000 proofs leading to 15,000 commitments, the MSM size is 15,000.</li>\n<li><strong>Verification Time:</strong> Approximately 50-150 milliseconds.</li>\n<li><strong>Marginal Cost for Another Opening:</strong> Very low, in the order of hundreds of nanoseconds.</li>\n</ul>\n<h3><a name=\"zk-snarks-4\" class=\"anchor\" href=\"https://ethresear.ch#zk-snarks-4\"></a>zk-SNARKs</h3>\n<h4><a name=\"prover-cost-5\" class=\"anchor\" href=\"https://ethresear.ch#prover-cost-5\"></a>Prover Cost:</h4>\n<ul>\n<li><strong>Proof Generation Time:</strong> On average, 10-15 minutes (a significant difference compared to Verkle trees).</li>\n<li><strong>Note:</strong> The computational cost in terms of field operations isn’t directly provided for zk-SNARKs but is known to be substantial due to the complex cryptographic computations involved.</li>\n</ul>\n<h4><a name=\"verifier-cost-6\" class=\"anchor\" href=\"https://ethresear.ch#verifier-cost-6\"></a>Verifier Cost:</h4>\n<ul>\n<li><strong>Verification Cost:</strong> 500,000 GWEI.</li>\n</ul>\n<h3><a name=\"comparison-summary-7\" class=\"anchor\" href=\"https://ethresear.ch#comparison-summary-7\"></a>Comparison Summary</h3>\n<ul>\n<li><strong>Proof Generation Time:</strong> Verkle trees are significantly faster in proof generation (milliseconds vs. minutes for zk-SNARKs).</li>\n<li><strong>Verifier Efficiency:</strong> Verkle trees offer a fast verification process (50-150 ms) without specifying a cost in GWEI, whereas zk-SNARK verification costs 500,000 GWEI. The time for zk-SNARK verification isn’t provided but is generally considered to be fast, often in the milliseconds range.</li>\n<li><strong>Scalability for Multiple Proofs:</strong> Verkle trees have a very low marginal cost for additional openings, enhancing scalability within the same commitment. zk-SNARKs have a fixed proof size and verification cost, independent of the number of operations proved.</li>\n</ul>\n<p><strong>Objectives</strong></p>\n<p>The Ethereum network is evolving to enhance scalability and minimize the burden on nodes by reducing the volume of data they need to handle. Traditional approaches where light clients individually request data from full nodes can significantly strain the network, especially when multiple clients make concurrent requests. To address this, a new method proposed in EIP-X focuses on leveraging the beacon chain’s events. When a new block is finalized, a witness of the previous block is generated, and a zero-knowledge proof (ZKP) of that witness is created. This ZKP is then sent in a single message over the Ethereum communication protocol, Discv5, effectively disseminating all the necessary data to all participating peer-to-peer (P2P) nodes in the Trin portal network with a single call. Stateless clients, which rely on proofs rather than storing the entire state for transaction execution, are fundamental to this advancement. This solution aims to utilize ZKPs to authenticate block headers and the transitions of state they imply, within a P2P network of light clients operating on the Portal Network, thereby streamlining data verification and transmission across the network.</p>\n<p><strong>Technical Proposal Overview</strong></p>\n<ul>\n<li>\n<p>Portal Network as the Foundation: The Portal Network is designed to improve the scalability and decentralization of the Ethereum network by allowing nodes to store and serve only a subset of the entire blockchain state. It’s a network of light clients that support Ethereum’s vision of a more accessible and efficient blockchain. By leveraging the Portal Network, we can facilitate a distributed environment where light clients efficiently verify and relay block information without needing the complete blockchain state【Portal Network Introduction】.</p>\n</li>\n<li>\n<p>Integration with Zero-Knowledge Proofs: At the heart of this proposal is the use of ZKPs to ensure the integrity and correctness of the block headers shared among the light clients. ZKPs can compactly prove that a block header is correct and that the transactions it contains have been accurately processed, without revealing the entire transaction data or state changes. This mechanism significantly enhances privacy and efficiency, as it allows for the verification of transactions with minimal data requirements. (Using geth ) challenge: new fork</p>\n</li>\n</ul>\n<p><strong>Communication protocol:</strong></p>\n<p>To effectively utilize the Discv5 messaging protocol for propagating Zero-Knowledge Proofs (ZKPs) across all light client recipients on the Portal Network, it is necessary to introduce a custom message type specifically designed for ZKP dissemination. The Discv5 protocol accommodates various message types, facilitating a range of network activities including peer discovery, information sharing, and data synchronization. Each message type is associated with a request and a corresponding response, ensuring a coherent communication flow.<br>\nProposed Enhancement:<br>\nCustom ZKP Message Type Implementation:<br>\nDefinition of a Custom Message Type for ZKPs: To streamline the propagation of ZKPs, a new custom message type within the Discv5 protocol must be defined. This message type will be exclusively used for transmitting encrypted ZKP data to light clients on the Portal Network.<br>\nUnique Message ID for ZKP Messages: Upon receiving a ZKP message, nodes will identify the message type through a distinct message ID designated for ZKP content. This identification process is crucial for nodes to recognize the received data as a ZKP and to respond appropriately.<br>\nNetwork Adaptation to Support New Message Type: For the seamless integration and support of the new ZKP message type, the entire P2P network must update to recognize and process the new message ID. This adaptation ensures that ZKP messages are correctly handled and propagated across the network.<br>\nRole of Custom Messages in Network Connectivity:<br>\nThese custom messages are pivotal in enhancing the network’s connectivity and efficiency. By allowing nodes to accurately locate peers, exchange ZKPs, and maintain up-to-date state information, the network can achieve higher levels of security and scalability. The integration of ZKPs, coupled with the Discv5 messaging protocol, enables a robust framework for stateless client operation within the Ethereum ecosystem.</p>\n<ul>\n<li>\n<p>P2P Network of Light Clients Using ZKPs: Light clients on this network will operate by receiving ZKPs of the last finalized block’s header. This header includes a root hash that represents the state changes made by transactions within the block. The clients will use these proofs to verify the integrity and correctness of the state transitions, ensuring they are consistent with the Ethereum protocol rules.</p>\n</li>\n<li>\n<p>Portal Subnetwork for State Verification: To facilitate consensus and state verification in a stateless manner, the plan includes the creation of a Portal subnetwork dedicated to comparing ZKPs of the last known state with the current state. This comparison verifies that state transitions are legitimate and that the block being proposed for inclusion in the blockchain is valid. This subnetwork will play a crucial role in maintaining the security and integrity of the blockchain while minimizing the computational and storage overhead for participants.</p>\n</li>\n</ul>\n<p>For more details on the Portal Network and its potential, refer to the Ethereum Portal Network documentation and resources:</p>\n<ul>\n<li><a href=\"https://ethereum.github.io/trin/introduction/index.html\" rel=\"noopener nofollow ugc\">Ethereum Portal Network Introduction</a></li>\n<li><a href=\"https://ethereum.org/de/developers/docs/networking-layer/portal-network/\" rel=\"noopener nofollow ugc\">Ethereum Developers Portal Network</a></li>\n<li><a href=\"https://github.com/sogolmalek/EIP-x\" rel=\"noopener nofollow ugc\">EIP-x github</a></li>\n<li><a href=\"https://dankradfeist.de/ethereum/2022/03/24/run-the-majority-client-at-your-own-peril.html\" rel=\"noopener nofollow ugc\">Ethereum Merge: Run the majority client at your own peril!-Dankrad Feist</a></li>\n</ul>\n            <p><small>2 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/towards-scalable-ethereum-staking-the-imperative-of-stateless-clients-with-compact-proof-sizes/19105\">Read full topic</a></p>","link":"https://ethresear.ch/t/towards-scalable-ethereum-staking-the-imperative-of-stateless-clients-with-compact-proof-sizes/19105","pubDate":"Mon, 25 Mar 2024 08:44:16 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19105"},"source":{"@url":"https://ethresear.ch/t/towards-scalable-ethereum-staking-the-imperative-of-stateless-clients-with-compact-proof-sizes/19105.rss","#text":"Towards Scalable Ethereum Staking: The Imperative of Stateless Clients with Compact Proof Sizes"},"filter":false},{"title":"How to purchase Ethereum GAS in advance","dc:creator":"web3skeptic","category":"Applications","description":"<h2><a name=\"abstract-1\" class=\"anchor\" href=\"https://ethresear.ch#abstract-1\"></a>Abstract</h2>\n<p>This topic introduces a method for purchasing gas on Ethereum. Technically it is a fully on-chain Ethereum gas futures market. The volatility of gas prices, primarily driven by network demand fluctuations, significantly obstructs user experience on Ethereum due to unpredictable costs. This solution aims to mitigate that issue.</p>\n<h2><a name=\"protocol-description-2\" class=\"anchor\" href=\"https://ethresear.ch#protocol-description-2\"></a>Protocol Description</h2>\n<h3><a name=\"general-3\" class=\"anchor\" href=\"https://ethresear.ch#general-3\"></a>General</h3>\n<p>The protocol users might be divided into to parties, gas purchasers (Purchaser) and gas providers (Executors)</p>\n<p>The general protocol workflow has these stages:</p>\n<ol>\n<li>Purchaser: Listing order</li>\n<li>Executor: Accepting order</li>\n<li>Purchaser: Requesting transaction execution</li>\n<li>Executor: Executing transaction</li>\n<li>Anyone: Liquidating executor</li>\n</ol>\n<h4><a name=\"listing-order-4\" class=\"anchor\" href=\"https://ethresear.ch#listing-order-4\"></a>Listing order</h4>\n<p>It is required to place order conditions onchain regarding the execution timeframe, gas price, and the expected security from the Executor, also the Purchaser locks the reward tokens responsible of paying the gas: <code>gasCost * gasAmount</code>.</p>\n<p>The GasOrder should include such fields:</p>\n<pre data-code-wrap=\"sol\"><code class=\"lang-plaintext\">struct GasOrder {\n    uint256 gasAmount; // The amount of Gas to book for future executions\n    uint256 executionPeriodStart; // Start timestamp when it is possible to use the gas within the order\n    uint256 executionPeriodDeadline; // End timestamp when it is possible to use the gas within the order\n    uint256 executionWindow; // This variable defines a window, measured in blocks, within which a\n    // transaction must be executed. This constraint is designed to optimize timing and prevent delays, while \n    // also safeguarding against the exploitation of gas price fluctuations by malicious actors.\n    TokenTransfer gasCost; // The cost of one Gas unit\n    TokenTransfer guarantee; // The guarantee security required from the Executor\n}\n</code></pre>\n<h4><a name=\"accepting-order-5\" class=\"anchor\" href=\"https://ethresear.ch#accepting-order-5\"></a>Accepting order</h4>\n<p>Some Executor accept the conditions of the GasOrder, and locks the guarantee. The guarantee security is execpected to be proportional to the purchased amount of Gas.</p>\n<h4><a name=\"requesting-transaction-execution-6\" class=\"anchor\" href=\"https://ethresear.ch#requesting-transaction-execution-6\"></a>Requesting transaction execution</h4>\n<p>When the GasOrder execution timeframe comes, the user might request transaction execution by signing the data structure with the transaction details.</p>\n<pre data-code-wrap=\"solidity\"><code class=\"lang-plaintext\">Message {\n  address from;\n  uint256 nonce;\n  uint256 gasOrder; // number of the employed order\n  uint256 deadline; // deadline of the msg execution, should be within the range of order execution\n  address to; // the contract which is being called\n  uint256 gas; // gas limit to spend\n  uint256 tips; // tips to the party which pushes the tx request onchain\n  bytes data; // execution request details\n  bytes signature; // the signature by the sender\n}\n</code></pre>\n<p>After the transaction is signed the hash of it should be published onchain. It might be done by transaction requester itself, or by anyone else. To incentives the posting the transaction request onchain the signer specifies the <code>tips</code>. The tips represents the Gas within the GasOrder which will be burned and the respective share of reward will be directed to the transaction request submitter.</p>\n<p>Executor takes the signed data and calls the <code>to</code> contract from function within the protocol which executes the call with the <code>data</code> from the <code>Message</code>. Consecuently the call unlocks the share of the reward and the guarantee for the Executor.</p>\n<p>If the transaction is not Executed because it is not profitable for the Executor than the Executor might be liquidated, it might be implemented in few ways, centralized and decentralized, lets review the decentralized version.</p>\n<h4><a name=\"decentralized-liquidation-logic-7\" class=\"anchor\" href=\"https://ethresear.ch#decentralized-liquidation-logic-7\"></a>Decentralized liquidation logic</h4>\n<p>During the transaction request, the transaction hash is posted on-chain, also the signature and remaining <code>Message</code> data posted as a calldata to be publicly available.</p>\n<p>If the Executor fails to execute the transaction before the <code>Message.deadline</code>, anyone can do so by providing the necessary data from the <code>Message</code> before the <code>deadline + CONSTANT_LIQUIDATION_TIME</code>. In return, the executor’s guarantee is partially forfeited, and the Liquidator requester receives a reward.</p>\n<h3><a name=\"bottlenecks-8\" class=\"anchor\" href=\"https://ethresear.ch#bottlenecks-8\"></a>Bottlenecks</h3>\n<h4><a name=\"executor-incentive-9\" class=\"anchor\" href=\"https://ethresear.ch#executor-incentive-9\"></a>Executor incentive</h4>\n<p>Executor incentives rely on adjusting GasCost to accommodate risk. As gas prices are unpredictable, Executors mitigate risks by charging extra. This flexible pricing model, determined by Executors, may evolve from sporadic agreements to a more standardized market, resulting in better price averages over time.</p>\n<h4><a name=\"gas-consumption-10\" class=\"anchor\" href=\"https://ethresear.ch#gas-consumption-10\"></a>Gas consumption</h4>\n<p>The protocol’s viability hinges on surpassing a certain threshold, as it necessitates gas for order publication, acceptance, signature verification, and transaction execution.</p>\n<h4><a name=\"split-of-liquidity-11\" class=\"anchor\" href=\"https://ethresear.ch#split-of-liquidity-11\"></a>Split of liquidity</h4>\n<p>Tokenizing each Gas order is straightforward, yet trading shares between orders poses a challenge due to their differing parameters. While securitization of long-term orders seems a plausible solution, preventing liquidity fragmentation remains elusive at present.</p>\n<h4><a name=\"lock-of-guarantee-12\" class=\"anchor\" href=\"https://ethresear.ch#lock-of-guarantee-12\"></a>Lock of guarantee</h4>\n<p>Executors must lock guarantees to deter liquidation risks, yet this restricts their flexibility. The locked guarantee could otherwise be utilized for liquidity elsewhere to generate yield. One potential solution is to lock tokens which represent shares in farming pools, enabling yield generation while locked. However, this introduces additional risks for gas Purchasers parties, as farming protocols entail additional security assumptions and associated risks.</p>\n<p>P.S. I’m really interested to get the feedback on the proposed mechanism</p>\n            <p><small>5 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/how-to-purchase-ethereum-gas-in-advance/19069\">Read full topic</a></p>","link":"https://ethresear.ch/t/how-to-purchase-ethereum-gas-in-advance/19069","pubDate":"Thu, 21 Mar 2024 17:34:01 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19069"},"source":{"@url":"https://ethresear.ch/t/how-to-purchase-ethereum-gas-in-advance/19069.rss","#text":"How to purchase Ethereum GAS in advance"},"filter":false},{"title":"Addressing systemic risks – discouragement attacks against centralized validator sets","dc:creator":"themandalore","category":"Uncategorized","description":"<p>This post describes a new discouragement attack which can reduce attestation and sync committee rewards of a targeted actor. This attack can theoretically be used to reduce a concentrated actor’s attestation and sync committee participation and reduces their rewards almost directly proportional to the number of validators participating in the attack.</p>\n<p>Of the options looked at, this attack was the one with the highest likelihood to achieve a finalized chain state with the following properties:</p>\n<ul>\n<li>reduced concentrated actor(CA) validator share</li>\n<li>reduced CA rewards</li>\n<li>minimal/nonexistent penalty to participating validators</li>\n<li>still Ethereum (aka not a minority fork)</li>\n</ul>\n<p><em>back to the start - identifying validators</em></p>\n<p>The first step to targeting a validator(s) is figuring out how to identify them. Ethereum PoS validators are not identified by address but by validator index. So rather than target validators set by public key, we must map their address to a valid Index. Since every validator is assigned a validator index upon depositing, we can create a map of validator indices by looking at deposit events in the beacon contract. [1]</p>\n<p>Note that for some CA’s, finding their addresses may be difficult (e.g. a list of validators run by a CEX). Fortunately though, LST’s and restaking protocols are much more transparent in their inner workings, so the indices are relatively easy to gather. Also note that LST’s could simply upgrade to obfuscate this or hide; but this isn’t actually too bad. An LST or CEX that has the ability to hide from the social layer is also hiding from would-be regulators or attackers looking to bribe/influence the party.</p>\n<p>A simple example script for finding Lido validator indexes (as of Feb 2024) <a href=\"https://github.com/danflo27/LidoValidators\" rel=\"noopener nofollow ugc\">can be found here</a>.</p>\n<p><em>midwest discouragement attacks</em></p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/2/27b832436de0e0d44604db4d9d19c7a3feaf217d.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/27b832436de0e0d44604db4d9d19c7a3feaf217d\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/2/27b832436de0e0d44604db4d9d19c7a3feaf217d_2_225x449.jpeg\" alt=\"\" data-base62-sha1=\"5Fng8ailCHGwGOD3NvCdLBW80TH\" width=\"225\" height=\"449\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/2/27b832436de0e0d44604db4d9d19c7a3feaf217d_2_225x449.jpeg, https://ethresear.ch/uploads/default/optimized/2X/2/27b832436de0e0d44604db4d9d19c7a3feaf217d_2_337x673.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/2/27b832436de0e0d44604db4d9d19c7a3feaf217d_2_450x898.jpeg 2x\" data-dominant-color=\"F3F3F4\"></a></div><p></p>\n<blockquote>\n<p>“Every slot a new committee becomes active and is expected to provide attestations. 440k validators / 64 committees = ~17k validators/committee. 17k validators poses a problem; it’s both too much network chatter and too many signatures to aggregate all at once. Fortunately, we’ve already split committees into 64 subnets. Each subnet consists of ~250 validators, of which 16 are designated as aggregators. As validators review blocks, they broadcast their attestations to their subnet. All 16 aggregators are attempting to build the same aggregate signatures, but network conditions often make perfection possible.”[2]</p>\n</blockquote>\n<p>Validators send attestations to committee subnet aggregators who then create an aggregated attestation that will be included in each block. To explain further, validators are partitioned into committees in each epoch, with one committee per slot. In each slot, one validator from the designated committee proposes a block. Then, all the members of that committee will attest to the newly proposed block and its position in the chain.[3] The goal of our modification is to make it so that when our validators are the subnet committee aggregators, they ignore CA attestations. Couple this with a selection rule that says when we are proposer, we only choose the aggregation that does not include CA validator attestations.</p>\n<p><em>more background</em></p>\n<p>There is a substantial overhead associated with passing attestation data around the network for every validator. Therefore, the attestations from individual validators are aggregated within subnets before being broadcast more widely.[4]</p>\n<p>During epochs where we have participating validators as the proposer AND aggregator, we can exclude a portion of attestations from CA validators in our blacklist set. Participating proposer clients will be modified such that rather than selecting the aggregation with the most attestations, they will choose the one with the most attestations and no CA attestations (a list of validator indices created from our participating aggregator). The probability for a given committee of having at least one of the aggregators and the proposer is as follows:</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/8/87907f11b34acec350a6b0c980871d1736f682d3.png\" data-download-href=\"https://ethresear.ch/uploads/default/87907f11b34acec350a6b0c980871d1736f682d3\" title=\"Chart\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/8/87907f11b34acec350a6b0c980871d1736f682d3_2_568x351.png\" alt=\"\" title=\"Chart\" data-base62-sha1=\"jlg2vQnAEgIzc0zumb8P8SKKOk3\" width=\"568\" height=\"351\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/8/87907f11b34acec350a6b0c980871d1736f682d3_2_568x351.png, https://ethresear.ch/uploads/default/optimized/2X/8/87907f11b34acec350a6b0c980871d1736f682d3_2_852x526.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/8/87907f11b34acec350a6b0c980871d1736f682d3_2_1136x702.png 2x\" data-dominant-color=\"FDFDFE\"></a></div><p></p>\n<p>A nice feature of this discouragement attack is that it scales almost identically with validator participation. What this means is that if you get 20% of the validators participating, you should be able to reduce CA attestation rewards by about 20%.[5]</p>\n<p>The downside for proposers here is that they would lose out on some attestation rewards as ⅛ of an attestation reward goes to the proposer. There is an argument to be made that participating validators will be open to losing this small amount as the CA (competing validators) will lose 7x the amount they amount will, thus making themselves more competitive and protecting the chain from centralization.</p>\n<p>The case where we have only the proposer (and no aggregator in a given committee) was also looked at. A participating validator could drop all aggregations with CA signatures (so have no attestations for a given committee), but then you would punish non-CA validators as well as CA validators if they were in the same subcommittee. Not to mention, the participating validator would lose more rewards, rewards which would not be offset necessarily by a decreased CA reward. For these reasons, this action is something that we will avoid, however we will need to look further into how to best include/exclude if target CA lists do match up perfectly (e.g. if you drop one honest validator is it ok if you’re still dropping 20 CA indices?).</p>\n<p><em>aftermath</em></p>\n<p><strong>consequences to the CA</strong>: Missing an attestation means missing rewards, about 84.4% of base rewards to be specific (see <a href=\"https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/attestations/#rewards\" rel=\"noopener nofollow ugc\">here</a> for the exact rewards structure). Depending on block space, they could try to get attached to subsequent blocks for partial rewards, but from my understanding, it rarely happens and they would likely lose out on most attestation rewards as laid out in the analysis.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/2/24b123b146940a78f99d6b9428d93a92ce2b253b.png\" data-download-href=\"https://ethresear.ch/uploads/default/24b123b146940a78f99d6b9428d93a92ce2b253b\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/2/24b123b146940a78f99d6b9428d93a92ce2b253b_2_565x481.png\" alt=\"\" data-base62-sha1=\"5eAHV5QVarqiXBHknWWDf7ygX1h\" width=\"565\" height=\"481\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/2/24b123b146940a78f99d6b9428d93a92ce2b253b_2_565x481.png, https://ethresear.ch/uploads/default/optimized/2X/2/24b123b146940a78f99d6b9428d93a92ce2b253b_2_847x721.png 1.5x, https://ethresear.ch/uploads/default/original/2X/2/24b123b146940a78f99d6b9428d93a92ce2b253b.png 2x\" data-dominant-color=\"F9F9F9\"></a></div><p></p>\n<p>Overall, any significant percentage of validators participating could be devastating to the CA with regard to staking competitively.</p>\n<p><strong>consequences to participating validators</strong>: Committee aggregators and proposers are incentivized to include as many valid attestations as they can see in their Aggregated Attestation. For every attestation the aggregator excludes, the reward decreases (see <a href=\"https://eth2book.info/capella/part2/incentives/rewards/#rewards-scale-with-participation\" rel=\"noopener nofollow ugc\">Rewards scale with participation</a>). In general, the rule for ETH2.0 is that “7/8 of rewards go to validators performing duties and 1/8 to the proposers including the evidence in blocks.” The aggregator and proposer both therefore would lose out like all ETH validators in the sense that total attestation rewards scale with participation as well, but there would be no direct punishment that would make the aggregator less competitive.[6,7]</p>\n<p><strong>consequences to the protocol</strong>: Of all discouragement attack options analyzed, excluding a limited number of attestations from certain parties seems to be the option with the lightest footprint. More research should be done on how this could affect fork-choice-rule, however theoretically it shouldn’t affect it (all else equal). One consideration to note is relative to the size of the CA. If the CA is &gt;33% of the validator set, you cannot ignore all their attestations, as it would affect block finality. Therefore you would need to limit the number of attestations ignored to only those greater than some threshold for finality.</p>\n<p><em>dousing the beacon - ignore sync committee messages</em></p>\n<p>In addition to attestations, we can reduce up to another 3.1% of CA rewards by doing a similar aggregation censoring in the sync committee. To briefly explain, Validators are rewarded for correctly participating in sync committee signatures (each day, 512 are chosen, so very rare), which are of use to light clients. This option may even be a preferable first step versus the attestation censorship, as the sync committee is not part of internal fork choice or even required for the main protocol at all.</p>\n<p>The same “rewards scale with participation” concept is present here, however the big difference is that we have no issues with finality, forks, or even chain issues at all, as the sync committee is for use in other protocols and not within Ethereum. That said, several light client bridge implementations take advantage of the sync committee to finalize transactions and would be affected if a large portion of the sync committee is owned by the blacklisted CA. Overall though, it seems a relatively light touch on the system, but the big downside is that the rewards here are relatively small and may not be enough to deter a CA. It might be useful to try out as a first step in testing support/ implementation issues.</p>\n<p><em>the journey is just beginning</em></p>\n<p>Other options should still be on table for addressing systemic risks, however this should be a great starting place for continuing discussions with stakeholders and developers in the Ethereum ecosystem as to the best path forward. I’m optimistic that just talking publicly about these options can be enough to deter a CA from growing any larger or maintaining threatening levels of control.</p>\n<p><strong>references</strong><br>\n[1] <a href=\"https://github.com/ethereum/annotated-spec/blob/master/phase0/beacon-chain.md\" rel=\"noopener nofollow ugc\">https://github.com/ethereum/annotated-spec/blob/master/phase0/beacon-chain.md</a><br>\n[2] <a href=\"https://inevitableeth.com/home/ethereum/network/consensus/pos\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Proof of Stake | Inevitable Ethereum</a><br>\n[3] <a href=\"https://arxiv.org/pdf/2003.03052.pdf\" rel=\"noopener nofollow ugc\">https://arxiv.org/pdf/2003.03052.pdf</a><br>\n[4] <a href=\"https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/attestations/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Attestations | ethereum.org</a><br>\n[5] <a href=\"https://docs.google.com/spreadsheets/d/1H3Mux2noz2NfKC3A63VX3ZBH4q_KP0MMk3OQM4Ysxpc/edit#gid=1960537153\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Aggregator selection - Google Sheets</a><br>\n[6]<a href=\"https://eth2book.info/capella/part2/incentives/rewards/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Upgrading Ethereum | 2.8.4 Rewards</a><br>\n[7] <a href=\"https://eth2book.info/capella/part3/transition/block/#def_process_attestation\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Upgrading Ethereum | 3.5.3 Block processing</a></p>\n<p><strong>further reading</strong></p>\n<p>I’ve done a few other articles on social values / systemic risks too. The first article<a href=\"https://medium.com/@nfett/why-values-matter-social-flexing-in-crypto-f971b23167e2\" rel=\"noopener nofollow ugc\"> is here</a>, the second is <a href=\"https://medium.com/@nfett/social-flex-2-dangers-d4701f525d28\" rel=\"noopener nofollow ugc\">here</a>, and the third is <a href=\"https://medium.com/@nfett/social-flex-3-options-3788c3dd4b7a\" rel=\"noopener nofollow ugc\">here</a>.</p>\n<ul>\n<li>Vitalik’s Discouragement attacks: <a href=\"https://github.com/ethereum/research/blob/master/papers/discouragement/discouragement.pdf\" rel=\"noopener nofollow ugc\">https://github.com/ethereum/research/blob/master/papers/discouragement/discouragement.pdf</a></li>\n<li>Aggregation process:\n<ul>\n<li><a href=\"https://github.com/ethereum/consensus-specs/blob/dev/specs/phase0/validator.md#construct-aggregate\" rel=\"noopener nofollow ugc\">https://github.com/ethereum/consensus-specs/blob/dev/specs/phase0/validator.md#construct-aggregate</a></li>\n<li><a href=\"https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/attestations/#aggregated-attestation\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Attestations | ethereum.org</a></li>\n<li><a href=\"https://notes.ethereum.org/@hww/aggregation\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">A note on Ethereum 2.0 attestation aggregation strategies - HackMD</a></li>\n</ul>\n</li>\n<li>Sync protocol spec: <a href=\"https://github.com/ethereum/consensus-specs/blob/dev/specs/altair/light-client/sync-protocol.md\" rel=\"noopener nofollow ugc\">https://github.com/ethereum/consensus-specs/blob/dev/specs/altair/light-client/sync-protocol.md</a></li>\n<li>Rewards: <a href=\"https://eth2book.info/capella/part2/incentives/rewards/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Upgrading Ethereum | 2.8.4 Rewards</a></li>\n<li>Gasper paper: <a href=\"https://arxiv.org/pdf/2003.03052.pdf\" rel=\"noopener nofollow ugc\">https://arxiv.org/pdf/2003.03052.pdf</a></li>\n<li>Understanding the <a href=\"https://notes.ethereum.org/@vbuterin/serenity_design_rationale?type=view#LMD-GHOST-fork-choice-rule\" rel=\"noopener nofollow ugc\">fork choice rule</a></li>\n<li>Inactivity Leak: <a href=\"https://notes.ethereum.org/@vbuterin/serenity_design_rationale?type=view#Inactivity-leak\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Serenity Design Rationale - HackMD</a></li>\n<li>Ethereum PoS Attack and Defense: <a href=\"https://mirror.xyz/jmcook.eth/YqHargbVWVNRQqQpVpzrqEQ8IqwNUJDIpwRP7SS5FXs\" rel=\"noopener nofollow ugc\">https://mirror.xyz/jmcook.eth/YqHargbVWVNRQqQpVpzrqEQ8IqwNUJDIpwRP7SS5FXs</a></li>\n<li>Shout to Banteg for ideas: <a href=\"https://twitter.com/bantg/status/1561177300741283842\" rel=\"noopener nofollow ugc\">https://twitter.com/bantg/status/1561177300741283842</a></li>\n<li>Proof-of-Stake overview: <a href=\"https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Proof-of-stake (PoS) | ethereum.org</a></li>\n<li>Vitalik on Lex Friedman on the Social Layer: <a href=\"https://www.youtube.com/watch?v=3yrqBG-7EVE\" rel=\"noopener nofollow ugc\">https://www.youtube.com/watch?v=3yrqBG-7EVE</a></li>\n<li>Can nodes go against the protocol: <a href=\"https://hackmd.io/@prysmaticlabs/finality#Can-nodes-go-against-the-protocol\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">What Happens After Finality in ETH2? - HackMD</a></li>\n<li>Consensus and Execution client connections: <a href=\"https://ethereum.org/en/developers/docs/networking-layer/#connecting-clients\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Networking layer | ethereum.org</a></li>\n<li>Mitigating attacks in PoS <a href=\"https://ethresear.ch/t/change-fork-choice-rule-to-mitigate-balancing-and-reorging-attacks/11127\" class=\"inline-onebox\">Change fork choice rule to mitigate balancing and reorging attacks</a></li>\n<li>Cool Epoch and Slot Visualization: <a href=\"https://beaconcha.in/charts/slotviz\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Charts - Open Source Ethereum Blockchain Explorer - beaconcha.in - 2024</a></li>\n<li>Censorship in the PBS Stack: <a href=\"https://www.youtube.com/watch?v=WcJlseuhbX8\" rel=\"noopener nofollow ugc\">https://www.youtube.com/watch?v=WcJlseuhbX8</a></li>\n</ul>\n            <p><small>9 posts - 3 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/addressing-systemic-risks-discouragement-attacks-against-centralized-validator-sets/19067\">Read full topic</a></p>","link":"https://ethresear.ch/t/addressing-systemic-risks-discouragement-attacks-against-centralized-validator-sets/19067","pubDate":"Thu, 21 Mar 2024 13:41:02 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19067"},"source":{"@url":"https://ethresear.ch/t/addressing-systemic-risks-discouragement-attacks-against-centralized-validator-sets/19067.rss","#text":"Addressing systemic risks – discouragement attacks against centralized validator sets"},"filter":true},{"title":"Public Projects with Preferences and Predictions","dc:creator":"bowaggoner","category":"Economics","description":"<p>A few years ago, I posted <a href=\"https://ethresear.ch/t/governance-mixing-auctions-and-futarchy/10772\">Governance mixing auctions and futarchy</a> here. Also relevant are <a href=\"https://ethresear.ch/t/practical-futarchy-setup/10339\">Practical Futarchy Setup</a> and <a href=\"https://ethresear.ch/t/votes-as-buy-orders-a-new-type-of-hybrid-coin-voting-futarchy/10305\">Votes as Buy Orders</a>. My post/question eventually led to research funding from the Ethereum Foundation and now my collaborator and I have a paper proposing a new governance mechanism: <a href=\"https://arxiv.org/abs/2403.01042\" rel=\"noopener nofollow ugc\"><strong>Public Projects with Preferences and Predictions</strong></a>. I hope you find it interesting and I would love to hear your feedback and thoughts! Here is a summary.</p>\n<hr>\n<p><strong>Problem:</strong> A group, such as members of a DAO, need to decide between one of several alternatives, such as which project to pursue (they must pick exactly one). I want to focus on two aspects of this problem:</p>\n<ul>\n<li>They want to base the decision on an aggregation of both <strong>preferences</strong> and <strong>information</strong>. For example, one way is to first hold discussions and conduct research in order to aggregate information. Then, if consensus is not reached, hold a vote to aggregate preferences into a final decision.</li>\n<li>A primary problem of any organization is to <strong>avoid capture</strong>. In particular, the individual preferences of the members are generally not perfectly aligned with the mission of the organization. The above “discuss-then-vote” approach doesn’t solve this. A decisionmaking mechanism should somehow be biased by a <em>credible</em> estimate of the impact on the organization’s mission.</li>\n</ul>\n<hr>\n<p><strong>Formalizing it:</strong> A group of agents must pick one of <span class=\"math\">m</span> alternatives. The group’s mission is quantified by what we call an “external welfare impact” of the decision. The goal is to maximize total welfare: the sum of the external welfare impact, plus the utilities of all the agents in the group. The group first wants to aggregate information, in particular about the external welfare impacts of each alternative choice it could make. We model information as predictions about the future, i.e. the group first wants to estimate the external welfare impact <span class=\"math\">B_k</span> of each alternative <span class=\"math\">k=1,\\dots,m</span>. After obtaining the estimates, the group will hold a vote. We suppose that each agent <span class=\"math\">i</span> has a preference over the alternatives, modeled as a value <span class=\"math\">v_k^i</span> for each alternative <span class=\"math\">k</span>. The group will use some sort of voting mechanism to combine the preferences as well as the external welfare impacts into a final decision.</p>\n<p>For example, consider a DAO whose charter requires consideration of climate impacts of its decisions. The external welfare impact <span class=\"math\">B_k</span> of an alternative <span class=\"math\">k</span> could be measured by the amount of extra tons of CO2 produced if that alternative is chosen. The DAO may still choose an alternative that produces more CO2 if the members as a whole strongly prefer that alternative.</p>\n<hr>\n<p><strong>Proposal:</strong> We propose it the <strong>Synthetic players QUAdratic transfers mechanism with Predictions (SQUAP)</strong>, and it works like this:</p>\n<ol>\n<li>We use an information-aggregation oracle to obtain an estimate of the external welfare impact <span class=\"math\">B_i</span> of each alternative. The oracle can be implemented in many possible ways, but in particular we consider using “decision markets” (i.e. sets of conditional prediction markets, as in futarchy).</li>\n<li>We use Quadratic Voting, specifically the Quadratic Transfers Mechanism studied by <a href=\"https://arxiv.org/abs/2301.06206\" rel=\"noopener nofollow ugc\">Eguia et al.</a> However, the mechanism casts “extra” votes based on <span class=\"math\">B_1,\\dots,B_m</span>.</li>\n</ol>\n<p>The extra votes are not simply the numbers <span class=\"math\">B_1,\\dots,B_m</span> for each alternative. Instead, they are the votes that “synthetic players” would cast in equilibrium of Quadratic Voting, if their total values for each alternative were <span class=\"math\">B_1,\\dots,B_m</span>. This is based on an analysis of the equilibrium, extending results of Eguia et al.</p>\n<hr>\n<p><strong>About the prediction markets:</strong> We need to suppose that the external welfare impact can be predicted before the fact and measured afterward. You can think of lots of ways to use proxies and estimates for quantities that are hard to measure or have long time horizons. For simplicity, in the paper we assume that if we take any decision <span class=\"math\">k</span>, then <span class=\"math\">B_k</span> will be directly observable and measurable. So we will set up <span class=\"math\">m</span> prediction markets, predicting for each alternative <span class=\"math\">k</span> the eventual impact <span class=\"math\">B_k</span> conditioned on making decision <span class=\"math\">k</span>. When we eventually make some decision <span class=\"math\">k</span>, we cancel all the trades in every market except the market for <span class=\"math\">k</span>. But we do eventually observe <span class=\"math\">B_k</span> for that chosen alternative and we resolve the market payoffs accordingly.</p>\n<p>The Quadratic Transfer Mechanism picks an alternative from a probability distribution, where the probability of taking decision <span class=\"math\">k</span> is <span class=\"math\">e^{A_k} / \\sum_{\\ell} e^{A_{\\ell}}</span> and <span class=\"math\">A_k</span> is total the number of votes cast for decision <span class=\"math\">k</span>. There is a nonzero probability of picking each alternative. That may sound odd, but it actually helps address a well-known problem with futarchy and decision markets – bad incentives around predictions about decisions that won’t actually be taken.</p>\n<hr>\n<p><strong>Formal results:</strong> Unfortunately, we can’t quite prove (yet) good results about SQUAP itself. However, we can analyze an “Impractical Variant” that is not practical because it requires some extra knowledge by the mechanism designer. Essentially, instead of calculating our synthetic votes based on what the real players vote for, we have to first commit to our synthetic votes based just on knowing <span class=\"math\">B_1,\\dots,B_m</span>, before we see what the players do. This is impractical to calculate without good estimates of what the players will do, but it’s not totally ridiculous. In any case, we can prove something:</p>\n<blockquote>\n<p><strong>Main Theorem.</strong> In the 2-alternative case, in Nash equilibrium, the Impractical Variant of SQUAP satisfies Social Welfare <span class=\"math\">\\to</span> Optimal as the “size” of the group grows large relative to the preferences of any one individual.</p>\n</blockquote>\n<p>The interesting and non-obvious part of all this is that the mechanism “works” in theory even though there are apparently bad incentives across the two stages. Someone with really strong preferences about the decision could try to manipulate the prediction market prices in order to manipulate the synthetic votes. And someone who bet a lot of money in the prediction market could try to cast really outsized votes in the Quadratic Voting stage in order to make their prediction-market payoffs come good. So the interesting part is in proving that these things don’t happen, or rather, they can happen a little but not enough to influence the outcome significantly … as long as each individual’s preferences are small compared to the group total.</p>\n<p>Our theorem quantifies the rate at which the social welfare approaches the optimal, and it’s reasonably fast as the size of the group grows large. But this post is too long already. I’ll just clarify that Social Welfare is formalized by the sum of the values of the participants in the group, plus the external welfare impact, of whatever the mechanism chooses.</p>\n            <p><small>2 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/public-projects-with-preferences-and-predictions/19024\">Read full topic</a></p>","link":"https://ethresear.ch/t/public-projects-with-preferences-and-predictions/19024","pubDate":"Sun, 17 Mar 2024 05:55:17 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19024"},"source":{"@url":"https://ethresear.ch/t/public-projects-with-preferences-and-predictions/19024.rss","#text":"Public Projects with Preferences and Predictions"},"filter":false},{"title":"Fractal Scaling on Ethereum (Layer 3 Rollups)","dc:creator":"purpleshadow2000","category":"ZK Rollup","description":"<h2><a name=\"abstract-1\" class=\"anchor\" href=\"https://ethresear.ch#abstract-1\"></a><strong>ABSTRACT</strong></h2>\n<p>In this article, we are going to review how current solutions in the public and private blockchain ecosystem look like when it comes to scalability and real-world utility. We are going to discuss the possible and effective transition of blockchain becoming the value layer of the new Internet. In order to achieve the goal which Ethereum community and blockchain technology started with “Becoming a decentralized, scalable and programmable layer of the Internet”, we are going to discuss the problems which needs to be solved and propose some of the probable solutions which might help the community as a whole achieve its goal. of becoming the data and financial layer of the world.</p>\n<h1><a name=\"the-beginnings-2\" class=\"anchor\" href=\"https://ethresear.ch#the-beginnings-2\"></a>The Beginnings</h1>\n<p>It all started with Bitcoin, a decentralized peer to peer transfer of value (tokens) over a distributed ledger technology (DLT). Although it was revolutionary at the time, it missed some of the functionalities which could be implemented to put the underlying DLT technology to better use as a wider scope for the world. In order to bring programmability to the DLT termed Blockchain technology, many new research and projects came by. Among all, Ethereum came out to be “THE” programmability layer of the new decentralized Internet that brought the best of blockchain technology in form of immutable “smart contracts”.</p>\n<p>But with time and increasing adoption by the day, the community realized it lacked many of the important factors to be considered for adoption by the masses. Over the years, most of the problems have been solved with major upgrades in the Ethereum beacon chain. However, with regards to this thesis, we would be focusing on solving three major problems:</p>\n<ul>\n<li>Hyper-Scalability</li>\n<li>Privacy</li>\n<li>Utility and Composability</li>\n</ul>\n<h2><a name=\"introduction-3\" class=\"anchor\" href=\"https://ethresear.ch#introduction-3\"></a>Introduction</h2>\n<p>With limited block size of Ethereum beacon chain, it limits the TPS i.e., number of transactions to be processed on the chain. This restricts Ethereum beacon chain from processing millions of transactions while maintaining its security. Therefore, Ethereum’s Rollup centric roadmap promotes creation of various types of rollups or supporting chains which execute and process their own transactions on their customized blockchains and only pass on proofs of execution and settlement on Layer 1 chain. This creates a separate layer for developers to build their applications with outsourced security of another blockchain and only use Ethereum for finality and data availability of the transactions processed on the rollup.</p>\n<p><strong>Join Pioneer Labs’ <a href=\"https://discord.gg/xPtF8jWk\" rel=\"noopener nofollow ugc\">Discord Community</a></strong></p>\n<p><strong>Follow us on <a href=\"https://twitter.com/labs_pioneer\" rel=\"noopener nofollow ugc\">Twitter</a></strong></p>\n<h3><a name=\"types-of-scaling-solutions-4\" class=\"anchor\" href=\"https://ethresear.ch#types-of-scaling-solutions-4\"></a>Types of Scaling Solutions</h3>\n<p><strong>Side Chains:</strong> A sidechain is a separate blockchain that runs independent of Ethereum and is connected to Ethereum Mainnet by a two-way bridge. Sidechains can have separate block parameters and <a href=\"https://ethereum.org/en/developers/docs/consensus-mechanisms/\" rel=\"noopener nofollow ugc\">consensus algorithms</a>, which are often designed for efficient processing of transactions.</p>\n<p><strong>Validiums:</strong> Validium is a <a href=\"https://ethereum.org/en/developers/docs/scaling/\" rel=\"noopener nofollow ugc\">scaling solution</a> that enforces integrity of transactions using validity proofs like ZK-rollups, but doesn’t store transaction data on the Ethereum Mainnet.</p>\n<p><strong>Optimistic Rollups:</strong> Optimistic rollups are layer 2 (L2) protocols designed to extend the throughput of Ethereum’s base layer. They reduce computation on the main Ethereum chain by processing transactions off-chain, offering significant improvements in processing speeds.</p>\n<p><strong><a href=\"https://ethereum.org/en/developers/docs/scaling/zk-rollups/\" rel=\"noopener nofollow ugc\">ZK Rollups</a>:</strong> Zero-knowledge rollups (ZK-rollups) are <a href=\"https://ethereum.org/en/developers/docs/scaling/\" rel=\"noopener nofollow ugc\">layer 2 scaling solutions</a> that increase throughput on Ethereum Mainnet by moving computation and state-storage off-chain. ZK-rollups can process thousands of transactions in a batch and then only post some minimal summary data to Mainnet. This summary data defines the changes that should be made to the Ethereum state and some cryptographic proof that those changes are correct.</p>\n<h2><a name=\"problems-with-current-frameworks-5\" class=\"anchor\" href=\"https://ethresear.ch#problems-with-current-frameworks-5\"></a>Problems with Current Frameworks</h2>\n<h3><a name=\"scalability-trilemma-6\" class=\"anchor\" href=\"https://ethresear.ch#scalability-trilemma-6\"></a>Scalability Trilemma</h3>\n<p>The scalability trilemma is a concept in blockchain technology that states that it is impossible for a blockchain system to achieve all three of the following properties simultaneously:</p>\n<ul>\n<li>Security</li>\n<li>Decentralization</li>\n<li>Scalability</li>\n</ul>\n<p>In the case of Ethereum, this means that the current proof-of-work consensus the algorithm is secure and decentralized, but it cannot scale to meet the needs of a truly global financial system. This is because the more transactions the network processes, the more difficult it becomes to maintain decentralization, as the network requires more and more computational power to process all of the transactions in a timely manner.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/d/d92b30c0ae257bdeb4e5c003c373e99a75981cb9.png\" data-download-href=\"https://ethresear.ch/uploads/default/d92b30c0ae257bdeb4e5c003c373e99a75981cb9\" title=\"Scalability Trilemma\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/d/d92b30c0ae257bdeb4e5c003c373e99a75981cb9_2_614x500.png\" alt=\"Scalability Trilemma\" data-base62-sha1=\"uZa93UqX3nXvNjQlXjzOgAySqP7\" width=\"614\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/d/d92b30c0ae257bdeb4e5c003c373e99a75981cb9_2_614x500.png, https://ethresear.ch/uploads/default/original/2X/d/d92b30c0ae257bdeb4e5c003c373e99a75981cb9.png 1.5x, https://ethresear.ch/uploads/default/original/2X/d/d92b30c0ae257bdeb4e5c003c373e99a75981cb9.png 2x\" data-dominant-color=\"ECF1F7\"></a></div><p></p>\n<p>Scalability Trilemma</p>\n<p>In order to solve the scalability trilemma, Ethereum focused towards the Rollup Centric Roadmap, however, the overall motive to achieve scalability has been unsuccessful during events of network clogging. For example, in the meme-coin season of $PEPE, gas fee on Ethereum L1 rose up to $100 and Optimistic L2s up to $8 making it infeasible to use by enterprises and protocols with wider use cases and network requirements</p>\n<p>Therefore, in order to make the Ethereum network the utility chain for the Internet which includes business solutions, private enterprise solutions, public solutions, defi, etc. the network needs to achieve hyper-scalability while maintaining a certain level of trust and security of Ethereum Virtual Machine.</p>\n<h3><a name=\"fragmentation-vs-adoption-7\" class=\"anchor\" href=\"https://ethresear.ch#fragmentation-vs-adoption-7\"></a>Fragmentation vs Adoption</h3>\n<p>With many new types and forms of rollups being developed, the blockchain community is getting more and more fragmented, developers must learn new tech stack, understand new concepts and build new architectures to make their applications compatible with each new type of chains. This not only hinders the goal for simplicity and improvement of User experience but also complicates the developer experience on the new chains.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/6/65c5c10864b4b0c8c9cb1a6e68805f2f06bc340b.png\" data-download-href=\"https://ethresear.ch/uploads/default/65c5c10864b4b0c8c9cb1a6e68805f2f06bc340b\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/original/2X/6/65c5c10864b4b0c8c9cb1a6e68805f2f06bc340b.png\" alt=\"\" data-base62-sha1=\"ewjUsgG8MOyscOoj5sqtTHLcSrV\" width=\"690\" height=\"389\" role=\"presentation\" data-dominant-color=\"E1E1E1\"></a></div><p></p>\n<p>With each new rollup framework, the community keeps on getting fragmented with no single form of framework being adopted across the market. With the creation of each new rollup, the team must bootstrap the entire community and create real value for the developers and businesses to build on their blockchain. This creates additional costs for adoption not only by developers but by enterprise. This creates very limited utilities for enterprises to adopt public blockchains for building their internal or external solutions for their businesses.</p>\n<h3><a name=\"siloed-ecosystem-vs-interoperability-8\" class=\"anchor\" href=\"https://ethresear.ch#siloed-ecosystem-vs-interoperability-8\"></a>Siloed Ecosystem vs Interoperability</h3>\n<p>With sidechains, plasma chains, Layer 2 Rollups, ZK Rollups each having separate execution environments, settlement layers, sequencers, proof generations, etc. This creates the major issue of cross- chain trustless messaging creating security issues for users for transferring assets from one chain to another through third parties such as bridges. Each blockchain and rollup have their siloed ecosystem and technical architecture creating it very difficult for developers to adapt to each architecture and develop their application compatible with all architectures.</p>\n<p>If multiple Layer 3 chains as well as app-chains are developed using the similar EVM equivalent framework without having to transition to other technologies or going through multiple developer education programs, it will make the development and deployment of trustless solutions not only faster but cheaper. This would allow multiple publics, private as well as enterprise chains to share security as well as arbitrary message passing creating trustless interoperability among rollups.</p>\n<h2><a name=\"proposed-solution-9\" class=\"anchor\" href=\"https://ethresear.ch#proposed-solution-9\"></a>Proposed Solution</h2>\n<p>For our research purposes, we’ll only be focusing on ZK-rollups for now i.e., we’ll be exploring solutions particularly on zk-EVMs and avoiding other frameworks for mainly two reasons:</p>\n<ol>\n<li>L3s cannot be effectively deployed on Optimistic rollups due to the 7-day fraud proof period. For deploying a Layer 3 chain, the execution of the chains from middleware to Layer 2 VM will become very complicated in terms of generating proofs. Not only the solution becomes complicated but not many optimistic rollups support the framework for fractal scaling.</li>\n<li>We’ll not be focusing on using an EVM sidechain or an EVM-compatible Layer 1 Chain for our thesis due to the adoption and security dilemma. Due to pre-existing community adoption of Ethereum by hundreds of millions and Ethereum beacon chain secured by tens of billions of dollars, it becomes difficult for any network to get the level of security and finality as Ethereum beacon chain.</li>\n</ol>\n<p>Our solution will focus on building the tech stack allowing developers and enterprises to create their own Layer 3 scaling solution(rollups) on Type 2.5 and/or Type 3 zk-EVMs using Ethereum equivalent Virtual Machine with ZK-SNARKS (Zero-Knowledge Succinct Non-Interactive Argument of Knowledge).</p>\n<h3><a name=\"why-zk-snarks-10\" class=\"anchor\" href=\"https://ethresear.ch#why-zk-snarks-10\"></a>Why Zk-SNARKS?</h3>\n<p>The solution would be built with ZK-Rollups since the time involved with fraud proof generation and the 7 day challenge period would make proof generation and validation of L3 rollups very complicated and the security with transaction finality would be weak to order-flow hacks. Why we chose <a href=\"https://scroll.io/blog/architecture\" rel=\"noopener nofollow ugc\">ZK-SNARKS</a> over STARKS is due to:</p>\n<ul>\n<li><strong>Efficiency and Scalability:</strong> ZK-SNARKs provide more efficient proofs compared to ZK-STARKs. They generate smaller proof sizes, which reduces the computational and storage requirements for validating transactions and executing smart contracts. This efficiency is particularly crucial in the context of fractal scaling, where aggregating multiple transactions into a single proof is essential for achieving scalability.</li>\n<li><strong>Ecosystem Maturity:</strong> The ecosystem around <a href=\"https://zcash.github.io/halo2/concepts/chips.html\" rel=\"noopener nofollow ugc\">ZK-SNARKs</a> is more mature and has seen wider adoption compared to ZK-STARKs. This maturity translates into more available tools, libraries, and expertise for implementing and utilizing ZK- SNARKs. The larger developer community and existing infrastructure make it easier to integrate ZK-SNARKs into Layer 3 rollups on the Ethereum blockchain.</li>\n</ul>\n<h3><a name=\"why-type-3-zk-evms-should-be-preferred-for-layer-3s-11\" class=\"anchor\" href=\"https://ethresear.ch#why-type-3-zk-evms-should-be-preferred-for-layer-3s-11\"></a>Why Type 3 ZK-EVMs should be preferred for Layer 3s</h3>\n<p>Type 2.5 or <a href=\"https://vitalik.ca/general/2022/08/04/zkevm.html\" rel=\"noopener nofollow ugc\">Type 3 ZK-EVMs</a> are more preferred than Type 4 ZK-EVMs for fractal scaling solutions:</p>\n<ul>\n<li><strong>Efficiency and Proof Size:</strong> Type 3 ZK-EVMs typically offer more efficient proofs compared to Type 4 <a href=\"https://hackmd.io/@yezhang/S1_KMMbGt\" rel=\"noopener nofollow ugc\">ZK-EVMs</a>. Fractal scaling heavily relies on aggregating multiple transactions into a single proof, and smaller proof sizes are desirable for improved efficiency. Type 3 ZK-EVMs are generally designed with succinct proof generation in mind, optimizing the proof size and reducing computational and storage overhead.</li>\n<li><strong>Interoperability:</strong> Type 3 <a href=\"https://privacy-scaling-explorations.github.io/zkevm-docs/introduction.html\" rel=\"noopener nofollow ugc\">ZK-EVMs</a> typically maintain a high level of interoperability and byte-code compatibility with existing Ethereum infrastructure. They are designed to be compatible with Ethereum’s EVM (Ethereum Virtual Machine), enabling seamless integration with smart contracts and existing decentralized applications (dApps). This [interoperability](<a href=\"https://polymerlabs.medium.com/developing-the-most-truly-decentralized-interoperability-solution-\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Medium</a> polymer-zk-ibc-f0287ea84a2b) is crucial for frictionless adoption and leveraging the robust ecosystem of Ethereum. Type 3 ZK-EVMs have byte-code level compatibility with EVM making the transitioning and network shifting much easier for developers and applications unlike Type 4 ZK-EVMs like CairoVM or VyperVM which use high level language ZK-Proofs which cannot be computed by Type 1 ZK-EVM.</li>\n</ul>\n<h3><a name=\"why-layer-3s-12\" class=\"anchor\" href=\"https://ethresear.ch#why-layer-3s-12\"></a>Why Layer 3s?</h3>\n<p>This would be most effective on Layer solutions due to hyper scalability and middleware. With the existence of Layer 2 middleware between app-chains and Ethereum beacon chains. When it comes to scaling solutions, privacy and customized scaling, <a href=\"https://vitalik.ca/general/2022/09/17/layer_3.html\" rel=\"noopener nofollow ugc\">Layer 3 rollups</a> have significant benefits over sovereign app-chains and Layer 2 app specific rollups.</p>\n<ul>\n<li><strong>Interoperability and Network Effects:</strong> Layer 3 rollups are designed to be fully compatible with the Ethereum network. They inherit Ethereum’s smart contract functionality and benefit from the existing ecosystem of decentralized applications (dApps), wallets, and tools. This interoperability allows for seamless integration with existing Ethereum applications and maximizes network effects. Sovereign app-chains, being separate chains, may lack the same level of interoperability and may require building a new ecosystem from scratch, potentially hindering adoption.</li>\n<li><strong>Security and Finality:</strong> Layer 3 rollups provide strong security guarantees through the finality of transactions. Once a transaction is included in a Layer 3 rollup, it is considered final and cannot be reversed or tampered with. Sovereign app-chains may have different security models, and the finality of transactions may depend on their specific consensus mechanism. The security and finality guarantee of Layer 3 roll ups provide users and developers with a higher degree of confidence.</li>\n<li><strong>Recursive Composition:</strong> Fractal scaling leverages recursive composition, enabling the aggregation of transactions at multiple levels. This recursive approach allows for more efficient proofs, as transactions can be grouped hierarchically and processed collectively. It provides a higher level of scalability compared to Layer 2 ZK-rollups, which typically focus on aggregating transactions within a single layer.</li>\n<li><strong>Flexibility and Granularity:</strong> Fractal scaling offers greater flexibility and granularity in scaling solutions. It allows for different layers of aggregation and enables selective inclusion or exclusion of transactions based on specific criteria or requirements. This flexibility allows developers to optimize the scaling solution based on their application’s needs. In contrast, Layer 2 ZK-rollups may have predefined structures and aggregation mechanisms, limiting the level of customization and fine-tuning.</li>\n<li><strong>Layer Composition:</strong> Fractal scaling supports the composition of multiple layers, each providing its own level of scalability. This hierarchical composition allows for a more modular and flexible approach to scaling, where different layers can be added or removed depending on network demands. Layer 2 ZK-rollups, while providing scalability within a specific layer, may not offer the same seamless composition of multiple layers.</li>\n</ul>\n<h2><a name=\"background-13\" class=\"anchor\" href=\"https://ethresear.ch#background-13\"></a>Background</h2>\n<h3><a name=\"app-chain-thesis-14\" class=\"anchor\" href=\"https://ethresear.ch#app-chain-thesis-14\"></a>App-Chain Thesis</h3>\n<p>The app chain thesis proposes that every application should have its own rollup over Ethereum. This would create a separate layer for developers to build their applications with outsourced security of another blockchain and only use Ethereum for finality and data availability of the transactions processed on the rollup. The idea behind this thesis is to solve the problem of fragmentation in the blockchain community.</p>\n<p>With many new types and forms of rollups being developed, developers have to learn new tech stacks, understand new concepts, and build new architectures to make their applications compatible with each new type of chain. By creating a separate rollup for each application, developers can build their applications on customized blockchains, allowing for greater flexibility and innovation while maintaining the security and finality of the Ethereum network.</p>\n<p>With Layer 3 scaling solutions, Ethereum can be aligned with the app-chain thesis. As mentioned above, it would be more beneficial for applications and enterprises to launch their app-chains with interoperability and shared security. With shared execution environments and shared settlement layers, thousands of app-chains would be able to operate in a shared environment and focus on developing their business logic instead of bootstrapping security or community as compared to other chains.</p>\n<h3><a name=\"mev-capture-15\" class=\"anchor\" href=\"https://ethresear.ch#mev-capture-15\"></a>MEV Capture</h3>\n<p>MEV, or miner extractable value, refers to the value that miners can capture by reordering or censoring transactions on a blockchain. App-chains and Layer 3 rollups can capture their own MEV by creating their own customized blockchains that enable them to process and settle transactions off-chain. This allows them to capture the full value of the transactions they process, rather than letting other blockchains capture the MEV of their users. By building customized blockchains, app-chains and Layer 3 rollups can also achieve greater scalability and efficiency, enabling them to process more transactions at a lower cost than other blockchains.</p>\n<h3><a name=\"modular-architecture-and-hyper-scalability-16\" class=\"anchor\" href=\"https://ethresear.ch#modular-architecture-and-hyper-scalability-16\"></a>Modular Architecture and Hyper-Scalability</h3>\n<p>Rollups would be built with <a href=\"https://github.com/ethereum/research/wiki/A-note-on-data-availability-and-erasure-coding\" rel=\"noopener nofollow ugc\">modular architecture</a> with customized data availability solutions. Unlike monolithic blockchain architecture where Ethereum blockchain is used for data availability instead of acting as a layer just for proof generation and transaction finality. This creates unnecessary data storage of Ethereum chain resulting in clogging of data and high gas fee for users of beacon chain as well as rollups. In order to achieve hyper-scalability, rollups need to adopt modular architecture where they use off-chain data availability solutions such as validiums, data availability committees, honest DAC minority, etc. Modular architecture will not only reduce gas costs and allow more transactions into a single batch but also help all Layer 3 rollups achieve customized privacy and scaling.</p>\n<h2><a name=\"existing-zk-rollup-framework-17\" class=\"anchor\" href=\"https://ethresear.ch#existing-zk-rollup-framework-17\"></a>Existing ZK-Rollup Framework</h2>\n<p>ZK Rollups are a layer 2 scaling solution that uses cryptographic validity proofs to scale computation: each batch of transactions comes with a cryptographic proof (SNARK) that is verified by an Ethereum smart contract. This way every single transaction is fully verified by all Ethereum full nodes before a block is finalized. Zero-knowledge rollups (ZK-rollups) are layer 2 scaling solutions that move computation and state storage off-chain to increase throughput on Ethereum Mainnet. ZK-rollups can process thousands of transactions in a batch and then only post some minimal summary data to Mainnet. This summary data defines the changes that should be made to the Ethereum state and some cryptographic proof that those changes are correct.</p>\n<p>ZK-rollups use zero-knowledge proofs to verify the validity of transactions and state transitions without revealing any of the transaction details, thereby maintaining privacy. These proofs are verified by a smart contract on Ethereum Mainnet, allowing the network to confirm the validity of the rollup’s state transitions without having to verify each transaction individually. This reduces the computational load on Ethereum Mainnet, allowing it to process significantly more transactions per second.</p>\n<h2><a name=\"utilities-of-a-fractal-scaling-solution-18\" class=\"anchor\" href=\"https://ethresear.ch#utilities-of-a-fractal-scaling-solution-18\"></a>Utilities of a Fractal Scaling Solution</h2>\n<p><strong>L2 is for Scaling, L3s are for customized Scaling:</strong> Layer 3 rollups offer significant benefits for customized scaling on the Ethereum blockchain. Their customizability, specialized features, enhanced scalability, independent governance, ecosystem integration, security, and reliability make them a compelling choice for developers seeking tailored scaling solutions that align closely with their application’s requirements and objectives. L3s can be used for application specific rollups where developers can build their custom solutions with their business logic on their customized Layer 3 rollup.</p>\n<p><strong>Privacy in blockchains:</strong> Layer 3 rollups can improve privacy in blockchain networks through various mechanisms and techniques.</p>\n<ul>\n<li>Zero-Knowledge Proofs: Layer 3 rollups can utilize zero-knowledge proofs (ZKPs) to provide privacy guarantees. ZKPs allow for the verification of certain properties or computations without revealing the underlying data. This enables users to prove the validity of their transactions or the correctness of certain operations without disclosing sensitive information.</li>\n<li>Trusted/Private Data Availability: Developers and Enterprises who want to utilize the security of Ethereum in a trusted environment while keeping their data private in a trusted environment can create their custom validium data availability models for their Layer 3 rollups.</li>\n</ul>\n<p><strong>Utilities for Enterprises:</strong> Enterprises can utilize Layer 3 rollups for their custom use cases to build their solutions by utilizing the security and ecosystem of Ethereum in their trusted environment. Layer 3 customized enterprise specific rollups can be utilized best for these solutions:</p>\n<ul>\n<li>High Volume Transaction Systems</li>\n<li>Tokenization of Real-World Assets</li>\n<li>Customized Financial Solutions</li>\n<li>Privacy-Focused Applications</li>\n<li>Supply Chain and Logistics Solutions</li>\n<li>Gaming and NFT Utility Applications</li>\n</ul>\n<h2><a name=\"conclusion-19\" class=\"anchor\" href=\"https://ethresear.ch#conclusion-19\"></a>Conclusion</h2>\n<p>In conclusion, the integration of a fractal scaling solution over Type 3 ZK-EVM using Halo2 presents a significant advancement in the scalability and privacy of blockchain networks. By combining the power of fractal scaling, which enables recursive composition of computation, and the privacy-preserving properties of the ZK-EVM protocol, this solution offers a promising path forward for addressing the limitations of existing blockchain systems.</p>\n<p>Fractal scaling allows for the efficient aggregation of multiple transactions into a single proof, reducing the computational and storage overhead associated with processing and validating transactions. This recursive composition enables a higher throughput of transactions, thereby significantly improving the scalability of blockchain networks. By leveraging this approach, the Type 3 ZK-EVM protocol can benefit from enhanced scalability while preserving the security guarantees and decentralized nature of the underlying blockchain.</p>\n<p>The utilization of the Halo2 protocol further enhances the privacy aspects of the system. Halo2 leverages zero-knowledge proofs to enable secure computation and verification without revealing any sensitive information. By employing this protocol within the fractal scaling solution, users can conduct transactions with confidentiality, ensuring that their personal and financial information remains private while still benefiting from the scalability improvements. With this integration, blockchain systems can handle a significantly higher transaction volume while maintaining the security, decentralization, and privacy that users expect. As this technology continues to mature, it has the potential to unlock new possibilities and drive widespread adoption of blockchain technology across various industries and use cases.</p>\n            <p><small>2 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/fractal-scaling-on-ethereum-layer-3-rollups/19022\">Read full topic</a></p>","link":"https://ethresear.ch/t/fractal-scaling-on-ethereum-layer-3-rollups/19022","pubDate":"Sat, 16 Mar 2024 11:30:55 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19022"},"source":{"@url":"https://ethresear.ch/t/fractal-scaling-on-ethereum-layer-3-rollups/19022.rss","#text":"Fractal Scaling on Ethereum (Layer 3 Rollups)"},"filter":false},{"title":"Minimal fully recursive zkDA rollup with sharded storage","dc:creator":"snjax","category":"ZK Rollup","description":"<h2><a name=\"current-zk-rollup-state-1\" class=\"anchor\" href=\"https://ethresear.ch#current-zk-rollup-state-1\"></a>Current zk rollup state</h2>\n<p>zkRollups scale execution efficiently, but publish all blocks at L1. This is not scalable for storage and forbids recursive rollups: if we deploy a rollup on a rollup, we need to publish all the blocks of the inner rollup on the outer rollup, and the outer rollup will publish all its blocks on L1.</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/2X/e/e04f5c7f84d40add7840afa6a1f5fdc05bbc30af.svg\" alt=\"native rollup\" data-base62-sha1=\"w0kZ1mEUZhJvjC0uhtXPS5jQqGH\" width=\"401\" height=\"445\"></p>\n<p>There were some attempts to solve this problem, like validiums, but they are weak on both decentralization and security (2 of 3 in Vitalik’s trilemma).</p>\n<h2><a name=\"existing-improvements-in-unlocking-data-availability-and-decentralized-storage-2\" class=\"anchor\" href=\"https://ethresear.ch#existing-improvements-in-unlocking-data-availability-and-decentralized-storage-2\"></a>Existing improvements in unlocking data availability and decentralized storage</h2>\n<h3><a name=\"chia-3\" class=\"anchor\" href=\"https://ethresear.ch#chia-3\"></a>Chia</h3>\n<p>Chia introduced a novel consensus algorithm called Proof of Space and Time (PoST), which provides a more decentralized and energy-efficient alternative to Proof of Work (PoW): Proof of Space-Time (PoST). PoST is a consensus algorithm that uses storage space as a resource to secure the network.<br>\nThe current capacity of Chia Network is 33 EiB.</p>\n<h3><a name=\"ethstorage-4\" class=\"anchor\" href=\"https://ethresear.ch#ethstorage-4\"></a>EthStorage</h3>\n<p>Ethstorage is replication-based DA and storage, managed by a smart contract.</p>\n<h2><a name=\"our-results-5\" class=\"anchor\" href=\"https://ethresear.ch#our-results-5\"></a>Our results</h2>\n<p>In our <a href=\"https://ethresear.ch/t/blockchain-sharded-storage-web2-costs-and-web3-security-with-shamir-secret-sharing/18881\">research draft</a> we propose a solution for storage and data availability, friendly to zk rollups and unlocking new scalability opportunities.</p>\n<h3><a name=\"sharding-instead-of-replication-6\" class=\"anchor\" href=\"https://ethresear.ch#sharding-instead-of-replication-6\"></a>Sharding instead of replication</h3>\n<p>It is proposed to use <span class=\"math\">k</span> of <span class=\"math\">n</span> threshold data representation. So, any <span class=\"math\">k</span> numbers from the source file are transformed into <span class=\"math\">n</span> numbers. And any <span class=\"math\">k</span> of these <span class=\"math\">n</span> numbers can restore the source <span class=\"math\">k</span> numbers. This is called Shamir’s Secret Sharing.</p>\n<p>This approach allows us to utilize storage 10-20 times more efficiently than the replication-based approach, according to our modeling.</p>\n<p>Also, it gives us better protection from physical-level attacks, like target node destruction.</p>\n<h3><a name=\"unlimited-horizontal-scalability-7\" class=\"anchor\" href=\"https://ethresear.ch#unlimited-horizontal-scalability-7\"></a>Unlimited horizontal scalability</h3>\n<p>We propose to use a 2-level nested rollup structure (below we will describe, why it is possible). The top-level rollup manages participants of low-level rollups and mixes them to prevent the accumulation of malicious participants in one low-level rollup. Low-level rollups manages the data, stored in the nodes.</p>\n<h3><a name=\"polynomial-commitments-everywhere-8\" class=\"anchor\" href=\"https://ethresear.ch#polynomial-commitments-everywhere-8\"></a>Polynomial commitments everywhere</h3>\n<p>We propose to use Merkle trees on the top level of database. However, the minimal structure is a polynomial commitment to a cluster of data. So, it is very friendly to rollups, because we can use the same polynomial commitment to represent the rollup’s block.</p>\n<p>Also, out of the box we have data availability oracle (just provide random polynomial lookup on the commitment) and all linear algebra we needed for sharding.</p>\n<h3><a name=\"data-mining-9\" class=\"anchor\" href=\"https://ethresear.ch#data-mining-9\"></a>Data mining</h3>\n<p>Nodes can use the data for mining, like in Chia. And the result of mining is zero-knowledge proof of data availability.</p>\n<p>The complexity of storage is leveled, so it is the same complexity to store random data or zeros.</p>\n<p>Nodes can join to network with trustless zk proof of their capacity.</p>\n<h2><a name=\"bring-it-all-together-10\" class=\"anchor\" href=\"https://ethresear.ch#bring-it-all-together-10\"></a>Bring it all together</h2>\n<p>ZK Rollups usually publish on-chain proof of execution and data of the block.<br>\nBut our data availability and proof of storage are zk. So, we can merge it all together and publish the proof of execution and data availability and storage in one single ZK proof.</p>\n<p>It unlocks the deployment of rollups on rollups, and the rollups on rollups on rollups, and so on. And way to transform Web2 into Web3.</p>\n<p>Also, we can prevent the bloating of the blockchain: if we publish the snapshot state of the rollup, previous history could be removed.</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/2X/e/e3e434b0992b133f32c8c4a33bcf390e04e571b6.svg\" alt=\"zkDA rollup\" data-base62-sha1=\"ww1juXvygtiVCbaiMwK42atJbEO\" width=\"482\" height=\"500\"></p>\n<h2><a name=\"some-economics-11\" class=\"anchor\" href=\"https://ethresear.ch#some-economics-11\"></a>Some economics</h2>\n<p>On 1st Jan 2024 cost of storage, 1GiB was:</p>\n<ul>\n<li>Ethereum $1.8M</li>\n<li>EthStorage $10k</li>\n<li>Celestia $300</li>\n<li>Near $10</li>\n</ul>\n<p>Based on <a href=\"https://www.hetzner.com/dedicated-rootserver/sx294/\" rel=\"noopener nofollow ugc\">Hetzner sx294</a> with 8 blowup factor (what we need for &gt;100 bits of security), the annual cost of storage 1GB is $0.15 usd.</p>\n<p>The cost will be lower on specialized rigs.</p>\n<h2><a name=\"call-for-discussion-and-feedback-12\" class=\"anchor\" href=\"https://ethresear.ch#call-for-discussion-and-feedback-12\"></a>Call for discussion and feedback</h2>\n<p>We believe our proposed solution has the potential to significantly improve the scalability and efficiency of zk rollups and upgrade Web2 to Web3. However, we acknowledge that this is still a research draft and there may be challenges or considerations we haven’t fully addressed.</p>\n<p>We welcome discussion, feedback, and constructive criticism from the community. If you have insights, ideas, or see potential issues with our approach, please share them.</p>\n            <p><small>7 posts - 3 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/minimal-fully-recursive-zkda-rollup-with-sharded-storage/19020\">Read full topic</a></p>","link":"https://ethresear.ch/t/minimal-fully-recursive-zkda-rollup-with-sharded-storage/19020","pubDate":"Sat, 16 Mar 2024 09:05:24 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19020"},"source":{"@url":"https://ethresear.ch/t/minimal-fully-recursive-zkda-rollup-with-sharded-storage/19020.rss","#text":"Minimal fully recursive zkDA rollup with sharded storage"},"filter":false},{"title":"Initial Analysis of Stake Distribution","dc:creator":"Julian","category":"Economics","description":"<p>Many thanks to <a href=\"https://twitter.com/casparschwa?s=21&amp;t=YeRrbBitxQ2f6XFXk1HmMg\" rel=\"noopener nofollow ugc\">Caspar</a>, <a href=\"https://twitter.com/adietrichs?s=21&amp;t=YeRrbBitxQ2f6XFXk1HmMg\" rel=\"noopener nofollow ugc\">Ansgar</a>, <a href=\"https://twitter.com/barnabemonnot?s=21&amp;t=YeRrbBitxQ2f6XFXk1HmMg\" rel=\"noopener nofollow ugc\">Barnabé</a>, <a href=\"https://twitter.com/weboftrees?lang=en\" rel=\"noopener nofollow ugc\">Anders</a> and <a href=\"https://twitter.com/soispoke?s=21&amp;t=YeRrbBitxQ2f6XFXk1HmMg\" rel=\"noopener nofollow ugc\">Thomas</a> for feedback and review. Review <span class=\"math\">\\neq</span> endorsement.</p>\n<h2><a name=\"introduction-1\" class=\"anchor\" href=\"https://ethresear.ch#introduction-1\"></a>Introduction</h2>\n<p>Ethereum issues ETH to validators for performing their consensus duties. The amount of issuance depends on the amount of ETH staked. The current issuance curve may result in a very high long-term staking ratio. This post aims to analyze whether a change in the level of issuance, as <a href=\"https://ethereum-magicians.org/t/electra-issuance-curve-adjustment-proposal/18825\" rel=\"noopener nofollow ugc\">proposed to be implemented in the next Electra upgrade</a>, affects the distribution of staking mediums investors use. We differentiate between three mediums of staking: 1) investors may solo-stake, 2) investors may deposit their tokens with a decentralized staking service provider (SSP), or 3) investors may deposit their tokens with a centralized SSP. Subsequently, we define the cost structures of each staking medium. Finally, we model a linear programming problem in which an investor decides what fraction of their endowment to hold or stake via which medium based on the expected monetary return and an investor’s non-monetary preference, such as convenience, trust, and decentralization. Our main result is that the distribution of stake does not depend on the level of issuance. We show how the model can be used with two examples.</p>\n<p>This post presents a minimum non-trivial model to analyze the distribution of stake with respect to the level of issuance. We refer the reader to this <a href=\"https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448\">post</a> for an explanation of why a change to the issuance curve may be useful. This post does not aim to discuss the motivation for an issuance reduction, nor does it aim to be maximally precise about the cost structures of different SSPs. What this post does aim to do is to ground the conversation around staking distributions in an addressable manner.</p>\n<h2><a name=\"model-2\" class=\"anchor\" href=\"https://ethresear.ch#model-2\"></a>Model</h2>\n<p>Consider an investor who receives a large endowment, <span class=\"math\">E</span>. It needs to decide how to invest this wealth. It can invest in a combination of the following products: solo-staking, “decentralized” SSP, “centralized” SSP, or simply holding the endowment.</p>\n<p>We want to highlight the words decentralized and centralized because, in practice, the state of an SSP is not so binary. Decentralization is a spectrum, and an SSP may be more decentralized than other SSPs while still being more centralized than others. For simplicity, we assume an SSP is either decentralized or centralized.</p>\n<p>Let <span class=\"math\">y</span> be the yield from issuance and MEV that each unit of stake accrues over some finite time interval.</p>\n<h2><a name=\"cost-structure-3\" class=\"anchor\" href=\"https://ethresear.ch#cost-structure-3\"></a>Cost Structure</h2>\n<p>Staking also incurs costs. The cost structure of the different staking mechanisms is given as</p>\n<ul>\n<li><strong>Solo-Staking.</strong> Solo-stakers need to acquire hardware in order to function; we model this as a fixed cost solo-stakers incur, <span class=\"math\">C^{Solo}_{F}</span>. Furthermore, unlike SSPs, solo-stakers forgo the liquidity of their stake as they cannot issue liquid staking tokens (LSTs). We model this as a variable cost per unit of stake, <span class=\"math\">C^{Solo}_{V}</span>, that resembles the <em>liquidity gap.</em> The liquidity gap is the foregone returns that a solo-staker could have gotten if it were able to issue LSTs. Therefore, the cost function of a solo-staker is given by the following: <span class=\"math\">S^{Solo} \\geq 0</span> is the amount of solo-staked capital deposited by the investor.</li>\n</ul>\n<div class=\"math\">\nC^{Solo} = C^{Solo}_{F} + C^{Solo}_{V} \\cdot S^{Solo}\n</div>\n<ul>\n<li><strong>Decentralized SSP.</strong> Decentralized SSPs must create nodes at different locations to remain decentralized. We assume that a decentralized SSP creates a new node for every <span class=\"math\">K \\cdot 32</span> ETH staked with the SSP. Furthermore, when staking with a decentralized SSP, the investor receives liquid staking tokens as a receipt for their provided capital, meaning that there is no - or little -  loss in the liquidity of capital. Therefore, we model the cost of a decentralized SSP as a step function that incurs a variable cost for every <span class=\"math\">K \\cdot 32</span> ETH staked with the decentralized SSP. <span class=\"math\">X^{DSSP} &gt; 0</span> is the amount of exogenous stake deposited with the decentralized SSP, and <span class=\"math\">S^{DSSP} \\geq 0</span>  is the amount of stake deposited by the investor.</li>\n</ul>\n<div class=\"math\">\nC^{DSSP} = C^{DSSP}_{V} \\cdot \\lfloor \\frac{X^{DSSP} + S^{DSSP}}{32 \\cdot K} \\rfloor\n</div>\n<ul>\n<li><strong>Centralized SSP.</strong> Centralized SSPs have negligible variable costs per unit of stake. However, they have large fixed costs, such as legal fees and infrastructure costs, that come with setting up a company to run a scalable staking operation. Moreover, stake deposited with a centralized SSP can also be exchanged for an LST. Thus, we model the costs of a centralized SSP as follows. In practice, we observe <span class=\"math\">C^{CSSP}_{F} \\gg C^{Solo}_{F}</span>.</li>\n</ul>\n<div class=\"math\">\nC^{CSSP} = C^{CSSP}_{F}\n</div>\n<h2><a name=\"returns-4\" class=\"anchor\" href=\"https://ethresear.ch#returns-4\"></a>Returns</h2>\n<p>The return on stake is given by the yield minus the costs. We assume SSPs <em>socialize</em> the costs and do not discriminate between stakers. Then, the yield on each unit of stake is defined as follows.</p>\n<div class=\"math\">\nr^{i} = y - \\frac{1}{X^{i} + S^{i}}C^{i}\n</div>\n<p>where <span class=\"math\">i = \\{Solo, DSSP,CSSP \\}</span>. Furthermore, we define <span class=\"math\">r^{Hold} = 0</span>. Let <span class=\"math\">r^{T} = \\{r^{Solo},  r^{DSSP},  r^{CSSP}, r^{Hold}\\}</span> denote the vector of returns.</p>\n<p>An investor measures its utility as follows</p>\n<div class=\"math\">\nU = (r + \\gamma)^{T} \\cdot S\n</div>\n<p>where <span class=\"math\">S^{T} = \\{S^{Solo}, S^{DSSP}, S^{CSSP}, S^{Hold}\\}</span> is the vector of stake and <span class=\"math\">\\gamma^{T} = \\{ \\gamma^{Solo}, \\gamma^{DSSP}, \\gamma^{CSSP}, \\gamma^{Hold}\\}</span> is the preference vector of the investor. It represents factors that an investor cares about that are not expressed in yield, such as decentralization, trust, technical capabilities, and convenience. In this post, we assume that preferences are fixed and do not depend on other factors, such as other stakers or the level of issuance. In the discussion, we add more nuance to this assumption; therefore, we present it here as Hypothesis 1.</p>\n<blockquote>\n<p><strong>Hypothesis 1.</strong> Investor’s preferences are independent of the level of yield.</p>\n</blockquote>\n<h2><a name=\"main-results-5\" class=\"anchor\" href=\"https://ethresear.ch#main-results-5\"></a>Main Results</h2>\n<p>The optimization problem of the investor is then given as</p>\n<div class=\"math\">\n\\max_{S} \\quad (r + \\gamma)^{T}S \\\\\n\\text{subject to} \\quad S \\in \\mathbb{R}_{+}^{4} \\\\\n\\qquad 1^{T}S = E\n \n</div>\n<p>This is a very simple linear programming problem. The Fundamental Theorem of Linear Programming states that if there is an optimal solution to a linear programming problem, and if the feasible region is non-empty and bounded, then there exists an optimal solution at one of the vertices of the feasible region. Therefore, solving the optimization problem is as simple as choosing the vertex that leads to the highest utility for the investor. This will lead to the investor depositing all assets in the maximum component of <span class=\"math\">(r + \\gamma)</span>. This leads us to our first result presented in Corollary 1.</p>\n<blockquote>\n<p><strong>Corollary 1.</strong> If an investor decides to stake, it will stake its entire endowment via one medium.</p>\n</blockquote>\n<p>We assume that an investor deposits their entire endowment with one staking medium in the event of multiple optimal solutions</p>\n<p>The decision whether to stake or not depends on the level of issuance. If issuance is too low, an investor will opt to hold ETH instead of staking it. However, by using Hypothesis 1, it becomes clear that an investor’s choice of a staking medium does not depend on the level of issuance. Aggregating this across investors, we obtain our main result presented in Theorem 1.</p>\n<blockquote>\n<p><strong>Theorem 1.</strong> The level of (issuance) yield does not affect the staking mediums used by individual stakers.</p>\n</blockquote>\n<p>Informally, Theorem 1 supports the argument that competition between staking mediums is similar at every level of issuance. Therefore, a lower level of issuance, as proposed to be implemented in Electra, does not necessarily lead to a decrease in the number of solo stakers. Further research may expand this model into a game-theoretic model that includes frictions investors may see when (un)staking.</p>\n<h2><a name=\"example-6\" class=\"anchor\" href=\"https://ethresear.ch#example-6\"></a>Example</h2>\n<p>In this section, we consider two examples. The first example is of an investor who only cares about monetary gains and has no preferences for, e.g., decentralization or trust. It shows that this will lead to the investor depositing its assets with a centralized SSP (in the case of no rent extraction). The second example is of a solo staker. This example is constructed to show how the model allows putting a monetary amount on revealed preferences.</p>\n<p><strong>Indifferent Investor</strong></p>\n<p>Consider an investor who does not care about any non-monetary factors and solely wants to maximize returns. Let the preferences of this investor be given as <span class=\"math\">\\gamma = 0</span>. We find then, by Lemma 1, that this investor deposits its entire endowment with the medium that gives the highest return. For this example, consider the price of 1 ETH to be 3000 USD and assume <span class=\"math\">y = 4\\%</span> per year.</p>\n<p>Solo-staking would cost the investor 1000 USD in fixed costs, depreciated over 10 years, so say 100 USD fixed per year. Furthermore, assuming the liquidity gap is <span class=\"math\">1\\%</span>, then the return for solo staking is roughly <span class=\"math\">2.9\\%</span> per year.</p>\n<p>Suppose a decentralized SSP has variable costs of 3000 USD per node per year, and creates a new node for every <span class=\"math\">1,000,000</span> ETH deposited (<span class=\"math\">K = 31,250</span>). Assume there is 10 million ETH staked through this SSP. The reward for staking through the decentralized SSP is then around <span class=\"math\">3.7\\%</span>.</p>\n<p>Finally, the centralized SSP has fixed costs of 10 million USD per year and it has 5 million ETH staked with it. The reward for staking with the centralized SSP is then around <span class=\"math\">3.9\\%</span>.</p>\n<p>Therefore, the indifferent investor will choose to stake with the centralized SSP.</p>\n<p><strong>Solo Staker</strong></p>\n<p>Consider an investor who is a solo-staker. Furthermore, assume that <span class=\"math\">r^{T} = \\{r^{Solo}, r^{DSSP}, r^{CSSP}, r^{Hold}\\} = \\{2\\%,  3\\%,  4\\%, 0\\}</span>, then we know that <span class=\"math\">r^{Solo} +\\gamma^{Solo} \\geq r^{CSSP} + \\gamma^{CSSP}  \\implies \\gamma^{Solo} \\geq 2\\%</span>. Therefore, if this solo-staker has staked <span class=\"math\">32</span> ETH,  its preference for solo-staking is worth at least <span class=\"math\">0.64</span> ETH per year in monetary terms.</p>\n<h2><a name=\"conclusion-discussion-7\" class=\"anchor\" href=\"https://ethresear.ch#conclusion-discussion-7\"></a>Conclusion &amp; Discussion</h2>\n<p>In conclusion, this model presents an investor’s optimization problem as a linear programming problem of the monetary and non-monetary returns that an investor will gain from depositing stake via a certain staking medium. Corollary 1 shows that an investor who stakes deposits their entire stake through one staking medium. Our main result is presented in Theorem 1. Since preferences do not depend on the level of yield, we find that the level of (issuance) yield does not affect the distribution of staking mediums used by investors. Although the model is very simple and does not consider frictions, this Theorem could be used to argue that a change in issuance, as proposed in Electra, will not change the distribution of staking mediums used.</p>\n<p>The model presented in this post is meant as a minimum non-trivial model to analyze the distribution of stake with respect to the level of issuance. Clearly, this model is very simplified and makes idealized assumptions about the cost structure of different staking mediums. The intention is not to be maximally precise but to capture the core characteristics that differentiate staking types. This post aims to ground the conversation around staking distributions in an addressable manner. Further, this model’s assumptions can be adjusted accordingly, and the reader is invited to do so.</p>\n<p>Finally, some other questions that might be considered are:</p>\n<ul>\n<li>How important are other investors’ investment decisions for an individual investor? For example, an investor’s preference for a staking medium may depend on the number of other investors who stake through it. Liquid staking tokens that are owned by more people could provide more use cases.</li>\n<li>As issuance reduces, which investors will unstake first? Potentially liquid stakers will unstake earlier as the SSP fees reduce future expected profits, whereas solo stakers’ costs can largely be considered sunk costs, thus not reducing future expected profits. In <a href=\"https://podcasts.apple.com/us/podcast/ethereum-devs-debate-account-abstraction-eips-for-electra/id1728091874?i=1000648049543\" rel=\"noopener nofollow ugc\">this podcast</a>, Christine Kim and nixo.eth discuss that liquid staking protocols will likely see investors unstake their capital quickly as frictions are low. Solo stakers may not unstake as quickly as frictions are higher.</li>\n<li>Some of the investors’ preferences can potentially be endogenized into an extended model. How do preferences develop over time? Is there a potential survivorship bias in the preferences?</li>\n</ul>\n            <p><small>13 posts - 4 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/initial-analysis-of-stake-distribution/19014\">Read full topic</a></p>","link":"https://ethresear.ch/t/initial-analysis-of-stake-distribution/19014","pubDate":"Fri, 15 Mar 2024 14:01:05 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19014"},"source":{"@url":"https://ethresear.ch/t/initial-analysis-of-stake-distribution/19014.rss","#text":"Initial Analysis of Stake Distribution"},"filter":false},{"title":"How AI Revolutionizes Ethereum, Another View on “AI+Blockchain”","dc:creator":"Mirror","category":"Applications","description":"<h1><a name=\"how-ai-revolutionizes-ethereum-another-view-on-aiblockchain-1\" class=\"anchor\" href=\"https://ethresear.ch#how-ai-revolutionizes-ethereum-another-view-on-aiblockchain-1\"></a>How AI Revolutionizes Ethereum, Another View on “AI+Blockchain”</h1>\n<p><a href=\"https://github.com/Mirror-Tang\" rel=\"noopener nofollow ugc\">Mirror Tang</a>, Salus &amp; <a href=\"https://cn.linkedin.com/in/yixin-ren-a6a49116b\" rel=\"noopener nofollow ugc\">Yixin Ren</a>, Sequoia Capital China</p>\n<p>Over the past year, as generative AI has repeatedly surpassed public expectations, the productivity revolution led by AI has swept through the cryptocurrency world like a tidal wave. We have witnessed numerous AI concept projects create wave after wave of wealth myths in the secondary market, while an increasing number of developers have plunged into developing their own “AI+Crypto” projects.</p>\n<p>However, upon closer inspection, it’s not hard to find that there is a widespread issue of homogeneity among these projects, and the efforts of most projects remain at the level of improving “production relations,” such as organizing computing power through decentralized networks or creating a “decentralized Hugging Face.” Very few projects genuinely integrate and innovate at the fundamental technological level. We believe this phenomenon is due to a “domain bias” between the fields of AI and blockchain. Although there is a broad intersection between these two fields, very few people deeply understand them. For example, for AI developers, delving into the technical implementation and historical infrastructure of Ethereum poses a significant challenge, let alone proposing in-depth optimization solutions.</p>\n<p>Take machine learning, a cornerstone of the AI field, as an example. It allows machines to make decisions based on data learning without explicit programming instructions, showing tremendous potential in data analysis and pattern recognition and becoming well-known in the web2 era. However, due to the limitations of the early era, even at the forefront of blockchain technology innovation like Ethereum, its architecture, network, and governance mechanisms have yet to employ machine learning as an effective tool for solving complex problems.</p>\n<p>We firmly believe that “great innovations often arise from the collision of cross-disciplinary fields.” The purpose of writing this article is to help AI developers gain a deeper understanding of the mysteries of blockchain, while also offering new perspectives to developers in the Ethereum community. We first explored the technical foundations of Ethereum and then proposed applying machine learning algorithms within the Ethereum network to enhance its security, efficiency, and scalability. We hope that this case will serve as an opportunity to provide a new perspective to the market, inspiring more innovation and cross-fertilization in the “AI+Blockchain” direction within the developer community.</p>\n<h2><a name=\"technical-implementation-of-ethereum-2\" class=\"anchor\" href=\"https://ethresear.ch#technical-implementation-of-ethereum-2\"></a>Technical Implementation of Ethereum</h2>\n<p><strong>1. Basic Data Structure</strong></p>\n<p>The essence of blockchain is a series of interconnected blocks forming a chain, with the key distinction between different blockchains lying in their chain configuration. This is an indispensable part of the genesis block, which constitutes the initial stage of a blockchain. In Ethereum, the chain configuration not only differentiates between various chains but also marks significant upgrade protocols and key events. For example, the DAOForkBlock signifies the block height of the hard fork following the DAO attack, while the ConstantinopleBlock indicates the specific block height of the Constantinople upgrade. Major upgrades, including numerous improvement proposals, have specific fields set to denote the respective block heights. Additionally, Ethereum’s various testnets and the mainnet are uniquely identified by their ChainID.</p>\n<p>As the starting point of the blockchain, the genesis block is the zeroth block that all other blocks directly or indirectly reference. Therefore, it’s crucial for nodes to load the correct genesis block information at startup, and this information cannot be arbitrarily modified. The configuration of the genesis block includes the aforementioned chain configuration as well as important fields such as mining rewards, timestamps, difficulty, and gas limits. It’s worth noting that Ethereum’s consensus mechanism has transitioned from Proof of Work (PoW) to Proof of Stake (PoS).</p>\n<p>Ethereum accounts are divided into two types: external accounts, controlled exclusively by a private key, and contract accounts, which have no private key control and can only be operated through calls made by external accounts. Both have unique addresses. The world state of Ethereum is a tree made up of all Ethereum accounts, with each account corresponding to a leaf node that stores the account’s state information, including various account data and code information.</p>\n<p>Transactions are the core of Ethereum, a decentralized platform aimed at facilitating transactions and executing contracts. An Ethereum block contains a series of packaged transactions and additional information, specifically divided into the block header and block body. The block header contains evidence that links all blocks into a chain, such as the hash of the previous block, and represents the global state of Ethereum with the state root, transaction root, receipt root, among other additional data like difficulty and nonce. The block body stores the list of transactions and the list of uncle block headers (with the transition to PoS, uncle block references no longer exist).</p>\n<p>Transaction receipts provide the outcome and additional information post-transaction execution, offering details that cannot be directly obtained from the transaction itself. This includes consensus content, transaction details, and block information, such as whether the transaction was successful, transaction logs, and gas consumption. Analyzing receipt information facilitates debugging of smart contract code and optimization of gas consumption, serving as a confirmation that the transaction has been processed by the network, and its results and impact can be reviewed.</p>\n<p>In Ethereum, gas fees essentially act as a transaction fee for sending tokens, executing contracts, transferring Ether, or performing other blockchain operations. These operations require gas fees because Ethereum consumes computational resources while processing transactions. Therefore, gas fees must be paid to execute these operations. Ultimately, gas fees are paid to miners as a service fee, with the cost formula being Fee = Gas Used * Gas Price, which is the actual amount of gas consumed multiplied by the gas price, set by the transaction initiator, typically influencing the transaction processing speed. If set too low, the transaction may not be executed; additionally, a gas limit must be set to prevent unexpected high gas consumption due to contract errors.</p>\n<p><strong>2. Transaction Pool</strong></p>\n<p>In Ethereum’s decentralized ecosystem, despite the vast number of transactions, the transaction processing rate per second pales in comparison to centralized systems. To manage the sheer volume of transactions, nodes maintain a transaction pool for proper handling. Transactions are broadcast through the P2P network, where a node broadcasts executable transactions to its neighboring nodes, which in turn broadcast the transaction further. This process enables a transaction to rapidly spread across the entire Ethereum network within six seconds.</p>\n<p>Transactions in the pool are categorized into executable and non-executable states, with executable transactions given higher priority to be executed and packed into blocks. Initially, all transactions entering the pool are non-executable and may later become executable. Executable and non-executable transactions are recorded in the “pending” and “queue” containers, respectively.</p>\n<p>The transaction pool also maintains a special list of local transactions, which enjoy several advantages, such as higher processing priority, no limitations by transaction volume, and the ability to be immediately reloaded into the pool upon node restart. Local transaction persistence is achieved through a journal, ensuring unfinished local transactions can be recovered after a node restart and periodically updated.</p>\n<p>Before entering the queue, transactions undergo legality checks, including various types of inspections such as preventing DOS attacks, ensuring transactions are non-negative, and adhering to transaction gas limits. The transaction pool is fundamentally composed of the “queue” and “pending” sections, together comprising all transactions. After passing the legality checks, further evaluations are conducted, including assessing whether the transaction queue has reached its limit, and determining if remote transactions (non-local transactions) have the lowest gas fees in the pool to replace the lowest-priced transactions. For executable transactions, by default, only those with a 10% higher gas fee are allowed to replace transactions waiting to be executed, which are then stored as non-executable. Additionally, during the maintenance of the transaction pool, invalid or over-limit transactions are removed, and transactions meeting specific criteria are replaced.</p>\n<p><strong>3. Consensus Algorithm</strong></p>\n<p>In Ethereum’s early days, the consensus mechanism primarily relied on calculating the difficulty value hash. That is, by computing the hash value of a block and ensuring it meets a specific difficulty condition to validate the block’s legitimacy. As Ethereum’s consensus algorithm shifted from PoW to PoS, theories related to mining are no longer the focus of discussion. Here, we primarily introduce the PoS algorithm.</p>\n<p>In September 2022, Ethereum completed the merger of the Beacon Chain and implemented the PoS algorithm. Under the PoS algorithm, the block generation time in the Ethereum network is stabilized at approximately 12 seconds. Users gain the opportunity to become validators by staking Ether, with the system randomly selecting a group of staking participants as validators. Each epoch consists of 32 slots, and in each slot, one validator is chosen as the proposer to generate a new block, while the rest of the validators form a committee responsible for verifying the legitimacy of the proposed block and reviewing the validity of blocks from the previous epoch. The PoS algorithm not only stabilizes and accelerates the speed of block generation but also significantly reduces the waste of computational resources.</p>\n<p><strong>4. Signature Algorithm</strong></p>\n<p>Ethereum inherited the signature algorithm standard from Bitcoin, adopting the same secp256k1 elliptical curve and using the Elliptic Curve Digital Signature Algorithm (ECDSA) for signing. Simply put, a complete signature contains three parts: R, S, and V. Each time a signature is generated, a random number is introduced, where R and S constitute the original output of the ECDSA. V, as the recovery field, represents the number of attempts needed to successfully recover the public key from the message content and signature. This is because, based on the R value, multiple points on the elliptical curve that satisfy the conditions may be found.</p>\n<p>The signing process can be summarized as follows: the transaction data and relevant signature information are encoded using RLP and hashed, then the signature is completed using the private key and the ECDSA algorithm, specifically using the secp256k1 elliptical curve. Integrating the signature result with the transaction data forms a signed transaction that can be broadcasted.</p>\n<p>In terms of data structure, Ethereum has integrated the Merkle Patricia Tree (MPT), which can efficiently store and verify a large volume of data. The MPT combines the cryptographic hash properties of the Merkle tree with the key path compression ability of the Patricia tree, offering a solution that both ensures data integrity and supports efficient querying.</p>\n<p><strong>5. MPT</strong></p>\n<p>In the Ethereum system, the Merkle Patricia Tree (MPT) plays a crucial role in storing all states and transaction data, ensuring that any change in data is directly reflected in the variation of the tree’s root hash. This mechanism means that by simply verifying the root hash, we can prove the integrity and authenticity of the data without having to inspect the entire database one by one. MPT, through the interaction of four different types of nodes—leaf nodes, extension nodes, branch nodes, and null nodes—forms a tree structure that can flexibly respond to dynamic changes in data. Whenever data is updated, MPT reflects these changes by adding, deleting, or modifying nodes, and accordingly updates the tree’s root hash value. Thanks to the encryption characteristics of hash functions, any minor change in data will lead to a significant variation in the root hash, thereby ensuring the high security and consistency of the data. Moreover, MPT’s unique design also supports “light client” verification, enabling nodes to verify the existence or state of specific information solely based on the tree’s root hash and the necessary path nodes, significantly reducing the demands on data storage and processing.</p>\n<p>With the help of MPT, Ethereum not only improves the efficiency of data management and access speed but also strengthens network security and decentralization features, providing a solid foundation for the stable operation and continuous development of the entire Ethereum network.</p>\n<p><strong>6. State Machine</strong></p>\n<p>Ethereum’s core architecture ingeniously integrates the concept of state machines, with one of its cores being the Ethereum Virtual Machine (EVM), a virtual environment for executing all smart contract code. This makes Ethereum fundamentally a global state transition system. Each block’s processing is essentially a transition from one globally shared state to another. This unique design not only ensures the unity and decentralization of the Ethereum network but also ensures that the outcomes of smart contracts are predictable and tamper-proof.</p>\n<p>In the Ethereum network, “state” refers to the record of current information for all accounts, including account balances, stored data, and smart contract code. The execution of transactions triggers the EVM to calculate the new state based on the content of the transactions and records this state transition process in a highly efficient and secure manner through the MPT. Each change in state not only updates the data of the accounts but also triggers an update in the MPT structure, reflected in changes to the root hash value.</p>\n<p>The collaboration between the EVM and MPT is crucial, providing solid guarantees of data integrity for state transitions within the Ethereum network. When the EVM processes transactions and updates account states, the related MPT nodes are updated synchronously to reflect these changes. The hash links of each node in the MPT ensure that any change in state leads to an update of the root hash, which is then included in the new block, thereby ensuring the overall consistency and security of the Ethereum network state. Let’s delve deeper into the EVM.</p>\n<p><strong>7. EVM</strong></p>\n<p>EVM is the foundational cornerstone for the execution of smart contracts and state transitions within Ethereum, enabling Ethereum to be recognized as a global computer. With its Turing completeness, the EVM endows Ethereum smart contracts with the ability to perform computations of any complexity. Furthermore, the introduction of the gas mechanism by the EVM effectively prevents infinite loops during contract execution, thereby ensuring the network’s high stability and security.</p>\n<p>On a deeper technical level, the EVM is a stack-based virtual machine that executes smart contracts through Ethereum-specific bytecode. Developers use high-level programming languages, such as Solidity, to write smart contracts and then compile them into bytecode that the EVM can recognize, enabling the deployment and execution of smart contracts. The existence of the EVM is a core driver of innovation within the Ethereum, not only ensuring the smooth operation of smart contracts but also laying a solid technical foundation for the development of decentralized applications (DApps). With the EVM, Ethereum is dedicated to building a decentralized, secure, and open digital future.</p>\n<h2><a name=\"historical-review-3\" class=\"anchor\" href=\"https://ethresear.ch#historical-review-3\"></a>Historical Review</h2>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/d/d533b678b318bc138191769817edc956d1320d7c.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/d533b678b318bc138191769817edc956d1320d7c\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/d/d533b678b318bc138191769817edc956d1320d7c_2_602x441.jpeg\" alt=\"\" data-base62-sha1=\"uq4uI1CnoUo4TWKOOVOdSQSAZbu\" width=\"602\" height=\"441\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/d/d533b678b318bc138191769817edc956d1320d7c_2_602x441.jpeg, https://ethresear.ch/uploads/default/optimized/2X/d/d533b678b318bc138191769817edc956d1320d7c_2_903x661.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/d/d533b678b318bc138191769817edc956d1320d7c_2_1204x882.jpeg 2x\" data-dominant-color=\"EEEEEE\"></a></div><br>\nFigure 1: Ethereum Historical Review<p></p>\n<h1><a name=\"challenges-faced-by-ethereum-4\" class=\"anchor\" href=\"https://ethresear.ch#challenges-faced-by-ethereum-4\"></a>Challenges Faced by Ethereum</h1>\n<h2><a name=\"security-5\" class=\"anchor\" href=\"https://ethresear.ch#security-5\"></a>Security</h2>\n<p>Smart contracts, as computer programs on the Ethereum blockchain, have enabled developers to create a diverse array of applications, including lending platforms, decentralized exchanges, insurance, crowdfunding, social media, and NFTs, among many other areas. The security of smart contracts is particularly critical for these applications, as they directly involve the handling and management of cryptocurrencies. Thus, any vulnerabilities in smart contracts or exposure to malicious attacks could directly endanger financial security and lead to significant economic losses. For example, on February 26, 2024, the DeFi lending platform Blueberry Protocol suffered an attack due to a logical error in its smart contract, resulting in a loss of approximately 1.4 million dollars.</p>\n<p>Smart contract vulnerabilities are varied and include issues such as flaws in business logic, improper access control, insufficient data validation, reentrancy attacks, and Denial of Service (DOS) attacks. These vulnerabilities can lead to erroneous contract execution, impacting the normal operation of smart contracts. For instance, in DOS attacks, attackers send a large volume of transactions to deplete network resources, causing transactions from regular users to not be processed in a timely manner. This not only affects user experience but can also lead to an increase in transaction gas fees. In situations of scarce resources, users may be forced to pay higher fees to have their transactions prioritized.</p>\n<p>Additionally, Ethereum users face investment risks, with financial security under threat. For example, the term “shitcoin” is used to describe cryptocurrencies considered to have little to no value or lacking long-term growth potential. Shitcoins are often used as tools for scams or price manipulation, posing high investment risks and leading to significant financial losses. Due to their low price and market cap, shitcoins are easily manipulated, with common schemes including pump-and-dump and “honey pot” scams, where investors are lured by fraudulent projects and then robbed of their funds.</p>\n<p>Another common risk is the “Rug Pull,” where project creators suddenly remove all liquidity, causing the token value to plummet. These scams are usually promoted through false partnerships and endorsements, and once the token’s value rises, scammers sell off their tokens, take the profits, and disappear, leaving investors with worthless tokens. Investing in shitcoins also diverts attention and resources away from legitimate cryptocurrencies with real-world applications and growth potential.</p>\n<p>Besides shitcoins, “air coins” and “ponzi coins” also represent high-risk investments for quick profits. For users lacking sufficient professional knowledge and experience, distinguishing them from legitimate cryptocurrencies is particularly challenging.</p>\n<h2><a name=\"efficiency-6\" class=\"anchor\" href=\"https://ethresear.ch#efficiency-6\"></a>Efficiency</h2>\n<p>Two key indicators of Ethereum’s efficiency are transaction speed and gas fees. Transaction speed indicates the number of transactions the Ethereum network can process in a given unit of time, serving as a direct measure of the network’s processing capacity, where faster speeds denote higher efficiency. Additionally, every transaction requires the payment of a certain amount of gas fees as compensation to miners for transaction verification. Lower gas fees thus imply greater network efficiency.</p>\n<p>A decrease in transaction speed often leads to an increase in gas fees. Typically, as the efficiency of transaction processing drops and the competition for limited block space intensifies, the number of transactions vying for entry into the next block surges. To ensure their transactions are processed preferentially, users may find themselves compelled to increase the gas fees they offer. Miners tend to prioritize transactions with higher gas fees for confirmation. As a result, an increase in gas fees can deteriorate the user experience.</p>\n<p>Transactions are just one of the foundational activities within the Ethereum ecosystem. Users can engage in a variety of activities such as lending, staking, investing, and insurance through specific DApps. However, the vast array of DApps, combined with a lack of personalized recommendation services, often leaves users confused about choosing the applications and products best suited to their needs. This not only reduces user satisfaction but also impacts the overall efficiency of the Ethereum ecosystem.</p>\n<p>Take DeFi lending platforms as an example. To ensure platform security and stability, they often employ an over-collateralization mechanism, requiring borrowers to pledge assets exceeding the loan amount as collateral. This restricts borrowers from freely using these assets during the loan period, thereby reducing the efficiency of capital utilization and market liquidity.</p>\n<h1><a name=\"application-of-machine-learning-in-ethereum-7\" class=\"anchor\" href=\"https://ethresear.ch#application-of-machine-learning-in-ethereum-7\"></a>Application of Machine Learning in Ethereum</h1>\n<p>In the Ethereum ecosystem, numerous machine learning models, including the RFM (Recency, Frequency, Monetary value) model, Generative Adversarial Networks (GANs), decision trees, the K-Nearest Neighbors (KNN) algorithm, and the Density-Based Spatial Clustering of Applications with Noise algorithm (DBSCAN), play a crucial role. The integration of these models not only provides a powerful boost to optimizing transaction processing efficiency but also significantly enhances the security level of smart contracts. Furthermore, by enabling refined user segmentation, they facilitate the provision of customized services and solidify the stable operation of the network.</p>\n<h2><a name=\"algorithm-introduction-8\" class=\"anchor\" href=\"https://ethresear.ch#algorithm-introduction-8\"></a>Algorithm Introduction</h2>\n<p>Machine learning algorithms comprise a series of instructions or rules designed to parse data, learn patterns within it, and make predictions or decisions based on these learnings. What sets these algorithms apart is their ability to automatically learn and improve from the provided data without relying on explicitly programmed instructions by humans. Within the Ethereum ecosystem, machine learning models like the RFM model, GANs, decision trees, KNN algorithm, and DBSCAN algorithm play a pivotal role. The application of these models enhances the efficiency of transaction processing, elevates the security level of smart contracts, enables precise user segmentation to lay a solid foundation for providing more personalized services, and contributes to the stability of the network’s operation.</p>\n<p><strong>1. Bayesian Classifier</strong></p>\n<p>The Bayesian classifier is a type of statistical classification method aimed at minimizing the probability of classification errors or minimizing the average risk within a specific cost framework. The design philosophy of this classifier is deeply rooted in Bayes’ theorem, enabling it to calculate the probability that an object belongs to a certain category based on known characteristics and make classification decisions accordingly. Specifically, the Bayesian classifier first considers the prior probability of the object and then uses Bayes’ theorem to consider observed data comprehensively, thereby updating its judgment on the object’s category. Among all possible classifications, the Bayesian classifier selects the category with the highest posterior probability for the object.</p>\n<p>As shown in Figure 2, in supervised learning scenarios, the Bayesian classifier makes classification decisions using data and a probability model based on Bayes’ theorem. It calculates the posterior probability of each class for the data points, based on the likelihood of the data points, categories as well as the prior probabilities of features, and then assigns the data points to the category with the highest posterior probability. In the scatter plot of Figure 2, the classifier aims to find a boundary line that best separates points of different colors, thereby minimizing classification errors.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/9/9c081e78cd99d22c71680954bbf516e59f806ae7.png\" data-download-href=\"https://ethresear.ch/uploads/default/9c081e78cd99d22c71680954bbf516e59f806ae7\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/9/9c081e78cd99d22c71680954bbf516e59f806ae7_2_236x194.png\" alt=\"\" data-base62-sha1=\"mgjSA05b1rwxLGQgE9tgwjao5dJ\" width=\"236\" height=\"194\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/9/9c081e78cd99d22c71680954bbf516e59f806ae7_2_236x194.png, https://ethresear.ch/uploads/default/optimized/2X/9/9c081e78cd99d22c71680954bbf516e59f806ae7_2_354x291.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/9/9c081e78cd99d22c71680954bbf516e59f806ae7_2_472x388.png 2x\" data-dominant-color=\"E7E3E2\"></a></div><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/4/4cc8830f060078828121aa009948eb5630f7e54e.png\" data-download-href=\"https://ethresear.ch/uploads/default/4cc8830f060078828121aa009948eb5630f7e54e\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/4/4cc8830f060078828121aa009948eb5630f7e54e_2_296x118.png\" alt=\"\" data-base62-sha1=\"aXfSINoK2WqeYevwYG0nktx9eyO\" width=\"296\" height=\"118\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/4/4cc8830f060078828121aa009948eb5630f7e54e_2_296x118.png, https://ethresear.ch/uploads/default/optimized/2X/4/4cc8830f060078828121aa009948eb5630f7e54e_2_444x177.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/4/4cc8830f060078828121aa009948eb5630f7e54e_2_592x236.png 2x\" data-dominant-color=\"F7F6F6\"></a></div><p></p>\n<p>Figure 2: Bayesian Classifier</p>\n<p><strong>2. Decision Tree Algorithm</strong></p>\n<p>The decision tree algorithm is a widely used machine learning method for classification and regression tasks. It operates on a hierarchical decision-making logic, branching out by selecting features with higher information gain rates to gradually build a decision tree model. In essence, the algorithm automatically learns decision rules from data to determine the values of variables. In its concrete implementation, a decision tree decomposes a complex decision-making process into several simpler sub-decision processes, each based on the decision criteria of its predecessor, thus forming a tree-like structure.</p>\n<p>As illustrated in Figure 3, each node in the decision tree represents a decision criterion for a certain attribute, while the branches indicate the outcomes of that decision. Each leaf node corresponds to the final predicted result or category. In terms of structure, the decision tree model is intuitive and easy to understand, offering strong interpretability.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/f/ff5b91d982ed5ff0b8f823ef66fedb6b1ead643f.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/ff5b91d982ed5ff0b8f823ef66fedb6b1ead643f\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/f/ff5b91d982ed5ff0b8f823ef66fedb6b1ead643f_2_481x245.jpeg\" alt=\"\" data-base62-sha1=\"AqZWpNGbXPf6tEZC8tDgLbzNYyb\" width=\"481\" height=\"245\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/f/ff5b91d982ed5ff0b8f823ef66fedb6b1ead643f_2_481x245.jpeg, https://ethresear.ch/uploads/default/optimized/2X/f/ff5b91d982ed5ff0b8f823ef66fedb6b1ead643f_2_721x367.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/f/ff5b91d982ed5ff0b8f823ef66fedb6b1ead643f_2_962x490.jpeg 2x\" data-dominant-color=\"F7F6EF\"></a></div><p></p>\n<p>Figure 3: <a href=\"https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147\" rel=\"noopener nofollow ugc\">Decision Tree</a></p>\n<p><strong>3. DBSCAN Algorithm</strong></p>\n<p>The DBSCAN algorithm is a density-based spatial clustering method for datasets with noise, particularly suited for discontinuous datasets. It can identify clusters of various shapes without the need to predefine the number of clusters. Moreover, the DBSCAN algorithm is capable of effectively identifying outliers, specifically those located in low-density regions, enhancing its applicability in complex data environments, as shown in Figure 4.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/a/a0734bd262bb3c089dae2e065b62d48438f5efe6.png\" data-download-href=\"https://ethresear.ch/uploads/default/a0734bd262bb3c089dae2e065b62d48438f5efe6\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/a/a0734bd262bb3c089dae2e065b62d48438f5efe6_2_280x273.png\" alt=\"\" data-base62-sha1=\"mTppNRaUCKP5UIXNyuYCC16RsdU\" width=\"280\" height=\"273\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/a/a0734bd262bb3c089dae2e065b62d48438f5efe6_2_280x273.png, https://ethresear.ch/uploads/default/optimized/2X/a/a0734bd262bb3c089dae2e065b62d48438f5efe6_2_420x409.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/a/a0734bd262bb3c089dae2e065b62d48438f5efe6_2_560x546.png 2x\" data-dominant-color=\"F1E2EB\"></a></div><p></p>\n<p>Figure 4: <a href=\"https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/\" rel=\"noopener nofollow ugc\">The DBSCAN Algorithm Identifies Noise</a></p>\n<p><strong>4. KNN Algorithm</strong></p>\n<p>The KNN algorithm is suitable for both classification and regression tasks. In classification, it uses a voting mechanism to determine the category of the target instance; for regression, it predicts by calculating the average or weighted average of the closest k neighbors.</p>\n<p>As illustrated in Figure 5, the core logic of the KNN algorithm in classification tasks is to identify the nearest K neighbors to a new data point and predict the category of the new data point based on the categories of these neighbors. When K=1, the new data point is simply assigned to the same category as its closest neighbor. If K&gt;1, the category of the new data point is usually determined by a majority vote, meaning it is assigned to the category most common among its neighbors. When applied to regression tasks, the KNN algorithm follows the same principle of proximity, but the prediction is made based on the average output values of the K closest samples.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/9/90d50bd706586c91db9a88751d5b40f012d1b160.png\" data-download-href=\"https://ethresear.ch/uploads/default/90d50bd706586c91db9a88751d5b40f012d1b160\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/9/90d50bd706586c91db9a88751d5b40f012d1b160_2_305x231.png\" alt=\"\" data-base62-sha1=\"kFfcITYxIeE6kvBU0LFbn4BAnxm\" width=\"305\" height=\"231\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/9/90d50bd706586c91db9a88751d5b40f012d1b160_2_305x231.png, https://ethresear.ch/uploads/default/optimized/2X/9/90d50bd706586c91db9a88751d5b40f012d1b160_2_457x346.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/9/90d50bd706586c91db9a88751d5b40f012d1b160_2_610x462.png 2x\" data-dominant-color=\"F5F5F4\"></a></div><p></p>\n<p>Figure 5: KNN Algorithm for Classification</p>\n<p><strong>5. Generative Artificial Intelligence</strong></p>\n<p>Generative Artificial Intelligence is an advanced AI technology capable of producing new content, such as text, images, and music, based on input requirements. The development of this technology has been propelled by continuous advancements in machine learning and deep learning, particularly in applications within natural language processing and image recognition. By analyzing large datasets, generative AI models learn patterns and associations, allowing them to generate entirely new outputs based on this acquired knowledge. The effectiveness of model training is crucial in generative AI, necessitating high-quality data for learning and training purposes. Throughout this process, models deeply analyze and understand the structure, patterns, and relationships within the dataset, progressively enhancing their capability to create new content.</p>\n<p><strong>6. Transformer</strong></p>\n<p>As a cornerstone in the field of generative artificial intelligence, the Transformer model has revolutionized information processing by introducing the attention mechanism, which enables both focused precision and a holistic overview, particularly demonstrating exceptional performance in text generation. With advanced natural language processing models, such as GPT, Transformers can accurately interpret user requirements expressed in natural language and convert them into executable code, significantly reducing development complexity and enhancing efficiency.</p>\n<p>As shown in Figure 6, the performance of Transformer in the generation task of natural language processing has been significantly improved by integrating multi-head attention mechanism and self-attention mechanism, combining residual connection and fully connected neural network, and utilizing word embedding technology.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/1/19cd90c6286fa65d63c7c9acf76ccf64c127f24a.png\" data-download-href=\"https://ethresear.ch/uploads/default/19cd90c6286fa65d63c7c9acf76ccf64c127f24a\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/1/19cd90c6286fa65d63c7c9acf76ccf64c127f24a_2_231x341.png\" alt=\"\" data-base62-sha1=\"3GgmftrzUMDCiXiHsHkMdefEhMS\" width=\"231\" height=\"341\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/1/19cd90c6286fa65d63c7c9acf76ccf64c127f24a_2_231x341.png, https://ethresear.ch/uploads/default/optimized/2X/1/19cd90c6286fa65d63c7c9acf76ccf64c127f24a_2_346x511.png 1.5x, https://ethresear.ch/uploads/default/original/2X/1/19cd90c6286fa65d63c7c9acf76ccf64c127f24a.png 2x\" data-dominant-color=\"E0E0DD\"></a></div><p></p>\n<p>Figure 6: Transformer Model</p>\n<p><strong>7. RFM Model</strong></p>\n<p>The RFM model is an analytical tool that utilizes data on customer transaction behavior to identify groups of users with varying levels of value. This model segments customers based on three key metrics: the Recency of the customer’s last purchase, the Frequency of purchases, and the Monetary value of those purchases.</p>\n<p>As shown in Figure 7, these three metrics form the foundation of the RFM model. By quantitatively scoring customer performance across these dimensions, the model ranks customers, thereby identifying those who are of the highest value. Furthermore, the RFM model facilitates precise customer segmentation, providing businesses with a solid basis for developing personalized marketing strategies and optimizing customer service.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/2/2af1c8a69e245482e088550c7f1289a0f0a3c683.png\" data-download-href=\"https://ethresear.ch/uploads/default/2af1c8a69e245482e088550c7f1289a0f0a3c683\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/2/2af1c8a69e245482e088550c7f1289a0f0a3c683_2_255x255.png\" alt=\"\" data-base62-sha1=\"67U4xauAaLjQExAWPQ4XdbpkGav\" width=\"255\" height=\"255\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/2/2af1c8a69e245482e088550c7f1289a0f0a3c683_2_255x255.png, https://ethresear.ch/uploads/default/optimized/2X/2/2af1c8a69e245482e088550c7f1289a0f0a3c683_2_382x382.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/2/2af1c8a69e245482e088550c7f1289a0f0a3c683_2_510x510.png 2x\" data-dominant-color=\"D4DEEF\"></a></div><p></p>\n<p>Figure 7: RFM Model</p>\n<h2><a name=\"possible-applications-9\" class=\"anchor\" href=\"https://ethresear.ch#possible-applications-9\"></a>Possible Applications</h2>\n<p>In exploring machine learning techniques to address Ethereum’s security challenges, we focus on research in four key areas:</p>\n<ol>\n<li>Identifying and Filtering Malicious Transactions with Bayesian Classifiers</li>\n</ol>\n<p>By deploying Bayesian classifiers, we can effectively identify and filter potential malicious transactions, especially those frequent, small-scale transactions that could lead to DOS attacks. This approach relies on a detailed analysis of transaction characteristics, such as gas prices and transaction frequency, ensuring the health and stability of the Ethereum network.</p>\n<ol start=\"2\">\n<li>Generating Smart Contract Codes that Meet Security Standards and Specific Requirements</li>\n</ol>\n<p>GANs and Transformer-based generative models can both produce smart contract codes that meet specific requirements and have high security. These two methods have different focuses in the data types they rely on for training: the former primarily learns from unsafe code samples, while the latter tends to focus on safe samples.</p>\n<p>Training GANs to learn existing safe contract patterns and self-generate potential unsafe code samples allows the model to further learn to identify these potential risks, achieving the goal of automatically generating high-quality smart contract codes with higher security levels. Using Transformer-based generative models, by analyzing and learning from a large number of safe contract examples, it is possible to generate contract codes that are safe and meet specific needs, such as optimized gas consumption, significantly improving the efficiency and safety of smart contract development.</p>\n<ol start=\"3\">\n<li>Risk Analysis of Smart Contracts with Decision Trees</li>\n</ol>\n<p>Utilizing decision tree models to analyze key features of smart contracts, such as the frequency of function calls, transaction values, and the complexity of the source code, can effectively identify potential risks in contracts. This method enables us to predict potential vulnerabilities and risk points in contracts, providing developers and users with accurate safety assessments and greatly enhancing the overall security of smart contracts in the Ethereum ecosystem.</p>\n<ol start=\"4\">\n<li>Building Cryptocurrency Evaluation Models to Reduce Investment Risks</li>\n</ol>\n<p>By integrating analysis of cryptocurrency transaction records, social media activities, and market performance using machine learning algorithms, we can construct an evaluation model that predicts the risk of cryptocurrencies. This model can help investors avoid potential investment risks, thereby promoting the healthy development of the cryptocurrency market.</p>\n<p>Furthermore, the application of machine learning also has the potential to further enhance the efficiency of Ethereum, which can be explored from three dimensions:</p>\n<ol>\n<li>Application of Decision Trees in Optimizing the Transaction Pool Queuing Mechanism</li>\n</ol>\n<p>Through decision tree models, we can effectively optimize the queuing strategy of the Ethereum transaction pool. By analyzing transaction characteristics such as gas prices and transaction sizes, decision trees help optimize the selection and ordering of transactions, significantly increasing transaction processing speed, reducing network congestion, and lowering wait times for users.</p>\n<ol start=\"2\">\n<li>Segmenting Users and Providing Customized Services</li>\n</ol>\n<p>As a widely used analysis tool in customer relationship management, the RFM model effectively segments users by evaluating their most recent transaction time, transaction frequency, and the monetary value of transaction. Applying the RFM model to the Ethereum platform can help identify high-value user groups, optimize resource allocation, and provide more customized services, thereby enhancing user satisfaction and platform efficiency.</p>\n<p>The DBSCAN algorithm assists in identifying different user groups on Ethereum by analyzing user transaction behavior, offering them more customized financial services. This strategy of user segmentation helps optimize marketing effectiveness, improve customer satisfaction, and service efficiency.</p>\n<ol start=\"3\">\n<li>Credit Scoring with KNN</li>\n</ol>\n<p>The KNN algorithm performs credit scores on users by analyzing their transaction history and behavior patterns, which plays a key role in activities such as financial lending. Credit scores provide financial institutions and lending platforms with an effective tool to assess borrowers’ repayment abilities and credit risks, making loan decisions more accurate. This not only helps avoid excessive borrowing but also enhances market liquidity.</p>\n<h1><a name=\"future-directions-10\" class=\"anchor\" href=\"https://ethresear.ch#future-directions-10\"></a>Future Directions</h1>\n<p>From the perspective of macro financial allocation, considering Ethereum as the world’s largest distributed computing platform, investment in its infrastructure is undoubtedly crucial. Thus, attracting developers with diverse backgrounds to jointly participate in construction has become an urgent need. This article, through a detailed analysis of Ethereum’s technical construction and the challenges it faces, discusses a series of machine learning application scenarios and looks forward with anticipation to AI developers transforming these ideas into tangible value.</p>\n<p>As the computational power on the blockchain continues to increase, we anticipate the development of more complex models to aid in network management, transaction monitoring, and security audits, thereby enhancing the efficiency and security of the Ethereum network.</p>\n<p>Moreover, governance mechanisms driven by artificial intelligence/smart agents are expected to become an innovative highlight within the Ethereum ecosystem. Such governance mechanisms will lead to a more efficient, transparent, and automated decision-making process, providing the Ethereum platform with a more flexible and reliable governance structure. These potential developments not only may propel continuous innovation in Ethereum technology but also offer users an improved on-chain experience.</p>\n<p>If you are an expert in artificial intelligence or Ethereum technology and are interested in further discussion or collaborative research on this topic, please contact us.</p>\n<p><a href=\"https://ethresear.ch/t/how-ai-revolutionizes-ethereum-another-view-on-ai-blockchain/19010/1\">Click to view the poll.</a></p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/how-ai-revolutionizes-ethereum-another-view-on-ai-blockchain/19010\">Read full topic</a></p>","link":"https://ethresear.ch/t/how-ai-revolutionizes-ethereum-another-view-on-ai-blockchain/19010","pubDate":"Fri, 15 Mar 2024 05:58:28 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19010"},"source":{"@url":"https://ethresear.ch/t/how-ai-revolutionizes-ethereum-another-view-on-ai-blockchain/19010.rss","#text":"How AI Revolutionizes Ethereum, Another View on “AI+Blockchain”"},"filter":false},{"title":"ZK Fraud Proof with ZK State Channel","dc:creator":"0x1cc","category":"Layer 2","description":"<blockquote>\n<p>Thanks <a class=\"mention\" href=\"https://ethresear.ch/u/qizhou\">@qizhou</a> for discussing the idea</p>\n</blockquote>\n<h2><a name=\"tldr-1\" class=\"anchor\" href=\"https://ethresear.ch#tldr-1\"></a>TL;DR</h2>\n<ul>\n<li>\n<p>We can use the zk state channel to provide a zk fraud proof to resolve the disputes swiftly in the optimistic systems, such as opRollup (optimistic rollup) and opML (optimistic machine learning)</p>\n</li>\n<li>\n<p>We can construct a zk state channel using zkVM</p>\n</li>\n<li>\n<p>When the challenger can open a ZK state channel with the defender (sequencer/submitter) for dispute resolution, followed by uploading ZK proof onto the chain</p>\n</li>\n</ul>\n<h2><a name=\"background-2\" class=\"anchor\" href=\"https://ethresear.ch#background-2\"></a>Background</h2>\n<p>Current interactive fraud-proof challenge protocols such as Optimism fruad proof system uses</p>\n<ul>\n<li>\n<p>binary search method to pinpoint the specific step or instruction where a discrepancy arises between the defender (submitter/sequencer) and the challenger.</p>\n</li>\n<li>\n<p>when the specific step of disagreement is found, a one-step on-chain executor is used to adjudicate whether the defender or challenger is correct.</p>\n</li>\n</ul>\n<p>Considering about 40+B steps (instructions) per block transition, the protocol will <strong>take about 36 interactions,</strong> which is costly in both time and gas.</p>\n<p>To reduce the number of interactions, umerous researchers have explored the use of Zero-Knowledge (zk) fraud-proof approaches (<a href=\"https://github.com/ethereum-optimism/ecosystem-contributions/issues/61\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">RFP: OP Stack Zero Knowledge Proof · Issue #61 · ethereum-optimism/ecosystem-contributions · GitHub</a>). Many have experimented with utilizing zkVM as the fraud-proof virtual machine to generate zk proofs for either full-step or multi-step executions. With zk proofs for multi-step executions, the number of interactions can be significantly reduced (<a href=\"https://ethresear.ch/t/almost-instant-interactive-fraud-proof-using-multi-sect-on-da-blobs-and-multi-step-zk-verifier/16169\" class=\"inline-onebox\">Almost Instant Interactive Fraud Proof using Multi-Sect on DA BLOBs and multi-step ZK verifier</a>). If a zk proof for the full-step execution can be provided, the fraud-proof system operates akin to a zk proof system, eliminating the need for any interaction to resolve disputes. However, the cost of generating zk proofs for multi-step executions in zkVM remains substantial. Besides, in instances where a zk proof for the full-step execution cannot be provided, on-chain interaction for locating the dispute step remains necessary.</p>\n<h2><a name=\"proposal-3\" class=\"anchor\" href=\"https://ethresear.ch#proposal-3\"></a>Proposal</h2>\n<ul>\n<li>\n<p>We can first construct a Zero-Knowledge (zk) state channel utilizing zkVM, where the zkVM will host the dispute game program.</p>\n</li>\n<li>\n<p>Upon a validator (challenger) initiating a challenge, they can establish a zk state channel with the defender (sequencer/submitter).</p>\n</li>\n<li>\n<p>Within this channel, the challenger and defender engage with the dispute game program hosted in zkVM, simulating the on-chain interaction typical of smart contracts. They employ binary search techniques to pinpoint the disputed step and subsequently utilize one-step on-chain execution to determine the correct party.</p>\n</li>\n<li>\n<p>Once the dispute resolution concludes, they can generate a zk proof encapsulating the entire process within zkVM, subsequently submitting it on-chain for arbitration.</p>\n</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/5/5072fa67ff17eb01526b11051b76dbcafba92952.png\" data-download-href=\"https://ethresear.ch/uploads/default/5072fa67ff17eb01526b11051b76dbcafba92952\" title=\"zkChannel\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/5/5072fa67ff17eb01526b11051b76dbcafba92952_2_671x500.png\" alt=\"zkChannel\" data-base62-sha1=\"btGxiWbqxAG2KonpuMOREzY2OvU\" width=\"671\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/5/5072fa67ff17eb01526b11051b76dbcafba92952_2_671x500.png, https://ethresear.ch/uploads/default/original/2X/5/5072fa67ff17eb01526b11051b76dbcafba92952.png 1.5x, https://ethresear.ch/uploads/default/original/2X/5/5072fa67ff17eb01526b11051b76dbcafba92952.png 2x\" data-dominant-color=\"EEEEEC\"></a></div><p></p>\n<h2><a name=\"advantages-4\" class=\"anchor\" href=\"https://ethresear.ch#advantages-4\"></a>Advantages</h2>\n<ul>\n<li>\n<p>The proposed zk fraud-proof mechanism integrated with zk state channels dramatically reduces on-chain interactions to just two: one for channel initiation and another for closure, including zk proof submission. All other interactions occur off-chain and are cryptographically guaranteed by zk proofs.</p>\n</li>\n<li>\n<p>As interactions between the defender and the challenger occur off-chain, the frequency of engagement can be heightened, allowing for a more dynamic interaction rate. The number of checkpoints can also be greatly increased (2048+ can be easily achieved), and the number of interactions can no longer be limited.</p>\n</li>\n<li>\n<p>This approach is more cost-effective compared to generating zk proofs for full-step or multi-step executions within zkVM.</p>\n</li>\n</ul>\n<h2><a name=\"security-concern-5\" class=\"anchor\" href=\"https://ethresear.ch#security-concern-5\"></a>Security Concern</h2>\n<p>One potential concern with this proposal is the possibility of non-cooperation between challengers and defenders. To address this issue:</p>\n<ul>\n<li>\n<p>We still retain the original on-chain interactive dispute game, allowing both defenders and challengers to close the zk state channel at any time. In scenarios where the channel is closed, yet the dispute remains unresolved, parties can resort to the on-chain interactive dispute game contract for resolution.</p>\n</li>\n<li>\n<p>We can design an incentive mechanism to encourage challengers and defenders to participate in the zk state channel. For instance, participants could face reduced penalties for losing the dispute game and receive larger bonuses for winning.</p>\n</li>\n</ul>\n<h2><a name=\"conclusion-6\" class=\"anchor\" href=\"https://ethresear.ch#conclusion-6\"></a>Conclusion</h2>\n<p>This proposal works like an auxiliary plugin for expediting dispute resolution. It does not compromise the security of the existing fraud-proof system but instead offers a cost-effective and swift solution for dispute resolution.</p>\n            <p><small>2 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/zk-fraud-proof-with-zk-state-channel/19004\">Read full topic</a></p>","link":"https://ethresear.ch/t/zk-fraud-proof-with-zk-state-channel/19004","pubDate":"Thu, 14 Mar 2024 15:02:32 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19004"},"source":{"@url":"https://ethresear.ch/t/zk-fraud-proof-with-zk-state-channel/19004.rss","#text":"ZK Fraud Proof with ZK State Channel"},"filter":false},{"title":"Analyzing EIP-7623: Increase Calldata Cost","dc:creator":"Nerolation","category":"Execution Layer Research","description":"<h1><a name=\"eip-7623-increase-calldata-cost-1\" class=\"anchor\" href=\"https://ethresear.ch#eip-7623-increase-calldata-cost-1\"></a>EIP-7623: Increase Calldata Cost</h1>\n<p><a href=\"https://eips.ethereum.org/EIPS/eip-7623\" rel=\"noopener nofollow ugc\">EIP-7623</a> aims to recalibrate the cost of <strong>nonzero</strong> calldata bytes.</p>\n<p>The proposal’s goal is to <strong>reduce the maximum possible block size</strong> without affecting regular users who are not using Ethereum exclusively for DA. This comes with <strong>reducing the variance in block size</strong> and <strong>makes room for scaling</strong> the block gas limit or the blob count.</p>\n<p>With implementing this EIP:</p>\n<ul>\n<li>DA transactions pay 68 gas per nonzero calldata byte.</li>\n<li>All others pay 16 gas per nonzero calldata byte.</li>\n</ul>\n<p>This is achived with a conditional formula for determining the gas used per transaction.</p>\n<h2><a name=\"why-eip-7623-2\" class=\"anchor\" href=\"https://ethresear.ch#why-eip-7623-2\"></a>Why EIP-7623?</h2>\n<p>Today, we see a huge discrepancy between the the average/median block size and the maximum possible block size.</p>\n<p><em><strong>This comes with the following downsides:</strong></em></p>\n<ul>\n<li>Inefficiencies as a result of not fully leveraging available resources while having them ready.</li>\n<li>Big variance in block size.</li>\n<li>Max-size blocks have no use case except DoS.</li>\n</ul>\n<p>With <a href=\"https://eips.ethereum.org/EIPS/eip-4844\" rel=\"noopener nofollow ugc\">EIP-4844</a>, DA users have the option to move to using blobs → “<em>they can just switch</em>”.</p>\n<p><strong>With increasing calldata cost for DA transactions to 68 gas per byte, the maximum possible block size can be reduced from ~2.8 MB to ~0.5 MB.</strong></p>\n\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/6/64d6985bb28d41f096444105322b52616dda16cb.png\" data-download-href=\"https://ethresear.ch/uploads/default/64d6985bb28d41f096444105322b52616dda16cb\" title=\"drawing\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/6/64d6985bb28d41f096444105322b52616dda16cb_2_500x214.png\" data-base62-sha1=\"eo3vVMsj0OSg1CERKCr8c7civoL\" alt=\"drawing\" width=\"500\" height=\"214\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/6/64d6985bb28d41f096444105322b52616dda16cb_2_500x214.png, https://ethresear.ch/uploads/default/original/2X/6/64d6985bb28d41f096444105322b52616dda16cb.png 1.5x, https://ethresear.ch/uploads/default/original/2X/6/64d6985bb28d41f096444105322b52616dda16cb.png 2x\" data-dominant-color=\"F4F5F6\"></a></div>\n\n<br>\n<p><strong>Reducing the Beacon block size makes room to increase the blob count and/or the block gas limit.</strong></p>\n<h2><a name=\"eip-7623-3\" class=\"anchor\" href=\"https://ethresear.ch#eip-7623-3\"></a>EIP-7623</h2>\n<p>We have two constants:</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>STANDARD_TOKEN_COST</code></td>\n<td>4</td>\n</tr>\n<tr>\n<td><code>TOTAL_COST_FLOOR_PER_TOKEN</code></td>\n<td>17</td>\n</tr>\n</tbody>\n</table>\n</div><p>Currently we determine the <strong>gas used for calldata per transaction</strong> as <strong><code>nonzero_bytes_in_calldata * 16 + zero_bytes_in_calldata * 4</code></strong>.</p>\n<p><em>We now one-dimensionalize the current gas formula:</em><br>\nLet <strong><code>tokens_in_calldata = zero_bytes_in_calldata + nonzero_bytes_in_calldata * 4</code></strong>.</p>\n<p><em>This effectively combines zero and nonzero byte calldata into <code>tokens_in_calldata</code>.</em></p>\n<p><strong>The current formula for determining the gas used per transaction is then is equivalent to:</strong></p>\n<pre><code class=\"lang-python\">tx.gasused = (\n    21000 \\ \n        + isContractCreation * (32000 + InitCodeWordGas * words(calldata)) \\\n        + STANDARD_TOKEN_COST * tokens_in_calldata \\\n        + evm_gas_used\n)\n</code></pre>\n<p><strong>The EIP changes it to:</strong></p>\n<pre><code class=\"lang-python\">tx.gasUsed = {\n    21000 \\ \n    + \n    max (\n        STANDARD_TOKEN_COST * tokens_in_calldata \\\n           + evm_gas_used \\\n           + isContractCreation * (32000 + InitCodeWordGas * words(calldata)),\n        TOTAL_COST_FLOOR_PER_TOKEN * tokens_in_calldata\n    )\n</code></pre>\n<p>This formula ensures that transactions that spend at least 52 (68-16) gas per calldata byte on EVM operations (=any Opcode) will continue having a calldata cost per byte of 16 gas.<br>\nDA transactions will pay 68 gas per calldata byte.</p>\n<blockquote>\n<p>E.g. widely used methods such as <code>transfer</code> or <code>approve</code> consume ~45k and require 64 bytes calldata. Thus, they spend more than the threshold of 52 (68-16) bytes per calldata byte on EVM operations (<code>45_000 - 21_000 - 64*16 &gt; 52*64</code>).</p>\n</blockquote>\n<p><strong>This formula ensures that regular users remain unaffected.</strong></p>\n<h2><a name=\"when-4\" class=\"anchor\" href=\"https://ethresear.ch#when-4\"></a>When?</h2>\n<p>Based on the low complexity and the community’s preference to scale blobs and/or block gas limit, this EIP should be considered for the Pectra hardfork.</p>\n<h2><a name=\"who-would-pay-the-68-gas-5\" class=\"anchor\" href=\"https://ethresear.ch#who-would-pay-the-68-gas-5\"></a>Who would pay the 68 gas?</h2>\n<p>Transactions are unaffected if they spend at least 3.25 times more gas on EVM operations than on calldata. The same can be expressed as spending ~76% of the total gas minus the 21k base cost:</p>\n<p>We have the following following variables:</p>\n<ul>\n<li><span class=\"math\">G_{\\text{total}}</span> as the total gas used by a transaction.</li>\n<li><span class=\"math\">G_{\\text{base}}</span> as the base cost of a transaction, which is 21,000 gas.</li>\n<li><span class=\"math\">G_{\\text{calldata}}</span> as the gas used for calldata.</li>\n<li><span class=\"math\">G_{\\text{EVM}}</span> as the gas used for EVM operations with <span class=\"math\">G_{\\text{EVM}} = G_{\\text{total}} - G_{\\text{base}} - G_{\\text{calldata}}</span></li>\n</ul>\n<p>The conditions described can be translated into the following expressions:</p>\n<ol>\n<li>\n<p><strong>Transactions continue with 16 gas per calldata byte if:</strong><br>\n$$G_{\\text{EVM}} \\geq 3.25 \\times G_{\\text{calldata}}$$</p>\n</li>\n<li>\n<p><strong>The same can be expressed as transactions need to spend at least ~76% of the total gas minus the 21k base cost:</strong><br>\n$$\\frac{G_{\\text{EVM}}}{G_{\\text{total}} - G_{\\text{base}}} \\geq 0.76$$</p>\n</li>\n</ol>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/7/7a09a8e24f53bd3ee043def76079d89063db6dd8.png\" data-download-href=\"https://ethresear.ch/uploads/default/7a09a8e24f53bd3ee043def76079d89063db6dd8\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/7/7a09a8e24f53bd3ee043def76079d89063db6dd8_2_690x345.png\" alt=\"\" data-base62-sha1=\"hpAWJ5uy61265tGE5FNcUizAcfS\" role=\"presentation\" width=\"690\" height=\"345\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/7/7a09a8e24f53bd3ee043def76079d89063db6dd8_2_690x345.png, https://ethresear.ch/uploads/default/original/2X/7/7a09a8e24f53bd3ee043def76079d89063db6dd8.png 1.5x, https://ethresear.ch/uploads/default/original/2X/7/7a09a8e24f53bd3ee043def76079d89063db6dd8.png 2x\" data-dominant-color=\"F8F7F8\"></a></div><p></p>\n<p>As visible in the above chart, in the last 7 days (17-24 Feb), 4.19% of the transaction would have paid the 68 gas cost. Those 4.19% of transactions were executed by <strong>1.5% of the total addresses</strong> who are responsible of <strong>20% of the total calldata</strong>.<br>\nFor more information on the impact on individual Solidity methods, check <a href=\"https://nerolation.github.io/eip-7623-impact-analysis/\" rel=\"noopener nofollow ugc\">this</a>.</p>\n<p>A vast majority of transactions have a significantly higher <em>EVM/calldata gas</em> ratio.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/a/a148ed536a2be0186e717ee530110582b516f872.png\" data-download-href=\"https://ethresear.ch/uploads/default/a148ed536a2be0186e717ee530110582b516f872\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/a/a148ed536a2be0186e717ee530110582b516f872_2_690x345.png\" alt=\"\" data-base62-sha1=\"n0N7kuInAMhpNRNAbZyXMYYo0a6\" role=\"presentation\" width=\"690\" height=\"345\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/a/a148ed536a2be0186e717ee530110582b516f872_2_690x345.png, https://ethresear.ch/uploads/default/original/2X/a/a148ed536a2be0186e717ee530110582b516f872.png 1.5x, https://ethresear.ch/uploads/default/original/2X/a/a148ed536a2be0186e717ee530110582b516f872.png 2x\" data-dominant-color=\"FAFBFB\"></a></div><p></p>\n<p>As visible above, most transactions already spend &gt;90% of their gas on EVM operations, compared to the total gas minus the 21k base cost.</p>\n<p>The thin bar on the very left of the chart at the 0% tick is mainly caused by DA and transactions having messages/comments in their calldata.</p>\n<p>The following chart visualizes the calldata size in bytes of those transations that spend 0% on EVM resources (ignoring the 21k base cost now).<br>\nWe can see that the large number of transaction had a very low number of calldata bytes. The higher numbers are DA transactions.<br>\n</p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/6/6ee9cb107deaf0ce71d6974a357364e12c5f9d43.png\" data-download-href=\"https://ethresear.ch/uploads/default/6ee9cb107deaf0ce71d6974a357364e12c5f9d43\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/6/6ee9cb107deaf0ce71d6974a357364e12c5f9d43_2_625x500.png\" alt=\"\" data-base62-sha1=\"fPbq7UKpoLdW4xg9i0dmsoyDsRl\" role=\"presentation\" width=\"625\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/6/6ee9cb107deaf0ce71d6974a357364e12c5f9d43_2_625x500.png, https://ethresear.ch/uploads/default/optimized/2X/6/6ee9cb107deaf0ce71d6974a357364e12c5f9d43_2_937x750.png 1.5x, https://ethresear.ch/uploads/default/original/2X/6/6ee9cb107deaf0ce71d6974a357364e12c5f9d43.png 2x\" data-dominant-color=\"E0EBF2\"></a></div><p></p>\n<h2><a name=\"useful-links-6\" class=\"anchor\" href=\"https://ethresear.ch#useful-links-6\"></a>Useful Links</h2>\n<ul>\n<li><a href=\"https://nerolation.github.io/eip-7623-impact-analysis/\" rel=\"noopener nofollow ugc\">Impact on individual Solidity methods.</a></li>\n<li>EIP-7623 (<a href=\"https://eips.ethereum.org/EIPS/eip-7623\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">EIP-7623: Increase calldata cost</a>)</li>\n<li><a href=\"https://github.com/ethereum/go-ethereum/pull/29040\" rel=\"noopener nofollow ugc\">Draft implementation</a> by Marius Van Der Wijden</li>\n<li><a href=\"https://ethresear.ch/t/on-increasing-the-block-gas-limit/18567\">On Increasing the Block Gas Limit</a></li>\n<li><a href=\"https://ethresear.ch/t/on-block-sizes-gas-limits-and-scalability/18444\">On Block Sizes, Gas Limits and Scalability</a></li>\n</ul>\n            <p><small>3 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/analyzing-eip-7623-increase-calldata-cost/19002\">Read full topic</a></p>","link":"https://ethresear.ch/t/analyzing-eip-7623-increase-calldata-cost/19002","pubDate":"Thu, 14 Mar 2024 12:09:02 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19002"},"source":{"@url":"https://ethresear.ch/t/analyzing-eip-7623-increase-calldata-cost/19002.rss","#text":"Analyzing EIP-7623: Increase Calldata Cost"},"filter":false},{"title":"LossyDAS: Lossy, Incremental, and Diagonal Sampling for Data Availability","dc:creator":"cskiraly","category":"Sharding","description":"<p><em>By <a class=\"mention\" href=\"https://ethresear.ch/u/cskiraly\">@cskiraly</a>, <a class=\"mention\" href=\"https://ethresear.ch/u/leobago\">@leobago</a>, and <a class=\"mention\" href=\"https://ethresear.ch/u/dryajov\">@dryajov</a>  from the <a href=\"https://codex.storage/\" rel=\"noopener nofollow ugc\">Codex</a> team.</em></p>\n<p><em>The aim of this post is to share some of our DAS related findings with the community. These have been shared with some of you already last year, and for reference we link the <a href=\"https://colab.research.google.com/drive/1Di1-hBae8tZr1tZqcu1JqYycOFy8FdAy\" rel=\"noopener nofollow ugc\">original documents</a>. Here below we hope to provide a more digestible summary.</em></p>\n<h2><a name=\"tldr-1\" class=\"anchor\" href=\"https://ethresear.ch#tldr-1\"></a>TL;DR</h2>\n<ul>\n<li>An important parameter of DAS is the sample size, i.e. the number of segments (or columns) a node downloads and verifies. It is sometimes better to allow a few missing segments and have a slightly larger sample size than to have a smaller sample size but try to retrieve all segments of the sample at all costs. We call this technique <strong>LossyDAS</strong>.</li>\n<li>It is possible to gradually increase the sample size, extending the sample if the test fails (or does not conclude in, e.g. a given time). We call this strategy <strong>IncrementalDAS</strong>.</li>\n<li>When selecting what to sample on a 2D Reed-Solomon encoded grid, the usual assumption is to use a uniform random choice. We show that choosing row and column distinct samples, which we call <strong>DiDAS</strong> (“distinct” or “diagonal” DAS), provides probabilistically better results. It is trivial to implement, while it doesn’t seem to have any disadvantages.</li>\n</ul>\n<h2><a name=\"intro-2\" class=\"anchor\" href=\"https://ethresear.ch#intro-2\"></a>Intro</h2>\n<p>Sharding and DAS have been in the works for some time now. While the <a href=\"https://arxiv.org/abs/1809.09044\" rel=\"noopener nofollow ugc\">initial paper</a> and the <a href=\"https://notes.ethereum.org/@dankrad/new_sharding\" rel=\"noopener nofollow ugc\">Danksharding proposal</a> focused on a 2D Reed-Solomon (RS) encoding, some <a href=\"https://ethresear.ch/t/from-4844-to-danksharding-a-path-to-scaling-ethereum-da/18046\">recent posts</a> are proposing a transitional one-dimensional encoding. In this post, we focus on some aspects of sampling and its probabilistic guarantees, and we propose three improvements to the system. The techniques described below have been developed for the case of the original 2D RS encoding, but to some extent apply also to the transitional proposals.</p>\n<p>First, we introduce lossy sampling for data availability (<strong>LossyDAS</strong>), which provides the same probabilistic guarantees as random uniform sampling, adding error tolerance in exchange for slightly larger sample sizes.</p>\n<p>Second, we discuss <strong>IncrementalDAS</strong>, which provides the basis for developing a dynamic sampling strategy with increasing sample sizes.</p>\n<p>Finally, we introduce diagonal sampling for data availability (<strong>DiDAS</strong>), a method that improves sampling performance over this specific 2D erasure code, providing better performance than uniform random sampling for worst-case “adversarial” erasure patterns.</p>\n<h2><a name=\"sampling-over-the-das-encoding-3\" class=\"anchor\" href=\"https://ethresear.ch#sampling-over-the-das-encoding-3\"></a>Sampling over the DAS encoding</h2>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/c/c7d211c3af73250fdd975a0b14f1eb73579856c0.png\" data-download-href=\"https://ethresear.ch/uploads/default/c7d211c3af73250fdd975a0b14f1eb73579856c0\" title=\"das-structure\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/c/c7d211c3af73250fdd975a0b14f1eb73579856c0_2_666x500.png\" alt=\"das-structure\" data-base62-sha1=\"svH5t6qSXtDjq6nw9TfImmGznBC\" width=\"666\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/c/c7d211c3af73250fdd975a0b14f1eb73579856c0_2_666x500.png, https://ethresear.ch/uploads/default/original/2X/c/c7d211c3af73250fdd975a0b14f1eb73579856c0.png 1.5x, https://ethresear.ch/uploads/default/original/2X/c/c7d211c3af73250fdd975a0b14f1eb73579856c0.png 2x\" data-dominant-color=\"E9E3E7\"></a></div><br>\n<em>Figure 1: Data Availability encoding structure</em><p></p>\n<p>Figure 1 shows the 2D data availability encoding as proposed in Danksharding, using a square structure, using the same RS code with K=256 and N=512 in both dimensions. This means that any row or column can be fully recovered if at least K=256 segments of it are available.</p>\n<p>In the baseline sampling process, a sampling node selects <strong>S segments</strong> uniformly random without replacement, and tries to download these from the network. If all S can be retrieved, it assumes the block is available. If at least one of these fails, it assumes the block is not available.</p>\n<h2><a name=\"sample-size-and-erasure-patterns-4\" class=\"anchor\" href=\"https://ethresear.ch#sample-size-and-erasure-patterns-4\"></a>Sample Size and Erasure Patterns</h2>\n<p>An important parameter of DAS is the <strong>sample size S</strong>, i.e. the number of segments a node downloads and verifies. When evaluating how many samples are needed, it is important to note that, contrary to a simple RS code, a multi-dimensional code is non-MDS (maximum distance separable). With an MDS code, repairability only depends on the number of segments lost, but with a non-MDS code, it depends on the actual erasure pattern as well. This means that, with some probability, non-trivial erasure patterns will happen. Even more importantly, this potentially allows malicious actors to inject crafted (adversarial) erasure patterns in the system. Therefore, when analyzing the sampling as a test, we should differentiate between the cases of</p>\n<ul>\n<li>random erasures,</li>\n<li>adversarial (e.g. worst-case) erasure patterns.</li>\n</ul>\n<h3><a name=\"random-erasures-5\" class=\"anchor\" href=\"https://ethresear.ch#random-erasures-5\"></a>Random Erasures</h3>\n<p>Random erasures are a plausible model when thinking of network and storage induced errors. The simplest model of random erasures is the case of uniform random erasures, which maps relatively well to the service provided by a DHT, as in the original proposal, when assuming honest actors. However, given the nature of the system, our main optimization target is not this, but worst-case or adversarial erasures.</p>\n<h3><a name=\"worst-case-erasures-6\" class=\"anchor\" href=\"https://ethresear.ch#worst-case-erasures-6\"></a>Worst-case Erasures</h3>\n<p>When thinking of malicious actors, it is important to analyze worst-case scenarios. While the code is not an MDS code, we can describe what a <strong>worst-case (or minimal size) erasure pattern</strong> is. If we select any N-K+1 rows and N-K+1 columns, and remove all segments in the intersections of these selected rows and columns (i.e. 257 * 257 segments out of the 512 * 512 segments), we prevent any recovery from taking place. There are only full rows and unrecoverable rows, and similarly, there are only full and unrecoverable columns. No row or column can be recovered therefore, the data is lost.</p>\n<p>It is easy to see that the above is the “worst-case”. First, observe that worst-case here means the least number of missing segments. In our context, this also means the minimum number of nodes/validators a malicious actor should control in order to perpetrate the attack. In the worst-case, there should be no possibility of repairing rows or columns. Assume there is a worst-case pattern that has less missing segments than (N-K+1) * (N-K+1). In that case, there would be at least one column or row that has less than N-K+1 missing segments. That column/row is repairable, which contradicts our assumption. <img src=\"https://ethresear.ch/images/emoji/facebook_messenger/black_medium_square.png?v=12\" title=\":black_medium_square:\" class=\"emoji\" alt=\":black_medium_square:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Note:  The above proof assumes an interactive row/columns based repair process. Multi-dimensional RS codes can also be repaired using multivariate interpolation, as discussed <a href=\"https://a16zcrypto.com/content/article/an-overview-of-danksharding-and-a-proposal-for-improvement-of-das/\" rel=\"noopener nofollow ugc\">here</a>. The proof described in their paper also applies to our case, confirming these patterns as worst-case minimal size patterns.</p>\n<p>Trivially, any erasure pattern that contains a worst-case erasure pattern, is also unrecoverable.</p>\n<h3><a name=\"setting-the-sample-size-7\" class=\"anchor\" href=\"https://ethresear.ch#setting-the-sample-size-7\"></a>Setting the Sample Size</h3>\n<p>When setting the sample size, what we are interested in is to limit the probability of a not-available block passing the test, in other words the false positive (FP) rate, independent of the erasure pattern. Clearly, with uniform random sampling, assuming an erasure pattern that makes the block unrecoverable, the larger the pattern, the easier it is to detect. Therefore, it is enough to study the case of the smallest such patterns, i.e. the worst-case erasure pattern. This probability can be approximated by a binomial distribution or - even better - calculated exactly using a hypergeometric distribution with</p>\n<ul>\n<li>population size: <span class=\"math\">N*N</span></li>\n<li>number of failure states in the population: <span class=\"math\">(K+1)*(K+1)</span></li>\n<li>Number of segments: <span class=\"math\">S</span></li>\n<li>Number of failures allowed: <span class=\"math\">0</span></li>\n</ul>\n<div class=\"math\">\nPr(X=0) = Hypergeom(0; N^2, (N-K+1)^2, S)\n</div>\n<p>This probability obviously depends on S, the sample size. In fact, our real question is:<br>\n<strong>How many segments do we need to reach a given level of assurance?</strong></p>\n<p>As shown on Fig. 2, if we have a 2D RS coding, K=256, N=512, and want this probability to be below 1e-9, we need S=73 segments. The figure also shows the theoretical cases of using a 1D RS code with the same number of segments and the same expansion factor, and the case of a 2D RS code with a smaller expansion factor.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/0/073035d8041cd58df6cb3e9b6991c7882b478cd4.png\" data-download-href=\"https://ethresear.ch/uploads/default/073035d8041cd58df6cb3e9b6991c7882b478cd4\" title=\"das-uniform-sampling\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/0/073035d8041cd58df6cb3e9b6991c7882b478cd4_2_690x437.png\" alt=\"das-uniform-sampling\" data-base62-sha1=\"11ADd6VrFneSQy21GJdu1qVQVoM\" width=\"690\" height=\"437\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/0/073035d8041cd58df6cb3e9b6991c7882b478cd4_2_690x437.png, https://ethresear.ch/uploads/default/original/2X/0/073035d8041cd58df6cb3e9b6991c7882b478cd4.png 1.5x, https://ethresear.ch/uploads/default/original/2X/0/073035d8041cd58df6cb3e9b6991c7882b478cd4.png 2x\" data-dominant-color=\"F9F9F9\"></a></div><br>\n<em>Figure 2: Performance of uniform random sampling over the DAS data structure, with different RS encodings, as a function of the sample size S</em><p></p>\n<h2><a name=\"what-about-false-negatives-8\" class=\"anchor\" href=\"https://ethresear.ch#what-about-false-negatives-8\"></a>What about False Negatives?</h2>\n<p>As with all statistical tests, there are both cases of FP (false positives: the test passes even if the block is not available), and FN (false negative: the sampling fails even if the block is available). The ratio of false negative tests can be quantified using the survival function of the hypergeometric distribution as follows:</p>\n<div class=\"math\">\nP_{FP} = Pr(\\{X&gt;0\\}) = Hypergeom.sf(0; N^2, M, S)\n</div>\n<p>Where M is the size of the erasure pattern.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/a/af403f88175519567b78dfefc3200aedfb52253b.png\" data-download-href=\"https://ethresear.ch/uploads/default/af403f88175519567b78dfefc3200aedfb52253b\" title=\"das-false-negative\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/a/af403f88175519567b78dfefc3200aedfb52253b_2_690x450.png\" alt=\"das-false-negative\" data-base62-sha1=\"p0lcV5f3oqQdh88hVCutXglEhDR\" width=\"690\" height=\"450\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/a/af403f88175519567b78dfefc3200aedfb52253b_2_690x450.png, https://ethresear.ch/uploads/default/original/2X/a/af403f88175519567b78dfefc3200aedfb52253b.png 1.5x, https://ethresear.ch/uploads/default/original/2X/a/af403f88175519567b78dfefc3200aedfb52253b.png 2x\" data-dominant-color=\"FAFAFB\"></a></div><br>\n<em>Figure 3: False Negative Rate as a function of missing samples</em><p></p>\n<p>As an example, if there is only 1 segment missing out of the 512*512 segments, the probability of an FN test result is 0.03%. If instead 1% of the segments (2621 segments) are missing, the probability of an FN test is 51.9%.</p>\n<p>So, how can this system work when there can be a large number of FN tests due to the unavailability of a few segments? More specifically, there are at least two issues here leading to FN cases:</p>\n<ul>\n<li>in a distributed setting with a large number of nodes, there are always <strong>transient errors</strong>, such as nodes being down, unreachable, or suffering network errors. Requiring all segments to be received means we should use reliable transport mechanisms (e.g. TCP connections, or other means of retransmission in case of failures) and even then, we need nodes to have relatively high availability and good network connectivity. Or, we need protocols that store data redundantly and retrieve it using multiple parallel lookups, like a typical DHT, and we still potentially need large redundancy and many retrievals.</li>\n<li>As we have seen, requiring all segments to be received also means that we might have a large amount of <strong>false negatives</strong> in our test if not all segments are actually available. In other words, the test easily fails when the data has some losses in the distributed structure, but it is actually reconstructable as a whole. While we could aim to avoid some losses by replication, retry, and repair, this might be very costly if not impossible due to the previous point.</li>\n</ul>\n<p>To understand what can be done, let’s carefully analyze our basic premise until this point, focusing on a few key parts:<br>\n“When a node <strong>cannot retrieve</strong> <strong>all</strong> <strong>S</strong> of the selected segments, it cannot accept the block.”</p>\n<ul>\n<li><strong>“cannot retrieve”</strong>: what “cannot retrieve” actually means in this context is something that needs more attention. It means we tried, and re-tried, and it was not retrievable. How long and with what dynamics should a node try to retrieve a segment? Should it also try to retrieve it by reconstruction using the erasure code? We argue that these network-level reliability techniques can be costly, and there is a trade-off between making the network more reliable and selecting the correct sampling process.</li>\n<li><strong>“all”</strong>: do we really need to retrieve all segments? Wouldn’t it make sense to ask for more segments and allow some losses?</li>\n<li><strong>“S”</strong>: Can we dynamically change S, looking for more samples if our first attempt fails?</li>\n</ul>\n<p>To answer what “cannot retrieve” means, we should delve into the details of the networking protocols. We leave this for a subsequent post. Here in what follows, we discuss the two other possibilities. LossyDAS addresses the question of whether “all” samples are needed, while IncrementalDAS focuses on making “S” dynamic. DiDAS, instead, focuses on achieving the same probabilistic guarantees with a smaller “S”,</p>\n<h2><a name=\"lossydas-accept-partial-sampling-9\" class=\"anchor\" href=\"https://ethresear.ch#lossydas-accept-partial-sampling-9\"></a>LossyDAS: accept partial sampling</h2>\n<p>Do we really need all segments of the sample? What happens if we test more segments, and allow for some losses? It turns out that we can define the test with these assumptions.</p>\n<p>In LossyDAS, just as in the original test, we set the target FP probability (i.e., cases when the test passes, we think the data has been released, but it is actually in a non-repairable state). We look for the minimal value of S (the sample size) as a function of</p>\n<ul>\n<li><span class=\"math\">P_{FP}</span>: the target FP probability threshold, and</li>\n<li>M: the number of missing segments we allow out of the S segments queried</li>\n</ul>\n<p>In the calculation we use the percentage point function (PPF), which is the inverse of the CDF.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/8/845b07b9794b32dc8b4233238b787f3b2353b196.png\" data-download-href=\"https://ethresear.ch/uploads/default/845b07b9794b32dc8b4233238b787f3b2353b196\" title=\"das-lossydas\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/8/845b07b9794b32dc8b4233238b787f3b2353b196_2_690x448.png\" alt=\"das-lossydas\" data-base62-sha1=\"iSS3yrsLKspZJCEe7yKgouov530\" width=\"690\" height=\"448\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/8/845b07b9794b32dc8b4233238b787f3b2353b196_2_690x448.png, https://ethresear.ch/uploads/default/original/2X/8/845b07b9794b32dc8b4233238b787f3b2353b196.png 1.5x, https://ethresear.ch/uploads/default/original/2X/8/845b07b9794b32dc8b4233238b787f3b2353b196.png 2x\" data-dominant-color=\"FAFAF9\"></a></div><br>\nFigure 4:  Number of failures that can be allowed is LossyDAS, respecting a given FP threshold<p></p>\n<p>As we can see in Fig.4, we can sample slightly more and allow for losses while keeping the same FP threshold. For example, with a target FP of 1e-9, we can choose a sample size of 84 and allow 1 missing segment. Or we can go for 103 segments and allow up to 3 losses. In general, instead of sampling with n=73, approximately every 10 more segments allows us to tolerate one more missing segment.<br>\nIn practice, this means that nodes can try to download slightly more segments, already factoring in potential networking issues or other transient errors.</p>\n<h2><a name=\"incrementaldas-dynamically-increase-the-sample-size-10\" class=\"anchor\" href=\"https://ethresear.ch#incrementaldas-dynamically-increase-the-sample-size-10\"></a>IncrementalDAS: dynamically increase the sample size</h2>\n<p>What happens when too many segments are actually missing? Should we wait for these to be repaired, repair them on our own, or can we do something else? One might try to argue that the test should fail until all tested segments are repaired, otherwise the block is not available, but our whole system of probabilistic guarantees is built on the principle that repairability (and not repair) is to be assured. Moreover, waiting for it to be repaired might be too long, while repairing on our own might be too costly. The question that naturally arises is whether we can re-test somehow with a different sample.</p>\n<p>First of all, we should clarify that simply doing another test on another random sample with the same S is a bad idea. That would be the “toss a coin until you like the result” approach, which is clearly wrong. It breaks our assumptions on the FP threshold set previously.  If we make another test, that should be based on the conditional probabilities of what we already know. This is what IncrementalDAS aims for.</p>\n<p>In IncrementalDAS, our node selects a first target loss size L1, and the corresponding sample size S(L1) according to LossyDAS. It requests these segments, allowing for L1 losses. If it receives at least S(L1) - L1 segments, the test passes. Otherwise, it extends the sample, using some L2&gt;L1, to size S(L2). By extending the sample, we mean that it selects the new sample such that it includes the old sample, it simply picks S(L2)-S(L1) new random segments. It is easy to see that we can do this, since we could have started with the larger sample right at the beginning, and that would have been fine from the point of view of probabilistic guarantees. The fact that we do the sampling in two steps does not change this. Clearly, the process can go on, extending the sample more and more. L2 (and further extensions) can be decided based on the actual number of losses observed in the previous step. Different strategies are possible, also based on all the other recovery mechanisms used, which we will not detail here.</p>\n<h2><a name=\"didas-steering-away-from-uniform-random-sampling-11\" class=\"anchor\" href=\"https://ethresear.ch#didas-steering-away-from-uniform-random-sampling-11\"></a>DiDAS: steering away from uniform random sampling</h2>\n<p>Until this point, it was our assumption that uniform random sampling is used over the erasure coded block structure. However, the code is non-MDS, and thus some segment combinations seem to be more important than others.</p>\n<p>In the base version of the test, we select segments uniformly at random without replacement over the whole square. Some of these might fall on the same row or column, which reduces the efficiency of the test. If, instead of the base version, we make sure to test with segments that are <strong>on distinct rows and distinct columns</strong>, we increase our chances of catching these worst-case erasures. We call this sampling distinct (or diagonal) sampling for data availability (DiDAS).</p>\n<p>Note: What we define here is a 2D variant of sampling without replacement. The diagonal is clearly a row and column distinct selection of segments, but of course there are many other possible selections. From the theoretical point of view, the whole erasure code structure is invariant to row and column permutations in the case of worst-case erasure patterns. Thus, when studying parameters, we can assume a sample in the diagonal and permutations of the erasure pattern.</p>\n<h3><a name=\"how-much-we-improve-with-didas-12\" class=\"anchor\" href=\"https://ethresear.ch#how-much-we-improve-with-didas-12\"></a>How much we improve with DiDAS</h3>\n<p>With the new sampling method, the probability of catching a worst-case erasure from S segments can be expressed as a combination of two hypergeometric distributions. See our <a href=\"https://colab.research.google.com/drive/1Di1-hBae8tZr1tZqcu1JqYycOFy8FdAy\" rel=\"noopener nofollow ugc\">original working document (Jupyter Notebook)</a> for the exact formulation. Fig. 5 shows the difference between the two sampling methods in the case of a worst-case erasure pattern.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/f/fcb426ee3f886c2acf47b675df8dfd877cc7f696.png\" data-download-href=\"https://ethresear.ch/uploads/default/fcb426ee3f886c2acf47b675df8dfd877cc7f696\" title=\"das-didas\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/f/fcb426ee3f886c2acf47b675df8dfd877cc7f696_2_690x433.png\" alt=\"das-didas\" data-base62-sha1=\"A3wio5vPwvAEOLZ5vjk5Nw3V59I\" width=\"690\" height=\"433\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/f/fcb426ee3f886c2acf47b675df8dfd877cc7f696_2_690x433.png, https://ethresear.ch/uploads/default/original/2X/f/fcb426ee3f886c2acf47b675df8dfd877cc7f696.png 1.5x, https://ethresear.ch/uploads/default/original/2X/f/fcb426ee3f886c2acf47b675df8dfd877cc7f696.png 2x\" data-dominant-color=\"FBFBFB\"></a></div><br>\n<em>Figure 5: Comparison of uniform random sampling with DiDAS, with N=256, K=512, and a worst-case erasure pattern</em><p></p>\n<p>Intuitively, for small S the difference is negligible: even if we don’t enforce distinct rows and columns, the segments sampled will most probably be on distinct rows and columns. S=73 is still relatively small in this sense.</p>\n<p>For larger S, the difference becomes significant on a logarithmic scale, as in Fig. 5. However, we are speaking of very small probabilities, and we admit that it is questionable how much this is of practical use with the currently proposed set of parameters.</p>\n<p>Nevertheless, implementing DiDAS is trivial. If we use it in combination with IncrementalDAS, it could help reduce the size of the increments required.</p>\n<p>Also note that DiDAS can’t go beyond S=N, since there are no more distinct rows or columns. When S=512, DiDAS always finds out about an unrecoverable worst-case erasure pattern, with probability 1. Is that possible? In fact, it is, but only for the “worst-case” pattern defined previously, which isn’t the worst-case for DiDAS. There are erasure patterns that are worse for DiDAS than the minimal-size erasure pattern. However, our preliminary results show that these are necessarily large patterns that are easily detectable anyway, and DiDAS still performs better than uniform random sampling on these.</p>\n<h2><a name=\"acknowledgements-13\" class=\"anchor\" href=\"https://ethresear.ch#acknowledgements-13\"></a>Acknowledgements</h2>\n<p>This work is supported by grant FY22-0820 on Data Availability Sampling from the Ethereum Foundation. We would like to thank the EF for its support, and Dankrad Feist and Danny Ryan for the original proposal and the numerous discussions on the topics of the post.</p>\n<h2><a name=\"references-14\" class=\"anchor\" href=\"https://ethresear.ch#references-14\"></a>References</h2>\n<p>[1] Original working document (Jupyter Notebook).</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://colab.research.google.com/drive/1Di1-hBae8tZr1tZqcu1JqYycOFy8FdAy\">\n  <header class=\"source\">\n      <img src=\"https://ethresear.ch/uploads/default/original/2X/2/24ca65eabb5a36bed9abf92ff3a781e4f6f35f15.png\" class=\"site-icon\" data-dominant-color=\"F29404\" width=\"16\" height=\"16\">\n\n      <a href=\"https://colab.research.google.com/drive/1Di1-hBae8tZr1tZqcu1JqYycOFy8FdAy\" target=\"_blank\" rel=\"noopener nofollow ugc\">colab.research.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://ethresear.ch/uploads/default/original/2X/5/5b77e9737d5f8f8bc5f7b35e7fc0f8088fd1ebd8.png\" class=\"thumbnail onebox-avatar\" data-dominant-color=\"F29304\" width=\"260\" height=\"260\">\n\n<h3><a href=\"https://colab.research.google.com/drive/1Di1-hBae8tZr1tZqcu1JqYycOFy8FdAy\" target=\"_blank\" rel=\"noopener nofollow ugc\">Google Colaboratory</a></h3>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>[2] Al-Bassam, Mustafa, Alberto Sonnino, and Vitalik Buterin. “Fraud and data availability proofs: Maximising light client security and scaling blockchains with dishonest majorities.” arXiv preprint arXiv:1809.09044 (2018).</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://arxiv.org/abs/1809.09044\">\n  <header class=\"source\">\n      <img src=\"https://ethresear.ch/uploads/default/original/2X/c/c683569a48ce1952ba841c851ae3b1f282d4b00f.png\" class=\"site-icon\" data-dominant-color=\"B36362\" width=\"32\" height=\"32\">\n\n      <a href=\"https://arxiv.org/abs/1809.09044\" target=\"_blank\" rel=\"noopener nofollow ugc\">arXiv.org</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://ethresear.ch/uploads/default/optimized/2X/d/db11c4139de0d4f279af48f5a1ade7b5181d481b_2_500x500.png\" class=\"thumbnail onebox-avatar\" data-dominant-color=\"865F5C\" width=\"500\" height=\"500\">\n\n<h3><a href=\"https://arxiv.org/abs/1809.09044\" target=\"_blank\" rel=\"noopener nofollow ugc\">Fraud and Data Availability Proofs: Maximising Light Client Security and...</a></h3>\n\n  <p>Light clients, also known as Simple Payment Verification (SPV) clients, are nodes which only download a small portion of the data in a blockchain, and use indirect means to verify that a given chain is valid. Typically, instead of validating block...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>[3] Dankrad Feist. “New sharding design with tight beacon and shard block integration.” blog post, available at</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://notes.ethereum.org/@dankrad/new_sharding\">\n  <header class=\"source\">\n      <img src=\"https://ethresear.ch/uploads/default/original/2X/1/164f4256a0019844abd16f69f3fd13ed310661d9.png\" class=\"site-icon\" data-dominant-color=\"696969\" width=\"134\" height=\"134\">\n\n      <a href=\"https://notes.ethereum.org/@dankrad/new_sharding\" target=\"_blank\" rel=\"noopener nofollow ugc\">HackMD</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://ethresear.ch/uploads/default/original/2X/c/c472cf9b85c388335cd1c7761336f8ed99174fa0.jpeg\" class=\"thumbnail onebox-avatar\" data-dominant-color=\"B5AAAD\" width=\"400\" height=\"400\">\n\n<h3><a href=\"https://notes.ethereum.org/@dankrad/new_sharding\" target=\"_blank\" rel=\"noopener nofollow ugc\">New sharding design with tight beacon and shard block integration - HackMD</a></h3>\n\n  <p># New sharding design with tight beacon and shard block integration  Previous data shard constructio</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>[4] Valeria Nikolaenko, and Dan Boneh. “Data availability sampling and danksharding: An overview and a proposal for improvements.” blog post, available at</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://a16zcrypto.com/posts/article/an-overview-of-danksharding-and-a-proposal-for-improvement-of-das/\">\n  <header class=\"source\">\n      <img src=\"https://a16zcrypto.com/favicon.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://a16zcrypto.com/posts/article/an-overview-of-danksharding-and-a-proposal-for-improvement-of-das/\" target=\"_blank\" rel=\"noopener nofollow ugc\" title=\"12:30PM - 27 April 2023\">a16z crypto – 27 Apr 23</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/362;\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/7/7055387bebf7fb5e535c20f343c90c0fe8c4b885_2_690x362.jpeg\" class=\"thumbnail\" data-dominant-color=\"DFBBBD\" width=\"690\" height=\"362\"></div>\n\n<h3><a href=\"https://a16zcrypto.com/posts/article/an-overview-of-danksharding-and-a-proposal-for-improvement-of-das/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data availability sampling and danksharding: An overview and a proposal for...</a></h3>\n\n  <p>a16z crypto is a venture capital fund that has been investing in crypto and web3 startups — across all stages — since 2013.</p>\n\n  <p>\n    <span class=\"label1\">Est. reading time: 0</span>\n  </p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>[5] From 4844 to Danksharding: a path to scaling Ethereum DA</p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"18046\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://ethresear.ch/user_avatar/ethresear.ch/fradamt/48/6474_2.png\" class=\"avatar\">\n    <a href=\"https://ethresear.ch/t/from-4844-to-danksharding-a-path-to-scaling-ethereum-da/18046\">From 4844 to Danksharding: a path to scaling Ethereum DA</a> <a class=\"badge-wrapper  box\" href=\"https://ethresear.ch/c/networking/27\"><span class=\"badge-category-bg\" style=\"background-color: #FFBE25;\"></span><span style=\"color: #FFFFFF\" data-drop-close=\"true\" class=\"badge-category clear-badge\">Networking</span></a>\n  </div>\n  <blockquote>\n    This post stems from ongoing discussions between many researchers and client devs on the approach to scaling of the DA layer beyond EIP-4844, and is meant to summarize and make accessible some of the prevailing ideas. <a href=\"https://ethresear.ch/t/peerdas-a-simpler-das-approach-using-battle-tested-p2p-components/16541\">PeerDAS</a> is a recommended first read. \nThanks to Danny Ryan, Ansgar Dietrichs, Dankrad Feist, Jacob Kaufmann, Age Manning, Csaba Kiraly, Leonardo Bautista-Gomez for feedback and discussions. \nEIP-4844 is the first milestone in the quest of scaling the data availability layer of Ethe…\n  </blockquote>\n</aside>\n\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963\">Read full topic</a></p>","link":"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963","pubDate":"Tue, 12 Mar 2024 07:45:47 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-18963"},"source":{"@url":"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963.rss","#text":"LossyDAS: Lossy, Incremental, and Diagonal Sampling for Data Availability"},"filter":false},{"title":"Promoting Ethereum Research to facilitate interdisciplinary collaboration and academic user engagement","dc:creator":"Mirror","category":"Administrivia","description":"<p>It’s delightful to see our community thriving, with an increasing number of lengthy research papers and constructive technical insights being published on this forum. Ethereum Research has become the main battlefield for Ethereum innovators to delve into research. I take pride in the growth of the community and express gratitude to those who share their research insights on this forum.</p>\n<p>As a researcher in Ethereum technology, I would like to discuss the challenges I have encountered. When I publish articles on Ethereum Research, they are not easily searchable on Google Scholar, and I have to manually add them to my Google Scholar profile. This has led me to ponder: how can we ensure that the research and creativity on this forum are discovered, discussed, and cited by more enthusiasts more effectively and efficiently?</p>\n<p>A prime example is a twitter bot <a href=\"https://x.com/ethresearchbot?s=11\" rel=\"noopener nofollow ugc\">@ethreserchbot</a><br>\nIt effectively extends the research of this forum to the user community on Twitter, and uses AI to summarize complex studies, making them more accessible to other researchers.</p>\n<p>I want to emphasize that this is crucial! Since the establishment of the scientific research system, a large amount of research progress has been generated through interdisciplinary collaboration. Speaking of which, we need to mention blockchain again. Today’s blockchain discipline is a hybrid discipline composed of computer science, cryptography, statistics, finance, social sciences, and various game theories. Blockchain was born from the collision of finance and computer science, becoming a gem of this era against political authority and financial hegemony.</p>\n<p>How to facilitate interdisciplinary research? It’s quite simple: make it more visible. By providing academic grants, collaborating with universities, and extensively recruiting researchers, Ethereum has established a vast knowledge network. However, for traditional researchers, it may not be very user-friendly, primarily due to the challenge of knowledge discovery. So, how can this be improved?</p>\n<p>One straightforward approach is to periodically compile forum content into paper collections similar to preprints for publication. Assigning a DOI number to each article would enable quick retrieval in various academic search engines, facilitating easier citation and dissemination.</p>\n<p>I look forward to seeing more ideas for promoting research on this forum. If you believe there are any mistakes in what I have said, I welcome criticism and corrections, as well as any valuable suggestions for improvement.</p>\n            <p><small>16 posts - 11 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/promoting-ethereum-research-to-facilitate-interdisciplinary-collaboration-and-academic-user-engagement/18918\">Read full topic</a></p>","link":"https://ethresear.ch/t/promoting-ethereum-research-to-facilitate-interdisciplinary-collaboration-and-academic-user-engagement/18918","pubDate":"Sun, 10 Mar 2024 16:11:46 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-18918"},"source":{"@url":"https://ethresear.ch/t/promoting-ethereum-research-to-facilitate-interdisciplinary-collaboration-and-academic-user-engagement/18918.rss","#text":"Promoting Ethereum Research to facilitate interdisciplinary collaboration and academic user engagement"},"filter":false},{"title":"How to hard-fork to save most users' funds in a quantum emergency","dc:creator":"vbuterin","category":"Execution Layer Research","description":"<p>Suppose that it is announced tomorrow that quantum computers are available, and bad actors already have access to them and are able to use them to steal users’ funds. Preventing such a scenario is the goal of <strong>quantum-resistant cryptography</strong> (eg. Winternitz signatures, STARKs), and once account abstraction is in place, any user can switch to using a quantum-resistant signature scheme on their own schedule. But what if we don’t have that much time, and a sudden quantum transition happens long before that?</p>\n<p>I argue that actually, <strong>we are <em>already</em> well-positioned to make a pretty simple recovery fork to deal with such a situation</strong>. The blockchain would have to hard fork and users would have to download new wallet software, but few users would lose their funds.</p>\n<p>The main challenge with quantum computers is as follows. An Ethereum address is defined as <code>keccak(priv_to_pub(k))[12:]</code>, where <code>k</code> is the private key, and <code>priv_to_pub</code> is an elliptic curve multiplication to convert the privkey into a pubkey. With quantum computers, elliptic curve multiplications become invertible (because it’s a discrete-log problem), but hashes are still safe. If a user has not made any transactions with their account, then only the address is publicly visible and they are already safe. But if a user has made even one transaction, then the signature of that transaction reveals the public key, which in a post-quantum world allows revealing the private key. And so most users would be vulnerable.</p>\n<p>But we can do much better. The key realization is that in practice, <strong>most users’ private keys are themselves the result of a bunch of hash calculations</strong>. Many keys are generated using <a href=\"https://github.com/bitcoin/bips/blob/b3701faef2bdb98a0d7ace4eedbeefa2da4c89ed/bip-0032.mediawiki\">BIP-32</a>, which generates each address through a series of hashes starting from a master seed phrase. Many non-BIP-32 methods of key generation work similarly: eg. if a user has a brainwallet, it’s generally a series of hashes (or medium-hard KDF) applied to some passphrase.</p>\n<p>This implies the natural structure of an EIP to hard-fork the chain to recover from a quantum emergency:</p>\n<ol>\n<li>Revert all blocks after the first block where it’s clear that large-scale theft is happening</li>\n<li>Traditional EOA-based transactions are disabled</li>\n<li>A new transaction type is added to allow transactions from smart contract wallets (eg. part of <a href=\"https://ethereum-magicians.org/t/rip-7560-native-account-abstraction/16664\">RIP-7560</a>), if this is not available already</li>\n<li>A new transaction type or opcode is added by which you can provide a STARK proof which proves knowledge of (i) a private preimage <code>x</code>, (ii) a hash function ID <code>1 &lt;= i &lt; k</code> from a list of <code>k</code> approved hash functions, and (iii) a public address <code>A</code>, such that <code>keccak(priv_to_pub(hashes[i](x)))[12:] = A</code>. The STARK also accepts as a public input the hash of a new piece of validation code for that account. If the proof passes, your account’s code is switched over to the new validation code, and you will be able to use it as a smart contract wallet from that point forward.</li>\n</ol>\n<p>For gas efficiency reasons (after all, STARKs are big), we can allow the STARK to be a batch proof, proving N STARKs of the above type (it has to be a STARK-of-STARKs rather than a direct proof of multiple claims, because each user’s <code>x</code> needs to be kept private from the aggregator).</p>\n<p>The infrastructure to implement a hard fork like this could in principle start to be built tomorrow, making the Ethereum ecosystem maximally ready in case a quantum emergency does actually come to pass.</p>\n            <p><small>20 posts - 15 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/how-to-hard-fork-to-save-most-users-funds-in-a-quantum-emergency/18901\">Read full topic</a></p>","link":"https://ethresear.ch/t/how-to-hard-fork-to-save-most-users-funds-in-a-quantum-emergency/18901","pubDate":"Sat, 09 Mar 2024 07:21:14 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-18901"},"source":{"@url":"https://ethresear.ch/t/how-to-hard-fork-to-save-most-users-funds-in-a-quantum-emergency/18901.rss","#text":"How to hard-fork to save most users' funds in a quantum emergency"},"filter":false},{"title":"Economic Analysis of Execution Tickets","dc:creator":"jonahb27","category":"Economics","description":"<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/1/1d8b400d2b06ae96128cab2f8df95a5ef4a8cb15.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/1d8b400d2b06ae96128cab2f8df95a5ef4a8cb15\" title=\"ET\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/1/1d8b400d2b06ae96128cab2f8df95a5ef4a8cb15_2_500x500.jpeg\" alt=\"ET\" data-base62-sha1=\"4dmbHPHMGtxj46utYBUT4KOwEaF\" width=\"500\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/1/1d8b400d2b06ae96128cab2f8df95a5ef4a8cb15_2_500x500.jpeg, https://ethresear.ch/uploads/default/optimized/2X/1/1d8b400d2b06ae96128cab2f8df95a5ef4a8cb15_2_750x750.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/2X/1/1d8b400d2b06ae96128cab2f8df95a5ef4a8cb15_2_1000x1000.jpeg 2x\" data-dominant-color=\"7F6B67\"></a></div><p></p>\n<p><em>By <a href=\"https://twitter.com/_JonahB_\">Jonah Burian</a> &amp; <a href=\"https://twitter.com/DavideCrapis\">Davide Crapis</a></em></p>\n<p><em>Thank you <a href=\"https://twitter.com/_alekslarsen\">Aleks Larsen</a>, <a href=\"https://twitter.com/barnabemonnot\">Barnabé Monnot</a>, <a href=\"https://twitter.com/drakefjustin\">Justin Drake</a>, <a href=\"https://twitter.com/mikeneuder\">Mike Neuder</a>, and others at <a href=\"https://twitter.com/blockchaincap\">Blockchain Capital</a> and the <a href=\"https://twitter.com/ethereum\">Ethereum Foundation</a> for the help.</em></p>\n<h1><a name=\"intro-1\" class=\"anchor\" href=\"https://ethresear.ch#intro-1\"></a>Intro</h1>\n<p>We present an economic analysis of the <a href=\"https://ethresear.ch/t/execution-tickets/17944\">Execution Tickets</a> mechanism. We setup a framework that allows us to characterize how the value of Execution Tickets (the assets) changes with different configurations of the allocation and lottery mechanisms. We derive foundational results that have important implications when thinking about the design and the feasibility of such a mechanism for selling the right to propose an execution block in Ethereum. Our goal is to build on these results and implications as we work on validating pricing and allocation mechanism designs.</p>\n<p>The post has three main sections: in Background Context we summarize motivations for the proposal, in Economic Analysis we setup the framework and present results and their implications, we conclude with Other Considerations that are important for practical design or to expand the analysis.</p>\n<p>Note: in this post we present the main observations of the analysis with minimal notation. For a full formal analysis with proofs see Jonah’s “<a href=\"https://jonahb.xyz/Execution-Tickets\">The Future of MEV</a>” paper.</p>\n<h1><a name=\"background-context-2\" class=\"anchor\" href=\"https://ethresear.ch#background-context-2\"></a>Background Context</h1>\n<p><strong>Execution Layer Rewards</strong></p>\n<p>Proposers have a <em>one-slot monopoly on block production</em>, affording them the ability to extract two key sources of revenue: priority fees and MEV. In the future, with preconfs and based sequencing, there is a potential that this monopoly will open up additional sources of revenue.</p>\n<p>In this analysis, <em>Execution Layer Rewards</em> (EL Rewards) will holistically represent all the value that can be captured from proposing a single block.</p>\n<p><strong>EL Reward Capture</strong></p>\n<p>Without external aids, proposers struggle to capture the bulk of accessible MEV. This is because effective capture necessitates sophistication given the need to optimally navigate through combinatorially complex search spaces swiftly. Acknowledging that validators might not be ideally suited for these intricate challenges, <em>Proposer Builder Separation</em> (PBS) has been widely embraced. This approach enables all proposers, irrespective of their level of sophistication, to capture the majority of the MEV value in their block.</p>\n<p>PBS distinguishes the block-building function from the proposing function. Proposers can opt into an auction where builders bid for the right to choose the execution payload. Proposers then propose the execution payload with the highest associated bid. Since the builders select the payload, they receive the EL Rewards minus the bid, while the validator receives the bid. Effectively, builders are paying validators for the right to construct an execution payload. Given the competitiveness of the market and the short term monopoly the validator has on block production, the winning bid ends up being a little shy of the value of the EL Rewards.</p>\n<p><strong>Reassessing the Distribution of EL Rewards</strong><br>\nThis situation brings to the forefront the issue of how EL Rewards are distributed. Validators currently capture most of these rewards, but this raises questions about the optimal allocation of value within the Ethereum ecosystem. Is it more beneficial for the network’s long-term health and security if a portion of these EL Rewards is captured by the protocol itself and redirected to benefit the community more broadly? This would require somehow brokering MEV payments at the protocol level. The potential redistribution of EL Rewards could lead to a more equitable and balanced economic model within Ethereum, enhancing network security and aligning with its deflationary goals.</p>\n<p>One approach to better distribute EL Rewards would be to enshrine PBS, also known as ePBS, at the protocol level and to burn the bid using a mechanism called MEV-Burn. While this proposal has been hotly debated, a different approach has surfaced: <strong>The Execution Tickets Mechanism.</strong></p>\n<h1><a name=\"economic-analysis-3\" class=\"anchor\" href=\"https://ethresear.ch#economic-analysis-3\"></a>Economic Analysis</h1>\n<p><strong>ET Mechanism</strong></p>\n<p>At a high level, this mechanism splits proposers into two distinct roles: <em>Beacon Block Proposer</em> (BP) &amp; <em>Execution Block Proposer</em> (EP) with the latter responsible for the construction of the execution payload. See a full description <strong><a href=\"https://ethresear.ch/t/execution-tickets/17944\">here</a></strong>. Selection for the EP occurs through a new mechanism:</p>\n<ul>\n<li>When the protocol activates, <em>n</em> lottery tickets are minted and can be purchased from the protocol.</li>\n<li>For every block, one ticket is drawn, which give its holder the right to propose.</li>\n<li>After the winner proposes the execution payload, the winner’s ticket is burned and a new one is minted which is available for anyone to buy.</li>\n<li>This process continues indefinitely.</li>\n</ul>\n<p>These tickets not only confer the <strong>right to EL Rewards</strong> but also introduce a <strong>new native asset to Ethereum</strong>, potentially creating their own economy and market. Revenue from ticket sales is intended to be burned, applying deflationary pressure on ETH’s supply, increasing ETH’s value, and thereby enhancing network security.</p>\n<p>The exact mechanism for selling these tickets is still an open research question (as the protocol will most likely only be able to capture the value of the tickets from primary, not secondary, sales).</p>\n<p><strong>Economic Model</strong><br>\nStart with <span class=\"math\">n</span> tickets. A ticket provides an opportunity to win a prize (EL Reward) at discrete time intervals (slot), denoted by <span class=\"math\">t</span>. At each increment, one ticket wins. Winning results in the ticket being voided (burned) and excluded from subsequent draws. A single fresh ticket is generated (minted) following the award of the prize at each interval.</p>\n<p>Key quantities:</p>\n<ul>\n<li><span class=\"math\">t</span> — discrete time intervals (slot) at which prizes are awarded.</li>\n<li><span class=\"math\">n</span> — the number of tickets.\n<ul>\n<li><span class=\"math\">\\frac{1}{n}</span> is the probability of a single ticket winning at each time interval <span class=\"math\">t</span>.</li>\n</ul>\n</li>\n<li><span class=\"math\">d</span> is the inter-slot discount rate used to calculate the present value of future prizes.</li>\n<li><span class=\"math\">\\mathcal{R}</span> is a random variable representing the value of the prize (EL Reward) at time <span class=\"math\">t</span>.\n<ul>\n<li>We assume that <span class=\"math\">\\mathcal{R}</span> has a distribution that does not vary with time and that each draw is independent. (This is usually not the case in practice, as EL rewards are time-varying and correlated, but it allows for a less complicated analysis that can be expanded later.)</li>\n</ul>\n</li>\n<li><span class=\"math\">\\mu_{\\mathcal{R}}</span> is the expected value of <span class=\"math\">\\mathcal{R}</span>.</li>\n<li><span class=\"math\">V_{ticket}</span> — Net Present Value (NPV) of a single ticket.\n<ul>\n<li>Note: present value of some value <span class=\"math\">X</span> realized at time <span class=\"math\">t</span> is calculated as <span class=\"math\">\\frac{X}{(1+d)^t}</span>.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Results and Implications</strong></p>\n<p>We presents the main results of the analysis as a series of five findings and focus on their implications for mechanism design.</p>\n<p><strong>Observation 1.</strong> <em>The expected net present value of all future EL Rewards is <span class=\"math\">NPV_{\\mathcal{R}} = \\frac{\\mu_{\\mathcal{R}}}{d}</span></em></p>\n<p><strong>Implication: Goal Setting.</strong> This is a clear, closed-form solution for the total capturable value. The question then becomes, how much of this value can execution tickets capture?</p>\n<p><strong>Observation 2.</strong> <em>The expected value of a single ticket is <span class=\"math\">E[V_{ticket}] =  \\frac{\\mu_{\\mathcal{R}}}{nd+1}</span></em></p>\n<p><strong>Implication: Breaking Information Asymmetry.</strong> Previously, <span class=\"math\">\\mu_{\\mathcal{R}}</span> was known ex post by all and ex ante only by highly sophisticated actors. Given the current market value and knowledge of the current discount rate, ticket sales reduce network information asymmetry by allowing the market to know the implied <span class=\"math\">\\mu_{\\mathcal{R}}</span> even ex ante.</p>\n<p><strong>Observation 3.</strong> <em>The net present value of all tickets issued and unissued equals the expected net present value of the EL Rewards:</em></p>\n<div class=\"math\">\nE[V_{\\text{all tickets}}] = E[V_{\\text{issued tickets}}] +  E[V_{\\text{unissued tickets}}] = NPV_{\\mathcal{R}}\n</div>\n<p><strong>Implications:</strong></p>\n<ul>\n<li><strong>Value Capture:</strong> Assuming an efficient market pricing, Execution Tickets capture all value in the system in expectation! This means that the protocol will be able to distribute this value, instead of all of it going to the validators.</li>\n<li><strong>Pricing Complexity:</strong> The effectiveness of the execution ticket system hinges critically on the method employed for setting ticket prices. It is imperative to consider the implicit discount that will inevitably factor into the pricing strategy (reflecting the operational costs, risks, and the required profit margins for execution proposers). Therefore, the pricing mechanism must be designed thoughtfully to capture as much value as possible for the protocol while still being attractive and viable for proposers. An optimal pricing strategy would aim to minimize the gap between the expected value of the tickets and the practical selling price, which would reduce the potential value leakage into secondary markets and benefit arbitrageurs over the Ethereum protocol.</li>\n</ul>\n<p><strong>Observation 4</strong></p>\n<ul>\n<li><em><strong>a)</strong> If <span class=\"math\">n</span> is sufficiently large, the current market cap of all tickets equals the present value of all future EL Reward, i.e., <span class=\"math\">\\lim_{n \\to \\infty} E[V_{\\text{issued tickets}}] =  NPV_{\\mathcal{R}}</span></em></li>\n<li><em><strong>b)</strong> The expected number of slots until a ticket wins is <span class=\"math\">n</span></em></li>\n<li><em><strong>c)</strong> The expected value of a ticket decreases with  <span class=\"math\">n</span></em></li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/c/c988576357e48df8838af501f72b34f3731fbe18.png\" data-download-href=\"https://ethresear.ch/uploads/default/c988576357e48df8838af501f72b34f3731fbe18\" title=\"Value of ticket\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/c/c988576357e48df8838af501f72b34f3731fbe18_2_690x279.png\" alt=\"Value of ticket\" data-base62-sha1=\"sKQ53zQKED0Q6k1ikOLaNlT3QCs\" width=\"690\" height=\"279\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/c/c988576357e48df8838af501f72b34f3731fbe18_2_690x279.png, https://ethresear.ch/uploads/default/optimized/2X/c/c988576357e48df8838af501f72b34f3731fbe18_2_1035x418.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/c/c988576357e48df8838af501f72b34f3731fbe18_2_1380x558.png 2x\" data-dominant-color=\"F9FAFA\"></a></div><p></p>\n<ul>\n<li><em><strong>d)</strong> The value of commanding <span class=\"math\">P\\%</span> of the outstanding tickets increases in <span class=\"math\">n</span></em></li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/d/d347b5b23d41dc1bbe4af3a0474846a90ca2bc2a.png\" data-download-href=\"https://ethresear.ch/uploads/default/d347b5b23d41dc1bbe4af3a0474846a90ca2bc2a\" title=\"Value of P%\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/d/d347b5b23d41dc1bbe4af3a0474846a90ca2bc2a_2_690x186.png\" alt=\"Value of P%\" data-base62-sha1=\"u94nMVT2Cf5MBkf7kNoP4PTvPMC\" width=\"690\" height=\"186\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/d/d347b5b23d41dc1bbe4af3a0474846a90ca2bc2a_2_690x186.png, https://ethresear.ch/uploads/default/optimized/2X/d/d347b5b23d41dc1bbe4af3a0474846a90ca2bc2a_2_1035x279.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/d/d347b5b23d41dc1bbe4af3a0474846a90ca2bc2a_2_1380x372.png 2x\" data-dominant-color=\"F9FAFA\"></a></div><p></p>\n<p><strong>Implications:</strong></p>\n<ul>\n<li>\n<p><strong>Critical design parameter:</strong> The selection of <span class=\"math\">n</span>, representing both the initial number of tickets issued and the ongoing number of outstanding tickets per block, emerges as a critical design parameter with significant implications for the system’s dynamics and economic outcomes.</p>\n<ul>\n<li><strong>Larger <span class=\"math\">n</span>:</strong>\n<ul>\n<li><strong>Benefits</strong>\n<ul>\n<li><strong>Less Centralization Risk:</strong> The cost of owning a significant share of tickets increases. This protects against monopolizing block construction rights and increases the cost of centralization.</li>\n<li><strong>Democratization</strong>: The value, and thus the cost, of each ticket will be lower, making the market more accessible to those with less capital.</li>\n<li><strong>Vanity value</strong>: as <span class=\"math\">n</span> grows sufficiently large, the current market cap of all tickets approximates the present value of all future EL Rewards.</li>\n</ul>\n</li>\n<li><strong>Problems</strong>\n<ul>\n<li><strong>Complicated Valuations:</strong> A larger <em>n</em> prolongs the expected timeframe for any individual ticket to win, complicating the valuation and forecasting efforts for participants. This is particularly problematic given the assumption that <span class=\"math\">\\mu_{\\mathcal{R}}</span>, the expected rewards, remains constant over time—an assumption that may not hold, further amplifying forecasting challenges. This uncertainty can impose a significant additional upfront discount on ticket values, allowing the secondary market to capture a large chunk of the ticket value.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Small <span class=\"math\">n</span>:</strong>\n<ul>\n<li><strong>Benefits:</strong>\n<ul>\n<li><strong>Simple Valuations:</strong> A smaller <span class=\"math\">n</span> shifts more value to later sales, providing market participants with more information and potentially stabilizing the ticket market, thereby enhancing primary market value capture.</li>\n</ul>\n</li>\n<li><strong>Problems:</strong>\n<ul>\n<li><strong>More</strong> <strong>Centralization Risk</strong></li>\n<li><strong>Less Democratized</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Effects of Pricing Mechanisms.</strong> Different pricing mechanisms may be employed for the initial sale of <span class=\"math\">n</span> tickets and subsequent per-block ticket sales, each with varying efficiencies in capturing the true value of a ticket. A more effective initial sale mechanism may justify a larger <span class=\"math\">n</span>, ensuring more value is captured upfront. In contrast, a superior per-block sale mechanism could justify a smaller <span class=\"math\">n</span>, facilitating an effective value capture over the long term.</p>\n</li>\n</ul>\n<p><strong>Observation 5.</strong> <em>The variance in the value of a single ticket is  <span class=\"math\">\\text{Var}(V_{ticket}) = \\frac{\\text{Var}(\\mathcal{R})+\\mu_{\\mathcal{R}}^2}{nd^2+2nd+1} - \\frac{\\mu_{\\mathcal{R}}^2}{n^2d^2 + 2nd + 1}</span></em></p>\n<p><strong>Implications:</strong></p>\n<ul>\n<li><strong>Variance Sensitivity.</strong> Buyers may be sensitive to variance and factor it into pricing (often placing a discount).</li>\n<li><strong>Valuation complexity</strong>\n<ul>\n<li>Valuing tickets will be challenging due to the necessity of forecasting over an infinite time horizon. This issue is further complicated by the lumpiness of EL Rewards, i.e., the high variance of <span class=\"math\">\\mathcal{R}</span> , which may change over time (contrary to the assumption). This variance contributes to the variance in the value of the ticket as <span class=\"math\">\\text{Var}(V_{ticket})</span> scales with <span class=\"math\">\\text{Var}(\\mathcal{R})</span>. Additionally, <span class=\"math\">d</span> will not remain static (contrary to the assumption), necessitating forecasts to account for events like rate hikes and cuts.</li>\n<li>Such complexity inevitably leads to the mispricing of tickets, raising concerns about whether this ticketing mechanism is purely theoretical and not suitable for practice. The primary concern lies in underpricing, which could suggest a failure to capture the intended EL Rewards. Conversely, overpricing (potentially more likely due to the speculative or gambling premium associated with the tickets) could result in capturing excess value (a scenario that might not be entirely negative since it redistributes rewards back to the protocol).</li>\n</ul>\n</li>\n<li><strong>Ticket Pooling:</strong> The emergence of ticket pooling can mitigate the impact of  <span class=\"math\">R</span>'s variance. By distributing rewards and variance across a pool, the system can reduce pricing complexity and could lead to more accurate ticket valuations. More work is needed to formalize the impact of pooling.</li>\n</ul>\n<h1><a name=\"other-considerations-4\" class=\"anchor\" href=\"https://ethresear.ch#other-considerations-4\"></a>Other Considerations</h1>\n<p><strong>Multi-Block MEV and Centralization Risks</strong><br>\nOne of the most critical challenges to this study relates to the phenomenon of multi-block MEV. This concept posits that controlling multiple consecutive blocks can yield disproportionately higher rewards than the sum of the individual blocks’ values. If multi-block MEV is prevalent (though its existence in practice is not yet clear), the advantage gained from controlling successive blocks could lead to significant, potentially exponential, centralization pressures. Multi-block MEV could lead to the formation of centralized ticket pools, with entities controlling a large number of consecutive blocks. This trend would be contrary to Ethereum’s ethos of decentralization and could pose a threat to network security. This complex dynamic suggests that if multi-block MEV does exist, it would require an entirely different model than the one proposed above to accurately factor in the greater-than-the-sum-of-its-parts effect inherent in multi-block control. The implications of such a scenario are existential and necessitate further investigation.</p>\n<p><strong>Related Work</strong></p>\n<p>This section contrasts the proposed Execution Tickets mechanism with established concepts like MEV-Burn and MEV-Smoothing, and highlights the unique advantages of the  Execution Tickets approach.</p>\n<ul>\n<li><strong>Comparison with MEV-Burn:</strong>\n<ul>\n<li><strong>Simplification:</strong> Neuder notes: “The current version of mev-burn is tightly coupled with the attesting committee for a given slot. The burning mechanism in the execution ticket design is more straightforward…\"</li>\n<li><strong>Timing and Value Capture:</strong><br>\nIn the <a href=\"https://ethresear.ch/t/mev-burn-a-simple-design/15590\">simple MEV-Burn model</a>, the amount burned equals the highest bid in the first <span class=\"math\">D</span> seconds of the auction. It is known that MEV bids increase monotonically over time because builders have more time to explore the problem space. Consequently, MEV-Burn fails to burn all potential MEV. The Execution Ticket model, conversely, captures the full MEV value, albeit in expectation.</li>\n<li><strong>Credible Neutrality</strong><br>\nAnother issue with MEV-Burn is its reliance on ePBS, and the problem lies in the very name — PBS is enshrined. This approach is opinionated and <a href=\"https://vitalik.eth.limo/general/2023/09/30/enshrinement.html\">not credibly neutral</a>. Better supply chains and auction systems might be discovered in the future. The ticketing system is agnostic to the method of MEV capture and simply presupposes that MEV value will flow to the execution proposer due to the proposer’s inherent block space monopoly.</li>\n<li><strong>Bypassability Issues:</strong><br>\nA significant challenge with MEV-Burn is the ePBS <a href=\"https://notes.ethereum.org/@mikeneuder/infinite-buffet\">bypassability problem</a>. There is no credible method to ensure all proposers and builders use the protocol. The ticketing system avoids this issue by not dictating an MEV capture method. This flexibility renders the ticketing system less prone to circumvention compared to ePBS.</li>\n</ul>\n</li>\n<li><strong>Comparison with <a href=\"https://notes.ethereum.org/cA3EzpNvRBStk1JFLzW8qg\">MEV-Smoothing</a>:</strong>\n<ul>\n<li>\n<p><strong>Philosophical Considerations on Reward Distribution:</strong></p>\n<p>A fair argument can be made that the protocol should not intervene in artificially smoothing rewards, but allow for a free market where individuals and entities decide their own tolerance for variances in earnings. Those seeking lower variance in the proposed Execution Tickets mechanism can join ticket pools akin to the current staking pools offered by Lido and Coinbase. However, it is important to consider the potential for centralization (like what is happening with liquid staking pools). Further research is necessary to determine the viability of decentralized ticket pools.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1><a name=\"conclusion-5\" class=\"anchor\" href=\"https://ethresear.ch#conclusion-5\"></a><strong>Conclusion</strong></h1>\n<p>When Execution Tickets are priced correctly they can indeed internalize all value generated from proposing execution payloads and redirect what was once validator revenue to the protocol. This is a significant breakthrough. It suggests that the contentious issue of MEV capture could potentially be resolved without necessitating ePBS.</p>\n<p>The analysis reveals that a ticket represents a clean and elegant abstraction: a share of future EL Rewards. Such a mechanism sets the foundation for a robust market where tickets serve as leading indicators of network value generation. Moreover, as these tickets are sold prior to a block construction, the revenue generated could bolster Ethereum’s security budget in anticipation of highly volatile events.</p>\n<p>However, the success of this mechanism hinges on the efficacy of the ticket sale process. The protocol must be capable of selling tickets at their intrinsic value; otherwise, the value would inevitably leak into a secondary market.</p>\n            <p><small>2 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894\">Read full topic</a></p>","link":"https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894","pubDate":"Thu, 07 Mar 2024 16:24:37 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-18894"},"source":{"@url":"https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894.rss","#text":"Economic Analysis of Execution Tickets"},"filter":false},{"title":"Considering Client Diversity through the lens of Network Performance","dc:creator":"umbnat92","category":"Proof-of-Stake","description":"<p>by <a href=\"https://twitter.com/umb_nat\" rel=\"noopener nofollow ugc\">U. Natale</a> and <a href=\"https://twitter.com/gabriellassh\" rel=\"noopener nofollow ugc\">G. Sofia</a>.</p>\n<p><strong>Acknowledgements</strong><br>\nThis research has been granted by <a href=\"https://chorus.one/\" rel=\"noopener nofollow ugc\">Chorus One</a>. We are grateful to <a href=\"https://twitter.com/plc_hld\" rel=\"noopener nofollow ugc\">M. Moser</a> for useful discussions and comments.</p>\n<h1><a name=\"h-1-introduction-1\" class=\"anchor\" href=\"https://ethresear.ch#h-1-introduction-1\"></a>1 Introduction</h1>\n<p>There’s a lot that’s been said about client diversity in the Ethereum ecosystem. A diverse set of clients (at the consensus layer, and even at the execution level) is one of the first line of defense strategies in the face of unforeseen events, prioritizing the liveness of the network and allowing for its self-healing properties. A level of decentralization of block production is necessary for this dynamic liveness, which can be appreciated at many levels (geographical, organizational, and during software development). Even when there is a single supermajority client, Ethereum can survive many liveness faults due to the use of different Ethereum clients in the network. Not all the risk scenarios are perfect, but it is one of it’s greatest strengths.</p>\n<p>Blockchains are complex coordination systems, and Ethereum is no exception. The underlying rules are defined in a single, open-source specification, freely accessible for anyone to contribute to. This core concept is separate from the various software programs individuals and institutions use to run nodes on the network.</p>\n<h3><a name=\"the-importance-of-client-diversity-2\" class=\"anchor\" href=\"https://ethresear.ch#the-importance-of-client-diversity-2\"></a>The importance of client diversity</h3>\n<p>Ethereum’s on chain incentives (and penalties) strongly encourage diversity among client deployments and infrastructure. The more validators rely on a single type of client software, the greater the risk of a single point of failure in case of a critical bug.</p>\n<p>We can consider for a moment what would happen in such an event:</p>\n<ul>\n<li>In the case of a liveness fault, most BFT networks will stall and not make progress when 67% of the network’s nodes are affected, thus there’s <em>no point</em> in having a minority client. In the case of Ethereum, it will allow the minority clients to produce blocks that make it into the chain, even if finalization is stalled for hours or days.</li>\n<li>In the event of a safety fault, the network behaves quite differently. Such a bug in the supermajority client (67%) can quite quickly lead to erroneous confirmation. Finality of the corrupted chain could then put the entire Ethereum community in a serious dilemma. Many of  the affected validators could be slashed by trying to join the correct chain, a catastrophic development that puts many node operators at risk.</li>\n</ul>\n<p>If the majority does not exceed that 67% threshold, a bug in a single client cannot be finalized.</p>\n<h3><a name=\"the-importance-of-client-diversity-3\" class=\"anchor\" href=\"https://ethresear.ch#the-importance-of-client-diversity-3\"></a>The importance of client diversity</h3>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/1/1bcac3ba84638e9bd0e8957479998d4b6841296d.png\" data-download-href=\"https://ethresear.ch/uploads/default/1bcac3ba84638e9bd0e8957479998d4b6841296d\" title=\"Screenshot 2024-03-04 at 2.57.19 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/1/1bcac3ba84638e9bd0e8957479998d4b6841296d_2_690x392.png\" alt=\"Screenshot 2024-03-04 at 2.57.19 AM\" data-base62-sha1=\"3XRjojU6fhRl4KPaq4D0vDvnZ5H\" width=\"690\" height=\"392\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/1/1bcac3ba84638e9bd0e8957479998d4b6841296d_2_690x392.png, https://ethresear.ch/uploads/default/optimized/2X/1/1bcac3ba84638e9bd0e8957479998d4b6841296d_2_1035x588.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/1/1bcac3ba84638e9bd0e8957479998d4b6841296d_2_1380x784.png 2x\" data-dominant-color=\"F1F2F3\"></a></div><br>\n<strong>Fig. 1:</strong> A somewhat naive but useful look at the current status of client diversity. Source: <a href=\"https://clientdiversity.org/\" rel=\"noopener nofollow ugc\">https://clientdiversity.org/</a><p></p>\n<p>True client diversity is not as simple as avoiding the majority client. Alongside issues in fingerprinting and a reliance on self-reporting, the real status of client diversity for CL and EL clients is difficult to pin point with exactitude. The previous chart proposes a view of network diversity that highlights a concerning prevalence of the Geth execution client, and a less than ideal distribution in the consensus layer.</p>\n<blockquote>\n<p>But what if a majority client consistently proposes the best blocks for the network, due to a more efficient, faster or more sophisticated implementation?</p>\n</blockquote>\n<p>A final, sometimes overlooked, point about having different clients is that it fosters innovation. Outside of the clear rules for confirmation, different teams can propose and implement changes to the Ethereum protocol. Today we want to focus on the performance effects of client diversity, by sharing an analysis on two of it’s most widely used CL clients: Lighthouse and Teku. The goal is to contribute to a better understanding and stability of the entire Ethereum ecosystem, and to consider potential improvements that can mitigate second-level effects for validator behavior and diverse participation.</p>\n<h1><a name=\"h-2-are-cl-clients-made-equal-4\" class=\"anchor\" href=\"https://ethresear.ch#h-2-are-cl-clients-made-equal-4\"></a>2 Are CL clients made equal?</h1>\n<p>The role of CL clients in the Ethereum ecosystem is critical to maintaining the integrity and efficiency of the blockchain. Among their responsibilities, we find the production of attestations and management of networking. These functions directly influence the overall effectiveness of validators, which in turn affects the robustness and reliability of the network. In this section, we aim to scrutinize the performance of Lighthouse and Teku, with respect to these core duties.</p>\n<p>We collected data from 2024-01-11 to 2024-02-27 using the <a href=\"https://api-docs.rated.network/getting-started/welcome\" rel=\"noopener nofollow ugc\">Rated Network API</a>, focusing on the aggregate performance of <a href=\"https://chorus.one/\" rel=\"noopener nofollow ugc\">Chorus One</a>’s validators during this period.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/d/d259446823f0ee1d9bbda1a331d46b35de792e20.png\" data-download-href=\"https://ethresear.ch/uploads/default/d259446823f0ee1d9bbda1a331d46b35de792e20\" title=\"Rolling Median Validator Effectiveness\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/d/d259446823f0ee1d9bbda1a331d46b35de792e20_2_690x230.png\" alt=\"Rolling Median Validator Effectiveness\" data-base62-sha1=\"u0Pwq65q4XMG3BBBJ7MKQXWsnmg\" width=\"690\" height=\"230\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/d/d259446823f0ee1d9bbda1a331d46b35de792e20_2_690x230.png, https://ethresear.ch/uploads/default/optimized/2X/d/d259446823f0ee1d9bbda1a331d46b35de792e20_2_1035x345.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/d/d259446823f0ee1d9bbda1a331d46b35de792e20_2_1380x460.png 2x\" data-dominant-color=\"F4F6F5\"></a></div><br>\n<strong>Fig. 2:</strong> Daily rolling median of avg. validator effectiveness for Chorus One validators from 2024-01-11 to 2024-02-27. Data from <a href=\"https://www.rated.network/?network=mainnet&amp;view=pool&amp;timeWindow=7d&amp;page=1&amp;poolType=all\" rel=\"noopener nofollow ugc\">Rated Network</a>.<p></p>\n<p>Figure 2 shows the rolling median of daily <a href=\"https://docs.rated.network/methodologies/ethereum-beacon-chain/rated-effectiveness-rating\" rel=\"noopener nofollow ugc\">validator effectiveness</a> distribution for these two clients. Despite the overall behavior being clearly correlated — primarily due to a combination of our infrastructure and network performance — a noticeable divergence exists in the performance of validators associated with these clients. Validators utilizing the Teku client consistently outperform those utilizing Lighthouse, as evidenced by both 7-days (left panel) and 30-days (right panel) rolling medians of effectiveness.</p>\n<p>When accounting for the daily distribution, we observed no significant anomalies that could be attributed to transient network effects, which might otherwise bias the outcome of the study. This daily granularity allow us to confirm that the observed performance disparity is consistent, and not a product of temporal network fluctuations. Preliminary findings suggest that the Teku client facilitates a higher level of validator effectiveness compared to Lighthouse.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/8/89edeaacca42d90eb3cb0ebc9526131980da7129.png\" data-download-href=\"https://ethresear.ch/uploads/default/89edeaacca42d90eb3cb0ebc9526131980da7129\" title=\"Rolling Median Attestations and Correctness and inc. delay\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/8/89edeaacca42d90eb3cb0ebc9526131980da7129_2_690x460.png\" alt=\"Rolling Median Attestations and Correctness and inc. delay\" data-base62-sha1=\"jGb99tYThQ7MrMc5diYUkamAaJr\" width=\"690\" height=\"460\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/8/89edeaacca42d90eb3cb0ebc9526131980da7129_2_690x460.png, https://ethresear.ch/uploads/default/optimized/2X/8/89edeaacca42d90eb3cb0ebc9526131980da7129_2_1035x690.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/8/89edeaacca42d90eb3cb0ebc9526131980da7129_2_1380x920.png 2x\" data-dominant-color=\"F1F3F2\"></a></div><p></p>\n<p><strong>Fig. 3:</strong> Daily rolling median of avg. total attestations (upper panels), avg. correctness (middle panels), and avg. inclusion delay (lower panel) for Chorus One validators from 2024-01-11 to 2024-02-27. Data from <a href=\"https://www.rated.network/?network=mainnet&amp;view=pool&amp;timeWindow=7d&amp;page=1&amp;poolType=all\" rel=\"noopener nofollow ugc\">Rated Network</a>.</p>\n<p>Figure 3 shows the relevant metrics that contribute to shape the final validator effectiveness. To be precise, we show the rolling median of average total attestations (upper panels), average correctness (middle panels), and average inclusion delay (lower panel) for Chorus One validators from 2024-01-11 to 2024-02-27. Analyzing the different metrics separately suggests that the primary disparities between the two CL clients lie in the number of attestations and their correctness, with the inclusion delay differences being less significant in comparison. However, it is worth noting that, despite the small difference in the median of inclusion delay, Teku still seems to perform better. The consistent disparity translates in a non-negligible effect on the final validator effectiveness.</p>\n<p>On a side note, also the daily rolling median of average uptime indicates a better network response for Teku validators, cfr. Fig. 4.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/4/43a59b8f244cf783db5c231a5f3415555de202d0.png\" data-download-href=\"https://ethresear.ch/uploads/default/43a59b8f244cf783db5c231a5f3415555de202d0\" title=\"Rolling Median Avg. Uptime\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/4/43a59b8f244cf783db5c231a5f3415555de202d0_2_690x230.png\" alt=\"Rolling Median Avg. Uptime\" data-base62-sha1=\"9EqNNO8vCQftKHiV33brHoTmM8w\" width=\"690\" height=\"230\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/4/43a59b8f244cf783db5c231a5f3415555de202d0_2_690x230.png, https://ethresear.ch/uploads/default/optimized/2X/4/43a59b8f244cf783db5c231a5f3415555de202d0_2_1035x345.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/4/43a59b8f244cf783db5c231a5f3415555de202d0_2_1380x460.png 2x\" data-dominant-color=\"F3F5F5\"></a></div><p></p>\n<p><strong>Fig. 4:</strong> Daily rolling median of avg. uptime for Chorus One validators from 2024-01-11 to 2024-02-27. Data from <a href=\"https://www.rated.network/?network=mainnet&amp;view=pool&amp;timeWindow=7d&amp;page=1&amp;poolType=all\" rel=\"noopener nofollow ugc\">Rated Network</a>.</p>\n<p>We then compared the overall distributions for each validator in the sample, removing the dependency on the specific day. The outcome of this is shown in Fig. 5.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/e/ebfae0d97f89073f460f65a6832e684d5a504d94.png\" data-download-href=\"https://ethresear.ch/uploads/default/ebfae0d97f89073f460f65a6832e684d5a504d94\" title=\"client_dependency_validator_pdfs\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/e/ebfae0d97f89073f460f65a6832e684d5a504d94_2_690x383.png\" alt=\"client_dependency_validator_pdfs\" data-base62-sha1=\"xFzI0KYv2qU0Ee1rLUgfDnSKe0c\" width=\"690\" height=\"383\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/e/ebfae0d97f89073f460f65a6832e684d5a504d94_2_690x383.png, https://ethresear.ch/uploads/default/optimized/2X/e/ebfae0d97f89073f460f65a6832e684d5a504d94_2_1035x574.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/e/ebfae0d97f89073f460f65a6832e684d5a504d94_2_1380x766.png 2x\" data-dominant-color=\"F8F9F9\"></a></div><p></p>\n<p><strong>Fig. 5:</strong> Probability density function of validator effectiveness, avg. inclusion delay, and avg. correctness for Chorus One Lighthouse and Teku validators from 2024-01-11 to 2024-02-27.</p>\n<p>The metric that differs most significantly is the correctness of attestations. The distributions of average correctness are the only ones that did not pass the Kolmogorov-Smirnov test, yielding a p-value of 1.16e-8. The hypothesis that the distributions of average inclusion delay are drawn from the same distribution cannot be excluded, given a p-value of 0.83. We observe a maximum inclusion of 1.08 for Teku client and 1.23 for Lighthouse, while median, 25% and 95% quantiles are consistent. Nonetheless, as noted in the daily distribution, validators running the Teku client tend to be more frequently in the region with lower inclusion delay on a daily basis. This may indicate some dependency on network behavior and warrants further investigation. A similar behavior is observed for total unique attestations.</p>\n<p>When comparing the global validator effectiveness distributions, we find a p-value of 0.08. Despite the absence of a strong rejection, we still detect an overall better performance for the Teku validators, with a median of 98.39% and a 25%-quantile of 97.25% (compared with 98.25% and 96.97%, respectively, for Lighthouse validators).</p>\n<h2><a name=\"h-21-effects-of-cl-on-mev-boost-5\" class=\"anchor\" href=\"https://ethresear.ch#h-21-effects-of-cl-on-mev-boost-5\"></a>2.1 Effects of CL on MEV-Boost</h2>\n<p><a href=\"https://arxiv.org/pdf/2305.09032.pdf\" rel=\"noopener nofollow ugc\">Since the formal introduction of timing games</a>, we have seen an <a href=\"https://timing.pics/\" rel=\"noopener nofollow ugc\">ever-increasing number of entities start playing such games</a>. While research has shed light on the potential network implications of the timing games, and <a href=\"https://ethresear.ch/t/timing-games-implications-and-possible-mitigations/17612\">discussions around mitigation strategies</a> are ongoing, we have only few data-driven studies in literature estimating the consequent effects on the network, see e.g. <a href=\"https://ethresear.ch/t/the-cost-of-artificial-latency-in-the-pbs-context/17847\">The cost of artificial latency in the PBS context</a> and <a href=\"https://hackmd.io/@dataalways/latency-is-money?utm_source=preview-mode&amp;utm_medium=rec\" rel=\"noopener nofollow ugc\">Latency is Money: Timing Games /acc</a>.</p>\n<p>A particularly underexplored avenue is the relationship between the choice of CL clients and the dynamics of MEV-Boost. In this section, we present a preliminary observation that seeks to start a broader exploration to fill this void in the literature.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/c/c492a7eaa077cc8409f7e2e35be79f4445f3de22.png\" data-download-href=\"https://ethresear.ch/uploads/default/c492a7eaa077cc8409f7e2e35be79f4445f3de22\" title=\"client_dependency_send_getHeader_adagio_pdf\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/c/c492a7eaa077cc8409f7e2e35be79f4445f3de22_2_690x230.png\" alt=\"client_dependency_send_getHeader_adagio_pdf\" data-base62-sha1=\"s2XNaAao4nA0J5KbKFbtHUL483U\" width=\"690\" height=\"230\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/c/c492a7eaa077cc8409f7e2e35be79f4445f3de22_2_690x230.png, https://ethresear.ch/uploads/default/optimized/2X/c/c492a7eaa077cc8409f7e2e35be79f4445f3de22_2_1035x345.png 1.5x, https://ethresear.ch/uploads/default/optimized/2X/c/c492a7eaa077cc8409f7e2e35be79f4445f3de22_2_1380x460.png 2x\" data-dominant-color=\"F6F7F7\"></a></div><p></p>\n<p><strong>Fig. 6:</strong> Probability density function for the start sent getHeader request (the start of validators’ view of the auction). The left panel shows the distribution for Lighthouse client, the right panel shows the distribution for the Teku client. The data are Chorus One proprietary data from 2024-01-11 to 2024-02-27.</p>\n<p>Our analysis, captured in Fig. 6, delineates a marked disparity between the CL clients that we are considering in the study. We observe that validators running the Teku client are able to initiate the MEV-Boost auction significantly earlier within the slot timeframe.</p>\n<p>Lighthouse validators start the send getHeader request with a median of 516.0 ms into the slot, with 95% and 25% quantiles at 821.2 ms and 473.5 ms. On the other hand, Teku validators start the auction with a median of 219.0 ms — 95% and 25% quantiles at 330.8 ms and 197.0 ms. We can observe that not only the Lighthouse distribution is shifted towards higher value, but also that the spread of the distribution is quite different — 347.7 ms vs 133.8 ms.</p>\n<p>This discovery is non-trivial; it indicates that, in the MEV landscape, validators operating on the Lighthouse client are at a comparative advantage due to an inherent latency. Indeed, such delay has pronounced implications in the context of MEV, where timing represents a critical factor in the efficacy of MEV extraction strategies. The timing differential observed here could, therefore, have an impact on the overall MEV extraction process, echoing the dynamics of timing games where every millisecond can influence the economic outcome.</p>\n<h1><a name=\"h-3-conclusions-6\" class=\"anchor\" href=\"https://ethresear.ch#h-3-conclusions-6\"></a>3 Conclusions</h1>\n<p>Our analysis has revealed nuanced disparities in network performance between two prevalent CL clients, Lighthouse and Teku, particularly in the context of MEV-Boost and associated timing games. These findings highlight the multifaceted nature of client behavior and the potential for unexpected consequences stemming from network interactions. While Teku clients demonstrate superior network performance metrics, this research uncovers layers of complexity that merit a cautious approach to client selection.</p>\n<p>As we previously mentioned, the principle of client diversity emerges as a prudent strategy in mitigating risks associated with network performance variability and liveness. Our observation of performance discrepancies, despite similar configurations for both validator sets, underscores the necessity to investigate the underlying causes of these differences.</p>\n<p>Moreover, the impact on the dynamics of MEV is significant. Teku clients exhibit a consistently reliable performance in the execution of timing games, suggesting a competitive edge in environments where precision is required. Conversely, the broader distribution observed in Lighthouse clients may offer an advantage in flexibly handling bid cancellation through statistical fluctuations. The higher median latency inherent to Lighthouse clients could inadvertently replicate the desired effect of additional artificial latency without the need to alter MEV-Boost’s underlying code.</p>\n<p>In the constantly evolving arena of timing games, where an increasing number of participants are vying for advantages, superior network performance may become the preferred tradeoff. With relays engaging in these games, excelling in CL duties could lead to augmented gains, particularly as CL rewards remain a substantial component of the total APR for validators.</p>\n<p>It is also worth mentioning that our observations on the performance of CL clients could have a non negligible impact with the introduction of EIP-4844. Indeed, it has already been shown that blob transactions risk slowing down block propagation by orders of a hundred milliseconds in times of increased network activity, cfr. <a href=\"https://mirror.xyz/preconf.eth/cxUO8pPBfqnqAlzFUzoEUa6sgnr68DRmsNhBWPb2u-c\" rel=\"noopener nofollow ugc\">Censorship, Latency, and Preconfirmations in the Blob Market</a>. More investigation is needed to capture the effects of this feature under the complexity of the CL client disparities.</p>\n<p>In conclusion, while our findings may not be definitive, we believe this research should serve as a call to action for all validators across the Ethereum network. We encourage all validators to engage in deep research, leveraging their unique datasets to illuminate the network’s intricacies further. The insights gleaned from such studies will be invaluable, not only in enhancing individual validator performance but also in contributing to the collective understanding and stability of the Ethereum ecosystem as a whole.</p>\n            <p><small>4 posts - 3 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/considering-client-diversity-through-the-lens-of-network-performance/18885\">Read full topic</a></p>","link":"https://ethresear.ch/t/considering-client-diversity-through-the-lens-of-network-performance/18885","pubDate":"Wed, 06 Mar 2024 17:38:57 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-18885"},"source":{"@url":"https://ethresear.ch/t/considering-client-diversity-through-the-lens-of-network-performance/18885.rss","#text":"Considering Client Diversity through the lens of Network Performance"},"filter":false},{"title":"How to create a custom bridge between two EVM-based blockcahin?","dc:creator":"SyedMuhamadYasir","category":"Proof-of-Stake","description":"<p>I have two private testnets running on a network - one is Ethereum using PoW consensus and the other is running Ethereum with PoS consensus</p>\n<p>i want to create a bridge between them - say i want to move ether, from the chain running PoW to the chain running PoS.</p>\n<p>how can i achieve this? do i have to learn to write a bridge - that will allow interoperability between these two chains ? If so, how can i learn to write a bridge between these two blockchains ?</p>\n<p>if there is a more suitable alternative solution to this, please let me know!</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/how-to-create-a-custom-bridge-between-two-evm-based-blockcahin/18883\">Read full topic</a></p>","link":"https://ethresear.ch/t/how-to-create-a-custom-bridge-between-two-evm-based-blockcahin/18883","pubDate":"Wed, 06 Mar 2024 14:46:26 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-18883"},"source":{"@url":"https://ethresear.ch/t/how-to-create-a-custom-bridge-between-two-evm-based-blockcahin/18883.rss","#text":"How to create a custom bridge between two EVM-based blockcahin?"},"filter":false},{"title":"Blockchain Sharded Storage: Web2 Costs and Web3 Security with Shamir Secret Sharing","dc:creator":"snjax","category":"zk-s[nt]arks","description":"<p><em><a href=\"https://github.com/snjax\" rel=\"noopener nofollow ugc\">Igor Gulamov</a>, <a href=\"https://github.com/zeropoolnetwork\" rel=\"noopener nofollow ugc\">ZeroPool</a>, March 2024</em><br>\n<em>Thanks to <a href=\"https://github.com/gnull\" rel=\"noopener nofollow ugc\">Ivan Oleynikov</a> for editing and feedback.</em></p>\n<h2><a name=\"abstract-1\" class=\"anchor\" href=\"https://ethresear.ch#abstract-1\"></a>Abstract</h2>\n<p>CPU scaling for blockchain is solved. However, storage scaling is still a problem. This document<br>\ndescribes a horizontally scalable fault-tolerant storage solution for blockchain that can process<br>\nlarge amounts of data (beyond petabytes) with Web2 storage overhead and Web3 security. With this<br>\nsolution, rollups no longer need to store their blocks on-chain. In other words, we can upgrade<br>\nvalidiums to rollups and nest the rollups recursively into each other with close to zero cost of data storage.</p>\n<p>Our solution uses <a href=\"https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing\" rel=\"noopener nofollow ugc\">Shamir’s Secret Sharing</a><br>\nto split the payload into shards and distribute it among nodes for storage,<br>\nand later retrieve the shards and recover the payload using <a href=\"https://zcash.github.io/halo2/background/polynomials.html#fast-fourier-transform-fft\" rel=\"noopener nofollow ugc\">Fast Fourier<br>\nTransform</a>.<br>\nNodes are paid by the file owner for storing the shards, which is periodically verified using<br>\nzkSNARK. We employ a special shuffling technique to decide which nodes will store the shards of a<br>\ngiven file.  As long as at least half of the network behaves honestly, this technique ensures that<br>\na malicious adversary can not cause a DoS attack on the file by controlling a critical number of its<br>\nshards.</p>\n<p>We present the details of our solution, analyze the cryptographic security guarantees it provides and propose a set of economic incentives to motivate honest node behavior.</p>\n<h2><a name=\"introduction-2\" class=\"anchor\" href=\"https://ethresear.ch#introduction-2\"></a>Introduction</h2>\n<h3><a name=\"problem-statement-3\" class=\"anchor\" href=\"https://ethresear.ch#problem-statement-3\"></a>Problem Statement</h3>\n<p>One of the solutions used for storage scaling on blockchain today is a replication of data.<br>\nIt stores each chunk of payload on multiple nodes.<br>\nNodes produce zero-knowledge proofs of data availability.<br>\nWhen the number of nodes storing the chunks is low,<br>\nthe network distributes the chunks to other nodes.</p>\n<p>Let’s discuss the features of this approach that make it more expensive than Web2 storage.</p>\n<ol>\n<li>\n<p>Redundancy. If we want to build a 50% fault-tolerant network with 128 bits of security,<br>\nwe need to replicate data 128 times. It means that the network should store and transfer 128 times<br>\nmore data than the payload.</p>\n</li>\n<li>\n<p>Preventing deduplication. The replicas of stored data are identical, therefore<br>\nthe nodes that are storing them could try to collude and deduplicate it: collectively store only one<br>\nreplica, saving the costs, but bill the network for storing many replicas. The network mitigates<br>\nthis by applying a distinct encoding to each replica and requiring each node to periodically<br>\nprove that it holds the encoding given to it. This encoding makes conversion between replicas<br>\ncomputationally hard, making deduplication impractical.</p>\n<p>The drawback of this approach is that zero-knowledge proofs now need to be aware of the encoding,<br>\nadding extra computational overhead for honest nodes.</p>\n</li>\n<li>\n<p>Data distribution.<br>\nThe nodes holding replicas of a file may go offline at any moment.<br>\nWhen this happens, the network redistributes the replicas to more nodes<br>\nto ensure the needed level of redundancy.<br>\nAn adversary who controls a large portion of the network<br>\ncould use this mechanism to try to collect all replicas of a given file.</p>\n<p>The network mitigates this by periodically randomly shuffling nodes between the pools of different files.<br>\nInformally speaking, this ensures that the fraction of the adversary’s nodes in each pool stays close to the fraction of its nodes in the whole network.</p>\n</li>\n</ol>\n<p>In the following sections, we propose our solution to this problem with better security and performance than replication.<br>\nAdditionally, our solution is natively zkSNARK-friendly,<br>\nas its polynomial computations can be efficiently done in a zkSNARK.<br>\nThat means that we can include proofs of data availability in proofs of rollup state transitions with little overhead.<br>\nIt will allow us to upgrade validiums to rollups with close to zero cost of data storage.</p>\n<p>Our solution implements data redistribution similarly to the above to account for nodes going offline.<br>\nWe additionally periodically shuffle the nodes between the pools<br>\nto make sure that malicious nodes are distributed evenly between the pools,<br>\nand that the adversary can not monotonously increase the presence of malicious nodes in a given pool<br>\ndue to honest nodes of that pool going offline over time.</p>\n<h3><a name=\"use-cases-4\" class=\"anchor\" href=\"https://ethresear.ch#use-cases-4\"></a>Use Cases</h3>\n<p>Our proposed solution can be seen as a very big decentralized HDD with large (megabytes) sectors.<br>\nAs bare metal HDDs,<br>\nit provides CRUD operations on sectors,<br>\nwhich is directly inefficient for many cases like small files or databases.<br>\nHowever, it is very efficient for storing big files, like videos, images, and backups.</p>\n<p>Also, it is efficient for rollups: the rollups can store their blocks in sectors of the network and merge state transition zk proofs with proofs of data availability.</p>\n<p>This approach makes validiums obsolete. We can upgrade validiums to rollups keeping the same level of security and cost of the storage.</p>\n<p>On the rollup level, we can implement all remaining use cases, like databases, small files, and so on.</p>\n<p>This can help solve the problem of blockchain bloat. We can directly fulfill the rollup state at the checkpoint and then all history of the rollup before the checkpoint could be removed.</p>\n<p>Also, very cheap storage may enable us to implement a lot of web2 solutions,<br>\nlike messengers, social networks, blogs, games, and so on on the blockchain with true decentralization.</p>\n<h2><a name=\"preliminaries-5\" class=\"anchor\" href=\"https://ethresear.ch#preliminaries-5\"></a>Preliminaries</h2>\n<h3><a name=\"shamirs-secret-sharing-6\" class=\"anchor\" href=\"https://ethresear.ch#shamirs-secret-sharing-6\"></a>Shamir’s Secret Sharing</h3>\n<p><a href=\"https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing\" rel=\"noopener nofollow ugc\">Shamir’s Secret Sharing</a> is a method for<br>\ndistributing a secret among a group of participants, each of which is allocated a share of the secret.<br>\nThe secret can be reconstructed only when a sufficient number of shares are combined. The sufficient<br>\nnumber is called the threshold. The threshold can be any number between 1 and the total number of shares.<br>\nThe secret cannot be reconstructed from any number of shares less than the threshold.</p>\n<p>One of the simplest ways to implement Shamir’s Secret Sharing is to use a polynomial of degree N-1. We can<br>\nrepresent the N-sized secret as a polynomial of degree N-1. We can evaluate this polynomial at M points and<br>\nget M values. Then we distribute these values among M participants. The secret can be restored from any<br>\nN values.</p>\n<blockquote>\n<p>The way we use Shamir’s Secret Sharing here can be alternatively characterized as encoding the data<br>\nwith <a href=\"https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction\" rel=\"noopener nofollow ugc\">Reed–Solomon Error Correcting Code</a><br>\nand decoding with erasures (not errors). Especially since the message we encode is not secret. In the following,<br>\nwe keep calling it Secret Sharing because more readers may be familiar with this term.</p>\n</blockquote>\n<p>For well-selected <span class=\"math\">N</span> and <span class=\"math\">M</span>, we can restore the secret if most of the participants will go offline.<br>\nWe will use this property to build a fault-tolerant storage of publicly available data.</p>\n<h3><a name=\"polynomial-computation-for-data-recovery-7\" class=\"anchor\" href=\"https://ethresear.ch#polynomial-computation-for-data-recovery-7\"></a>Polynomial Computation for Data Recovery</h3>\n<p>One can recover a secret shared using Shamir’s scheme using<br>\n<a href=\"https://en.wikipedia.org/wiki/Lagrange_polynomial\" rel=\"noopener nofollow ugc\">Lagrange interpolation</a>,<br>\nwe briefly outline the mechanism below.</p>\n<p>Let’s consider <span class=\"math\">p(x)</span> is a polynomial of degree <span class=\"math\">N-1</span> and the secret is the evaluation representation of<br>\nthis polynomial over evaluation domain <span class=\"math\">\\mathbf{D}=\\{0,\\ 1,\\ 2,\\ ...,\\ N-1\\}</span>:</p>\n<p><span class=\"math\">\\mathbf{S} = \\{p(0),\\ p(1),\\ p(2),\\ ...,\\ p(N-1)\\}.</span></p>\n<p>We will compute the polynomial over the extended evaluation domain <span class=\"math\">0,\\ 1,\\ 2,\\ ...,\\ M-1</span> and<br>\ndistribute the values to M participants.</p>\n<p>Let’s represent the case when all participants excluding N are going offline. So, we get the following<br>\nvalues:</p>\n<p><span class=\"math\">\\mathbf{V} = \\{p(k_0),\\ p(k_1),\\ p(k_2),\\ ...,\\ p(k_{N-1})\\}</span></p>\n<p>over evaluation domain</p>\n<p><span class=\"math\">\\mathbf{K} = \\{k_0,\\ k_1,\\ k_2,\\ ...,\\ k_{N-1}\\}.</span></p>\n<p>Let’s define Lagrange polynomials over evaluation domain <span class=\"math\">\\mathbf{K}</span>:</p>\n<p><span class=\"math\">\\mathbf{L_i}(x) = c_i \\prod_{j \\neq i} \\left(x - k_j\\right),</span></p>\n<p>where <span class=\"math\">c_i</span> is a constant coefficient, so that <span class=\"math\">\\mathbf{L}_i(k_i) = 1</span>.</p>\n<p>Let’s define matrix <span class=\"math\">\\mathbf{L}_{ij}=\\mathbf{L}_i(j)</span>.</p>\n<p>Then the secret can be restored as follows:</p>\n<p><span class=\"math\">\\mathbf{S_j} = \\sum_{i} \\mathbf{V_i} \\cdot L_{ij}</span></p>\n<blockquote>\n<p>What happens if some of the participants are malicious and send incorrect values?<br>\nThere is more than one way to solve this.<br>\nFor our partial case, we will merkelize all values and distribute them to all participants with Merkle proofs.<br>\nThen we can check the correctness of each value,<br>\nchecking the root of the Merkle proof.<br>\nIf the root is incorrect, we can ignore the value.<br>\nIn terms of error-correcting codes, this corresponds to an erasure.</p>\n</blockquote>\n<h3><a name=\"zksnarks-8\" class=\"anchor\" href=\"https://ethresear.ch#zksnarks-8\"></a>zkSNARKs</h3>\n<p><a href=\"https://en.wikipedia.org/wiki/Non-interactive_zero-knowledge_proof\" rel=\"noopener nofollow ugc\">Zero-Knowledge Succinct Non-Interactive Argument of Knowledge</a><br>\nis a cryptographic primitive that lets a Prover convince a Verifier that it knows a secret witness <span class=\"math\">y</span><br>\nsuch that <span class=\"math\">P(x, y)</span> for public polynomially-computable predicate <span class=\"math\">P</span> and public instance value <span class=\"math\">x</span>.</p>\n<p>They are heavily used to provide privacy for the data a blockchain works with.<br>\nIn this architecture, a smart contract on blockchain only holds a commitment to its state,<br>\nand clients initiate transactions asking to update that hash providing the zkSNARK proof of the transition being done correctly.<br>\nThis way, the state held by the smart contract (or some parts of such state) can remain private,<br>\nwhile still ensuring that state transition happens according to some rules.</p>\n<p>Another use-case for zkSNARKs is CPU scaling of blockchain.<br>\nzkSNARKs allow verifying the proof faster than the computation of predicate <span class=\"math\">P</span> would take.<br>\nThis way, if verifying state transition requires too many resources,<br>\nwe can use recursion and only verify the final result on the blockchain.</p>\n<p>We use zkSNARKs in this solution for both.<br>\nIn the following description, we often make the use of zkSNARKs implicit.</p>\n<h3><a name=\"polynomial-commitment-schemes-9\" class=\"anchor\" href=\"https://ethresear.ch#polynomial-commitment-schemes-9\"></a>Polynomial Commitment Schemes</h3>\n<p>Polynomial Commitment Schemes are <a href=\"https://en.wikipedia.org/wiki/Commitment_scheme\" rel=\"noopener nofollow ugc\">commitment schemes</a><br>\nwhere one can commit to a polynomial of a fixed degree,<br>\nand then reveal individual points of that polynomial<br>\nand prove that the degree of the polynomial committed to is limited.</p>\n<h4><a name=\"selection-of-polynomial-commitment-scheme-10\" class=\"anchor\" href=\"https://ethresear.ch#selection-of-polynomial-commitment-scheme-10\"></a>Selection of Polynomial Commitment Scheme</h4>\n<p>We propose using FRI, because it is not additive-homomorphic, and it is more suitable for<br>\nour case. The usage of additive-homomorphic commitment schemes could lead to attacks when<br>\nthe malicious nodes use MPC to compute proofs of data availability without storing all data.</p>\n<p>To build a random proof of random opening of the polynomial commitment, the prover should<br>\nkeep all the data. Other nodes cannot help him to compute this proof with the MPC procedure.</p>\n<h3><a name=\"space-time-tradeoff-and-plotting-11\" class=\"anchor\" href=\"https://ethresear.ch#space-time-tradeoff-and-plotting-11\"></a>Space-time Tradeoff and Plotting</h3>\n<p>For proof of space-time mining, we need to build a plot which is an array of high entropy data,<br>\nand computing any one element of this array without storing the whole array should be a hard<br>\nproblem.</p>\n<p>The approach how to build plots is described at <a href=\"https://eprint.iacr.org/2017/893.pdf\" rel=\"noopener nofollow ugc\">AACKPR2017</a>.</p>\n<p>To build the plot, let’s define</p>\n<p><span class=\"math\">f_1(x)=h(x),</span></p>\n<p><span class=\"math\">f_{i+1}(x) = h(x, x_1),</span> where</p>\n<p><span class=\"math\">|f_i(x)+f_i(x_1)| &lt; s_0,</span></p>\n<p><span class=\"math\">x_1 = 0 \\mod s_1,</span></p>\n<p><span class=\"math\">h</span> is a hash function.</p>\n<p>At <a href=\"https://eprint.iacr.org/2017/893.pdf\" rel=\"noopener nofollow ugc\">AACKPR2017</a> it is shown that the space-time tradeoff formula for <span class=\"math\">f_n</span> takes the form</p>\n<p><span class=\"math\">S^n T = O(N^n)</span>.</p>\n<p>If n is big enough, it is optimal for a server to store all data.</p>\n<p>To perform spacetime proof, the node receives a random challenge <span class=\"math\">c</span> and should find a <span class=\"math\">s_0</span>-close preimage <span class=\"math\">x_c</span> of <span class=\"math\">f_n</span>:</p>\n<p><span class=\"math\">|f_n(x_c)-c| &lt; s_0</span>, and also provide all computations of <span class=\"math\">f_n(x_c)</span>.</p>\n<p>Proof complexity is growing as <span class=\"math\">O(2^n)</span>, so in practice, it is useful to build proof for <span class=\"math\">k=7</span> or <span class=\"math\">k=8</span> (<a href=\"https://www.chia.net/wp-content/uploads/2022/09/Chia_Proof_of_Space_Construction_v1.1.pdf\" rel=\"noopener nofollow ugc\">Chia proof of space construction</a>). That means that if the node stores twice less data then it should compute 128 or 256 times more hashes to provide the proof.</p>\n<p>Proofs with bigger <span class=\"math\">k</span> could be used inside zkSNARKs.</p>\n<h2><a name=\"architecture-12\" class=\"anchor\" href=\"https://ethresear.ch#architecture-12\"></a>Architecture</h2>\n<p>In this section, we first give a high-level overview of the proposed L1-L3 architecture.<br>\nThen describe the commissioning and decommissioning of L3 pools.<br>\nFinally, we discuss plotting,<br>\nthe mechanism nodes use to prove that they have enough space to store the shards.</p>\n<h3><a name=\"overview-13\" class=\"anchor\" href=\"https://ethresear.ch#overview-13\"></a>Overview</h3>\n<p>Consider a 4-level model of the sharded storage network illustrated below.</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/2X/1/1ab0332f7dab1781881337c04b8cf2b3ac63d00b.svg\" alt=\"Architecture\" data-base62-sha1=\"3O5V5OXiPzKXlhA9hfeD8uZGzKr\" width=\"690\" height=\"419\"></p>\n<p>At the first level, we have the L1 blockchain.<br>\nThe L2 rollup publishes state-to-state transition proofs<br>\nand the root hash of the state on the L1 blockchain.</p>\n<blockquote>\n<p>We do not need to publish the data of blocks. We are describing sharded storage, so, all data will<br>\nbe safely stored at the nodes and the zk proof contains the proof of data availability.</p>\n</blockquote>\n<p>At the second level, we have the L2 rollup.<br>\nIt checks proofs of space-time for new nodes,<br>\nadds it to the list of active nodes, removes inactive nodes,<br>\nand performs mixing of nodes between pools to prevent potential attacks.<br>\nAlso, state-to-state transition proofs for L3 rollups are published here.</p>\n<p>At the third level, we have the L3 rollup.<br>\nThe sharding means that we need to convert the data into <span class=\"math\">n</span> shards when <span class=\"math\">k\\leq n</span> shards are enough to restore the data.<br>\nThe L3 rollup is responsible for consistency between all nodes.<br>\nAlso, users rent space at the L3 rollup using their payment bridges.<br>\nL3 rollup aggregates proof of the data availability using function interpolation at random points for data blocks.</p>\n<p>The L3 rollups run their consensus protocols<br>\n(e.g. using <a href=\"https://en.wikipedia.org/wiki/Proof_of_authority\" rel=\"noopener nofollow ugc\">Proof of authority</a>),<br>\nand members of the consensus are the nodes of the corresponding pool.<br>\nThey can perform their operations and maintain their state by synchronizing every step with the L1 blockchain,<br>\nand only referring to when they explicitly choose to (e.g. to save their state hash).</p>\n<p>Users and smart contracts can rent space for the tokens with the L3 rollup.<br>\nSo, the set of all L3 rollups is working as a very big decentralized HDD with CRUD operations on sectors of this disk.</p>\n<p>At the fourth level, we have storage nodes. The nodes are part of the consensus for the corresponding<br>\npool. Also, the nodes store the data and provide proof of data availability. All space of<br>\nthe nodes should be filled with special plots, like in Chia Network, but with some differences,<br>\nmaking it more suitable for our case and ZK-friendly.</p>\n<h3><a name=\"commissioning-and-decommissioning-of-l3-pools-14\" class=\"anchor\" href=\"https://ethresear.ch#commissioning-and-decommissioning-of-l3-pools-14\"></a>Commissioning and Decommissioning of L3 pools</h3>\n<p>Nodes can join and leave the pool at any time.<br>\nThe L2 rollup is responsible for commissioning and decommissioning L3 pools<br>\ndepending on the number of unallocated nodes and the amount of data in the pools.</p>\n<h4><a name=\"commissioning-15\" class=\"anchor\" href=\"https://ethresear.ch#commissioning-15\"></a>Commissioning</h4>\n<p><img src=\"https://ethresear.ch/uploads/default/original/2X/7/77872ededb46880dbc4b3836eaf37fadc41dc1da.svg\" alt=\"Commissioning\" data-base62-sha1=\"h3orJRectJKc3ShVZ4FEv1njdxM\" width=\"690\" height=\"495\"></p>\n<p>Commissioning of the pool is a simple process:<br>\nthe L2 rollup selects a random set of nodes from existing nodes in other pools<br>\nand replaces them with unallocated nodes.<br>\nWhen the number of nodes in a new pool reaches a level with enough security,<br>\nthe pool is commissioned and can accept new files for storage.</p>\n<h4><a name=\"decommissioning-16\" class=\"anchor\" href=\"https://ethresear.ch#decommissioning-16\"></a>Decommissioning</h4>\n<p><img src=\"https://ethresear.ch/uploads/default/original/2X/7/72662c68d033b46b2f37a8e365d6e706cd0cf31d.svg\" alt=\"Decommissioning\" data-base62-sha1=\"gk1kUxJwuEpdzMW1d6e9RfDSYax\" width=\"639\" height=\"500\"></p>\n<p>Decommissioning is a more complex procedure.<br>\nAt first, the L2 rollup selects two pools with a low percentage of rented space (both should be <span class=\"math\">&lt;50\\%</span> full).<br>\nThen it moves all data from one pool to another.<br>\nAfter that, the nodes of the empty pool are considered to be unallocated and the pool is removed from the list of active pools.</p>\n<blockquote>\n<p>This solution works well if the data blocks are distributed amoung pools unequally,<br>\ni.e. as many pools as possible are fully filled.<br>\nThis means that fewer pools and fewer nodes are in use,<br>\nand the resources of those nodes are utlizied to the fullest.</p>\n<p>This is similar to disk defragmentation problem:<br>\nthe placement of data on the disk impacts performance,<br>\nand we would like to distribute it in a way that is more efficient.<br>\nThe difference is that our disk is very big and decentralized,<br>\nand the user renting space is freely choosing the pool to put her data in<br>\n(using L2 consensus to assign pools for each file upload will not scale well).<br>\nTherefore, we address this problem by designing a special economic model,<br>\nto incentivize the users to utilize the pools already in use to their fullest before touching fresh ones.<br>\nWe assign each pool a fee rate that depends on the percentage of currently rented space.<br>\n<a href=\"https://ethresear.ch#economic-model-25\">Economic Model</a> section describes it in more detail.</p>\n</blockquote>\n<p>When a pool gets decommissioned, the nodes move the data to a new pool without any confirmation from the users owning that data.<br>\nThe whole procedure needs no interaction from the user, she can be offline the whole time.<br>\nLater, when the user comes online and wants to retrieve her data,<br>\nshe can look at L2 records<br>\n(L2 has the records since it decided to decommission the pool),<br>\nfigure out what pool currently stores her data<br>\nand retrieve it from there.</p>\n<h3><a name=\"polynomial-representation-of-the-data-17\" class=\"anchor\" href=\"https://ethresear.ch#polynomial-representation-of-the-data-17\"></a>Polynomial Representation of the Data</h3>\n<p>Let’s consider <span class=\"math\">F_i</span> as an N-sized array of data we need to store. We can represent it as table<br>\n<span class=\"math\">F_{ij}</span> with <span class=\"math\">M</span> rows and <span class=\"math\">K</span> columns, <span class=\"math\">N=M \\cdot K</span>.</p>\n<p>Then we can represent the table as bivariate polynomial <span class=\"math\">F(x,y)</span> of degree <span class=\"math\">M-1</span> over <span class=\"math\">x</span> and<br>\ndegree <span class=\"math\">K-1</span> over <span class=\"math\">y</span>:</p>\n<p>It can be noted that <span class=\"math\">F(x,y_0)</span> represents the linear combination of the columns of the table.<br>\nTo distribute the data to <span class=\"math\">K_1</span> nodes, we can evaluate the polynomial at <span class=\"math\">K_1</span> <span class=\"math\">y</span> points and<br>\ndistribute the values to the nodes.</p>\n<p>We can verify the following polynomial equation using the polynomial commitment scheme:</p>\n<p><span class=\"math\">F(x,x^M)-F(x,y_0) = (x^M-y_0) \\cdot Q(x),</span></p>\n<p>where <span class=\"math\">Q(x)</span> is a quotient polynomial.</p>\n<h3><a name=\"plotting-18\" class=\"anchor\" href=\"https://ethresear.ch#plotting-18\"></a>Plotting</h3>\n<p>To prevent spam from malicious nodes with not enough space, we should implement an efficient mechanism,<br>\nallowing nodes to prove, that they have enough space to store the data.</p>\n<p>We use the technique described in <a href=\"https://ethresear.ch#space-time-tradeoff-and-plotting-11\">Space-Time Tradeoff</a> to achieve this.</p>\n<h3><a name=\"state-of-the-node-and-data-availability-mining-19\" class=\"anchor\" href=\"https://ethresear.ch#state-of-the-node-and-data-availability-mining-19\"></a>State of the Node and Data Availability Mining</h3>\n<p>Each data sector of the node could be represented as a polynomial.<br>\nThe node can store all polynomial commitments inside the Merkle tree.<br>\nThen proof of data availability could be computed as a set of random openings of the polynomial commitments at random points.<br>\nChallenge values for the openings and commitment selection could be derived from the timestamps of the blocks of the L2 rollup.<br>\nThe proofs of data availability can be compressed using recursive zkSNARKs.</p>\n<h2><a name=\"cryptographic-analysis-20\" class=\"anchor\" href=\"https://ethresear.ch#cryptographic-analysis-20\"></a>Cryptographic Analysis</h2>\n<h3><a name=\"security-model-21\" class=\"anchor\" href=\"https://ethresear.ch#security-model-21\"></a>Security Model</h3>\n<p>In our security model, we assume that at least 50% of the nodes on the network are honest, and the L1-2 consensus is secure,<br>\ni.e. the L1 and L2 layers of our network are uncorrupted.<br>\nThe only thing that an adversary is allowed to do is spawn malicious nodes<br>\n(no more than 50% of the network).<br>\nThe malicious nodes are allowed not to follow the prescribed protocol but can deviate from it as chosen by the adversary.</p>\n<p>Honest nodes are assumed to not deviate from the protocol<br>\nunless that lets them earn more (of L1 or L2 tokens that users pay for renter space) than honest behavior would.</p>\n<h3><a name=\"statistical-security-analysis-22\" class=\"anchor\" href=\"https://ethresear.ch#statistical-security-analysis-22\"></a>Statistical Security Analysis</h3>\n<p>Let’s consider <span class=\"math\">p</span> as part of honest nodes in the network, So, if a total number of nodes is <span class=\"math\">N</span>,<br>\n<span class=\"math\">pN</span> are honest, and <span class=\"math\">(1-p)N</span> of them are malicious. If shards are distributed by nodes by random,<br>\n<span class=\"math\">p</span> also is the probability, that the node will be honest. Then if we have <span class=\"math\">n</span> shards with threshold<br>\n<span class=\"math\">k</span>, the probability that the secret cannot be restored means that only strictly less than <span class=\"math\">k</span> shards<br>\nare stored by honest nodes. The probability is defined by the following binomial distribution:</p>\n<p><span class=\"math\">\\mathbf{P}(p,n,k) = \\sum_{i=0}^{k-1} \\binom{n}{i} p^i (1-p)^{n-i},</span></p>\n<p>where</p>\n<p><span class=\"math\">\\binom{n}{i} = \\frac{n!}{i!(n-i)!}</span></p>\n<p>is a binomial coefficient.</p>\n<p>For <span class=\"math\">0.05 &lt; p &lt; 0.95</span>, <span class=\"math\">n&gt;30</span>, <span class=\"math\">np&gt;5</span>, <span class=\"math\">n(1-p)&gt;5</span>, we can use the normal approximation of<br>\nthe binomial distribution (<a href=\"https://online.stat.psu.edu/stat414/lesson/28/28.1\" rel=\"noopener nofollow ugc\">source</a>).</p>\n<p><span class=\"math\">\\mathbf{P}(p,n,k) \\approx \\frac{1}{2} \\left[1 + \\mathrm{erf}\\left(\\frac{k-1/2-np}{\\sqrt{2np(1-p)}}\\right)\\right].</span></p>\n<p>The bits of statistical security of this solution could be defined as follows:</p>\n<p><span class=\"math\">\\mathbf{S}(p,n,k) = -\\log_2 \\mathbf{P}(p,n,k).</span></p>\n<p>Then we can calculate the statistical security for different values of <span class=\"math\">p</span>, <span class=\"math\">n</span>, and <span class=\"math\">k</span>.<br>\nFor example, if <span class=\"math\">p=1/2</span>, <span class=\"math\">n=1024</span>, <span class=\"math\">k=256</span>,</p>\n<p>then <span class=\"math\">\\mathbf{S}(1/2,1024,256) = 190</span> bits of security.</p>\n<h3><a name=\"complexity-leveling-and-protection-against-centralized-supernodes-23\" class=\"anchor\" href=\"https://ethresear.ch#complexity-leveling-and-protection-against-centralized-supernodes-23\"></a>Complexity Leveling and Protection Against Centralized Supernodes</h3>\n<p>Let’s consider the following two attack vectors:</p>\n<ol>\n<li>\n<p>Files have different entropy. Low entropy files are easier to store. But the reward is<br>\nthe same. That means that malicious nodes can generate large entropy files and store them<br>\non small disk drives. To prevent this attack, we need to level the complexity of the data.</p>\n</li>\n<li>\n<p>Also, the nodes can collude and store all the data on one node. If we use k-of-n sharding,<br>\nthis one supernode can store only source data, which has the same complexity as storing k<br>\nshards only. This is not safe. To prevent this attack, we need to make the same<br>\ncomplexity for decentralized and centralized cases, so the nodes will not have any profit<br>\nfrom centralization.</p>\n</li>\n</ol>\n<p>All these attacks could be prevented with the following approach:</p>\n<p>Each node generates a high entropy plot and commits to function <span class=\"math\">G</span>, which is very<br>\nclose to this plot. This fact could be verified with random openings of the polynomial:</p>\n<p><span class=\"math\">G(x_i) = \\text{plot}(x_i)</span></p>\n<p>If we perform enough random openings, we can be sure that the entropy of <span class=\"math\">G</span> is high enough.</p>\n<p>The seed of the plot should be derived from the commitment of the shard. Then the node can store<br>\nthe sum of the shard and plot and provide proof of data availability for this sum to receive<br>\nthe reward.</p>\n<p><span class=\"math\">F'(x) = F(x, y_0) + G(x)</span></p>\n<p>So, minimal storage complexity for all nodes and one malicious supernode is the same, and<br>\ncomplexity leveling is achieved: it is enough hard to store the array of zeros and the array<br>\nof random values.</p>\n<p>If <span class=\"math\">G</span> is not exactly equal to the plot, it does not matter. When the network recovers the data,<br>\nthe honest node can restore the initial data by itself or send the deterministic script on how<br>\nto do it.</p>\n<h3><a name=\"dynamic-nodes-mixing-against-malicious-actors-24\" class=\"anchor\" href=\"https://ethresear.ch#dynamic-nodes-mixing-against-malicious-actors-24\"></a>Dynamic Nodes Mixing against Malicious Actors</h3>\n<p>If sharding was static over time,<br>\nwe would need just initially select the nodes for each pool.<br>\nHowever the uptime of the nodes is not infinite,<br>\nand as honest nodes go offline (for natural reasons),<br>\nthe adversary could use this to concentrate its malicious nodes in a given pool.<br>\nIf only malicious nodes in a given pool reach a critical amount,<br>\nthey can cause DoS and lose the data.<br>\nTo prevent this problem,<br>\nwe need to mix the nodes in the pools periodically.<br>\nThe mixing should be done in a way,<br>\nthat the malicious nodes cannot predict the new shard for the data.</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/2X/d/d79ee25c1598642029f46f1a0159e2041bad7b6e.svg\" alt=\"Mixings\" data-base62-sha1=\"uLt3Y42EJyxZgnCmHZSCCOpFDX0\" width=\"617\" height=\"499\"></p>\n<p>Let’s consider <span class=\"math\">n</span> as the number of nodes in a pool.</p>\n<p>Each time an honest node leaves the pool, the network performs the following:</p>\n<ol>\n<li>\n<p>Select a random node (from an unallocated or another pool), and move it to the current pool.<br>\nIf the selected node was previously in another pool, move a random unallocated node in place of it.</p>\n</li>\n<li>\n<p>Perform “mixing” <span class=\"math\">m</span> times:<br>\nselect a random node from the current pool<br>\nand swap it with a random node from outside of this pool<br>\n(from unallocated or another pool).</p>\n</li>\n</ol>\n<p>During step 1, the number of malicious nodes in the pool will go up by <span class=\"math\">1</span> with probability <span class=\"math\">p</span>.<br>\nBut then, each mixing will probabilistically balance the number of malicious nodes inside the pool with the number outside.<br>\nThe current pool will also be impacted by steps 1-2 being triggered in the other pools when some node leaves that pool;<br>\nwe assume that the node leaving the pool is honest (and the attacker is waiting til a lot of honest nodes leave the pool).</p>\n<p>In our security model, the best strategy for the adversary is this:</p>\n<ul>\n<li>keep the malicious nodes in the pool online,</li>\n<li>wait for some honest node of that pool to go offline.</li>\n</ul>\n<p>We can describe the evolution of the pool as a Markov process.<br>\nTo protect from this strategy,<br>\nthe network performs <span class=\"math\">m</span> mixings.<br>\nWe can find the equilibrium distribution for this process<br>\nand find the probability that less than <span class=\"math\">k</span> nodes in the pool are honest.</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/2X/2/2052a5ebb746aee4dcae4babcc561158e0f5cc53.svg\" alt=\"Soundness\" data-base62-sha1=\"4BWlOwaqQiJBzbrCbrDY5QVIHYL\" width=\"461\" height=\"346\"></p>\n<p>For example, if <span class=\"math\">p=1/2</span>, <span class=\"math\">k=64</span>, <span class=\"math\">n=512</span>, <span class=\"math\">m=3</span>, then this solution achieves <span class=\"math\">115</span> bits of statistical security.</p>\n<h2><a name=\"economic-model-25\" class=\"anchor\" href=\"https://ethresear.ch#economic-model-25\"></a>Economic Model</h2>\n<p>All nodes receive the same reward for storing the data or maintaining the free space, which is the same complexity due to <a href=\"https://ethresear.ch#complexity-leveling-and-protection-against-centralized-supernodes-23\">complexity leveling</a>.</p>\n<p>The first source of rewards is token emission with a Bitcoin-like formula. Rewards are distributed to the nodes using <a href=\"https://ethresear.ch#space-time-tradeoff-and-plotting-11\">proof of space-time mining</a>, like in the Chia Network.</p>\n<p>Another source of rewards is the fee for space rent.</p>\n<p>The fee should depend on:</p>\n<ol>\n<li>\n<p>Percentage of rented space.<br>\nThe more space is rented, the higher the fee.<br>\nThe dependency should be hyperbolic,<br>\nso the fee will be very high when the pool is almost full.<br>\nThen if somebody wants to rent all free space, the fee will grow very fast.</p>\n<p><span class=\"math\">\\phi = O\\left(\\Psi^{-1}\\right),</span></p>\n<p>where <span class=\"math\">\\Psi</span> is part of free space in the whole network, <span class=\"math\">\\phi</span> is a fee.</p>\n</li>\n<li>\n<p>Percentage of rented space.<br>\nIf more space is rented,<br>\nthe multiplier is exponentially growing over time.<br>\nIf less space is rented,<br>\nthe multiplier is exponentially decreasing over time.</p>\n<p><span class=\"math\"> \\mathbf{d} \\phi = \\phi\\cdot(\\Psi - \\beta)\\cdot \\gamma \\mathbf{d}t.</span></p>\n<p>The solution of this equation is</p>\n<p><span class=\"math\">\\phi = O\\left(\\exp(\\gamma\\int\\limits_0^t (\\Psi(t)-\\beta) \\mathbf{d}t)\\right).</span></p>\n</li>\n<li>\n<p>Percentage of rented space of the pool.<br>\nThe more space is rented, the lower the fee.<br>\nThe dependency should be linear.</p>\n<p><span class=\"math\">\\phi = O\\left(\\alpha - \\psi\\right),</span></p>\n<p>where <span class=\"math\">\\psi</span> is part of the free space in the current pool.</p>\n</li>\n</ol>\n<p>The resulting formula takes the form:</p>\n<p><span class=\"math\">\\phi = K \\cdot \\frac{\\alpha - \\psi}{\\Psi} \\cdot \\exp\\left(\\gamma \\int\\limits_0^t (\\Psi(t)-\\beta) \\mathbf{d}t\\right)</span></p>\n<p>Also, to make the network more stable, we propose the following mechanism:</p>\n<ol>\n<li>\n<p>Each node instantly receives only part of the reward.<br>\nThe rest of the reward is locked for some time.<br>\nIf the node goes offline, the locked reward is burned.</p>\n</li>\n<li>\n<p>Prolonging the rent of the existing space should be cheaper than renting a new space by some discount factor.</p>\n</li>\n</ol>\n<p>During the setup of the network,<br>\na significant part of rewards should be generated by the mining.<br>\nThen, when the network is stable,<br>\nthe fee for space rent should be the main source of rewards.</p>\n<h2><a name=\"evaluation-26\" class=\"anchor\" href=\"https://ethresear.ch#evaluation-26\"></a>Evaluation</h2>\n<h3><a name=\"comparison-with-replication-27\" class=\"anchor\" href=\"https://ethresear.ch#comparison-with-replication-27\"></a>Comparison with Replication</h3>\n<p>Replication is a partial case of sharding when threshold <span class=\"math\">k=1</span>. We compute soundness for replication and sharding with different blowup factors and different levels of security and compare the results.</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>Blowup</th>\n<th><span class=\"math\">p</span></th>\n<th><span class=\"math\">k=128</span></th>\n<th><span class=\"math\">k=64</span></th>\n<th><span class=\"math\">k=32</span></th>\n<th><span class=\"math\">k=1</span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>4</td>\n<td>0.25</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1.3</td>\n</tr>\n<tr>\n<td>8</td>\n<td>0.25</td>\n<td>0.1</td>\n<td>0.2</td>\n<td>0.3</td>\n<td>2.1</td>\n</tr>\n<tr>\n<td>16</td>\n<td>0.25</td>\n<td>15.6</td>\n<td>9.2</td>\n<td>5.8</td>\n<td>3.1</td>\n</tr>\n<tr>\n<td>32</td>\n<td>0.25</td>\n<td>90.8</td>\n<td>47.5</td>\n<td>25.7</td>\n<td>4.4</td>\n</tr>\n<tr>\n<td>64</td>\n<td>0.25</td>\n<td>274.1</td>\n<td>139.6</td>\n<td>72.2</td>\n<td>6.5</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.38</td>\n<td>1.4</td>\n<td>1.3</td>\n<td>1.3</td>\n<td>2.3</td>\n</tr>\n<tr>\n<td>8</td>\n<td>0.38</td>\n<td>59.6</td>\n<td>31.8</td>\n<td>17.8</td>\n<td>3.8</td>\n</tr>\n<tr>\n<td>16</td>\n<td>0.38</td>\n<td>261.9</td>\n<td>133.7</td>\n<td>69.4</td>\n<td>6.3</td>\n</tr>\n<tr>\n<td>32</td>\n<td>0.38</td>\n<td>731.5</td>\n<td>369.1</td>\n<td>187.6</td>\n<td>10.8</td>\n</tr>\n<tr>\n<td>64</td>\n<td>0.38</td>\n<td>1717.1</td>\n<td>862.3</td>\n<td>434.6</td>\n<td>19.1</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.5</td>\n<td>35.1</td>\n<td>19.4</td>\n<td>11.4</td>\n<td>3.5</td>\n</tr>\n<tr>\n<td>8</td>\n<td>0.5</td>\n<td>224.7</td>\n<td>115.2</td>\n<td>60.2</td>\n<td>6</td>\n</tr>\n<tr>\n<td>16</td>\n<td>0.5</td>\n<td>701.1</td>\n<td>354</td>\n<td>180.3</td>\n<td>10.7</td>\n</tr>\n<tr>\n<td>32</td>\n<td>0.5</td>\n<td>1729.5</td>\n<td>868.8</td>\n<td>438.2</td>\n<td>19.6</td>\n</tr>\n<tr>\n<td>64</td>\n<td>0.5</td>\n<td>3845</td>\n<td>1926.9</td>\n<td>967.7</td>\n<td>36.9</td>\n</tr>\n</tbody>\n</table>\n</div><p>From the modeling, we can observe:</p>\n<ol>\n<li>The blowup factor for replication is much higher than for sharding</li>\n<li>The blowup factor for sharding is growing slower than for replication when security<br>\nis growing</li>\n<li>The blowup factor depends on the sharding threshold <span class=\"math\">k</span>, the higher the threshold,<br>\nthe lower the blowup factor</li>\n</ol>\n<h2><a name=\"conclusion-28\" class=\"anchor\" href=\"https://ethresear.ch#conclusion-28\"></a>Conclusion</h2>\n<p>This article has presented a novel sharded storage solution leveraging blockchain technology to tackle the significant challenge of scaling storage for vast data volumes, reaching beyond petabytes. By integrating Shamir’s Secret Sharing and Fast Fourier Transform, we have developed a framework that not only surpasses the limitations of current replication-based methods but also seamlessly integrates with zkSNARK-friendly environments. This enables the secure and efficient storage of data with the cost-effectiveness of Web2 solutions while maintaining the robust security features characteristic of Web3 applications.</p>\n<p>Our proposed architecture redefines the concept of data storage and retrieval within blockchain networks, offering a scalable, fault-tolerant solution that significantly reduces the necessity for on-chain data storage. This advancement allows for the transformation of validiums into rollups, thereby enhancing their utility and efficiency. The economic model underpinning our solution ensures a fair and incentivized participation of nodes, thereby promoting a healthy and dynamic ecosystem conducive to the long-term sustainability of the network.</p>\n<p>Comparative analyses have highlighted the superiority of our sharded storage solution over traditional replication methods, demonstrating a significant reduction in the blowup factor required to achieve comparable levels of security. Furthermore, our approach to dynamic node mixing and space-time tradeoffs introduces a robust mechanism against potential malicious activities, ensuring the integrity and availability of data within the network.</p>\n<p>The theoretical framework presented herein lays a solid foundation for future research and development in the field of blockchain storage solutions. It opens new avenues for the application of blockchain technology in areas previously constrained by storage limitations, such as large-scale data backups, content delivery networks, and decentralized applications requiring extensive data storage capabilities.</p>\n<p>In conclusion, the Blockchain Sharded Storage solution represents a significant leap forward in the quest for scalable, secure, and cost-effective data storage on the blockchain. It addresses the critical challenges faced by current blockchain infrastructures, offering a viable pathway towards the realization of truly decentralized, efficient, and secure data storage systems. As we continue to explore and refine this technology, it is poised to become a cornerstone in the development of next-generation blockchain applications, furthering the integration of blockchain technology into mainstream use cases and marking a pivotal moment in the evolution of decentralized data storage.</p>\n<h3><a name=\"future-directions-29\" class=\"anchor\" href=\"https://ethresear.ch#future-directions-29\"></a>Future Directions</h3>\n<p>In the future, we plan to do a more thorough economic modeling of this solution as well as do cryptographic security analysis in one of the established formal frameworks.</p>\n<p><em>Crossposting with <a href=\"https://github.com/zeropoolnetwork/sharded-storage/blob/9b4f7ef7b3e286a64dca59f01fa348b781dd3509/article/main.md\" rel=\"noopener nofollow ugc\">zeropoolnetwork/sharded-storage</a></em>.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/blockchain-sharded-storage-web2-costs-and-web3-security-with-shamir-secret-sharing/18881\">Read full topic</a></p>","link":"https://ethresear.ch/t/blockchain-sharded-storage-web2-costs-and-web3-security-with-shamir-secret-sharing/18881","pubDate":"Wed, 06 Mar 2024 12:14:02 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-18881"},"source":{"@url":"https://ethresear.ch/t/blockchain-sharded-storage-web2-costs-and-web3-security-with-shamir-secret-sharing/18881.rss","#text":"Blockchain Sharded Storage: Web2 Costs and Web3 Security with Shamir Secret Sharing"},"filter":false},{"title":"Burnth/WormCash: A practical implementation of EIP-7503 on Ethereum","dc:creator":"keyvank","category":"Privacy","description":"<p>By Nobitex Labs (<a class=\"mention\" href=\"https://ethresear.ch/u/keyvank\">@keyvank</a>, <a class=\"mention\" href=\"https://ethresear.ch/u/ostadgeorge\">@ostadgeorge</a> et al)</p>\n<p>We are thrilled to introduce our proof-of-concept implementation of EIP-7503, Burnth, which simply is an ERC-20 smart-contract that can be minted by providing proofs-of-burn. We are using Circom/SnarkJS for implementing our circuits and building zk-proofs.</p>\n<p>Our circuit simply checks if there is an account with an unspendable address in the stateRoot of a block, by verifying a Merkle-Patricia-Trie proof inside a R1CS circuit. We use a modified version of MPT proof verifier which significantly reduces the number of constraints needed. We are also not verifying the entire MPT proof in a single circuit, but we are chaining some subcircuits together by commiting into intermediary layers, and checking if the commitments (Which are fed as public-inputs) are chained together. This results in two Groth16 circuits, with parameter files of size around 500MB, and it takes around 1 minute to generate a private-proof-of-burn on a laptop.</p>\n<p>You can find the codes here: <a href=\"https://github.com/nobitex/burnth\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - nobitex/burnth: Ether, but burnt</a><br>\nFor more info on EIP-7503 itself: <a href=\"https://eip7503.org\" rel=\"noopener nofollow ugc\">https://eip7503.org</a></p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/burnth-wormcash-a-practical-implementation-of-eip-7503-on-ethereum/18875\">Read full topic</a></p>","link":"https://ethresear.ch/t/burnth-wormcash-a-practical-implementation-of-eip-7503-on-ethereum/18875","pubDate":"Tue, 05 Mar 2024 12:25:37 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-18875"},"source":{"@url":"https://ethresear.ch/t/burnth-wormcash-a-practical-implementation-of-eip-7503-on-ethereum/18875.rss","#text":"Burnth/WormCash: A practical implementation of EIP-7503 on Ethereum"},"filter":false},{"title":"Why wait a week? Fast Finality Optimistic Rollups","dc:creator":"Mikerah","category":"Layer 2","description":"<p>Author: <a class=\"mention\" href=\"https://ethresear.ch/u/silentcicero\">@SilentCicero</a><br>\nReviewers: <a class=\"mention\" href=\"https://ethresear.ch/u/pixelcircuits\">@pixelcircuits</a>, <a class=\"mention\" href=\"https://ethresear.ch/u/mikerah\">@Mikerah</a>, <a class=\"mention\" href=\"https://ethresear.ch/u/brapse\">@brapse</a>. Special thanks to L2Beat and Justin Drake.</p>\n<p>Crost post from <a href=\"https://forum.fuel.network/t/why-wait-a-week-fast-finality-optimistic-rollups/4471/1\" rel=\"noopener nofollow ugc\">here</a>.</p>\n<p>Optimistic Rollups (“ORs”) have become a de facto scaling technique for Ethereum. ORs enable Ethereum grade security so long as a single honest minority is validating and can submit fraud proofs back to Ethereum. However, ORs come with a fairly significant short-coming, a long finalisation window (conventionally 7 days) due to potential mass censorship attacks on Ethereum [1]. In this post, we will explore a censorship-resistant check-in based model for ORs which aims to safely reduce that window down to less than a few Ethereum blocks while retaining only an honest minority assumption.</p>\n<p><strong>Definitions</strong>:</p>\n<ul>\n<li><strong>State Transition Function</strong>: the mechanism which honest actors use to progress chain state into future.</li>\n<li><strong>Validator</strong>: an actor running validation software containing the state transition function.</li>\n<li><strong>Honest Minority</strong>: at least one actor running validation software and correctly responding to events.</li>\n<li><strong>Censorship</strong>: the prevention of transactions from state transitioning a blockchain.</li>\n<li><strong>Fraud</strong>: a state transition outside the state transition function (including bugs or faults).</li>\n</ul>\n<p><strong>Censorship Assumptions</strong>:</p>\n<ul>\n<li>For the purposes of this post, we must assume an adversary to be an extremely well funded entity that has billions of dollars at their disposal and that 2/3 of Ethereum validators can be entirely co-opted for at least a moderate window of time (several hours to many days) to exclude specific or all transactions (either by filling up Ethereum blocks or taking over a large portion of the validator set) [1-7].</li>\n<li>We also assume intentional censorship is extremely hard to detect and likely impossible to prove with on-chain mechanisms.</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/c/ce931fb05198139169b564fcdc3f2e007b550354.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/ce931fb05198139169b564fcdc3f2e007b550354\" title=\"1\"><img src=\"https://ethresear.ch/uploads/default/original/2X/c/ce931fb05198139169b564fcdc3f2e007b550354.jpeg\" alt=\"1\" data-base62-sha1=\"ttrzlIBgc7ucNrPRnGToC2npaSM\" width=\"586\" height=\"500\" data-dominant-color=\"EBEBEC\"></a></div><p></p>\n<p><strong>Prior Art: Fraud Proving:</strong></p>\n<ul>\n<li>Fraud proving games of the past either require a single (e.g. Fuel V1) or multi-round challenge (e.g. Arbitrum) process, whereby a single honest validator may submit a fraud proof after witnessing an invalid state transition on a rollup [8].</li>\n<li>The fraud proving window is typically set to 7 days.</li>\n<li>The 7 day window is set to ensure that an adversary, even under the event of mass censorship, would either run out of money or be forked away by Ethereum social consensus.</li>\n<li>The longer window also accommodates for time to path major bugs in the state transition function or rollup software.</li>\n<li>We believe even 24 hours may be long enough to thwart this attack, but the 7 day mark provides enough assurances that the attack will be unprofitable on Ethereum [12].</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/3/395962922defe7f081c1d5f61baac7b420f43de6.png\" data-download-href=\"https://ethresear.ch/uploads/default/395962922defe7f081c1d5f61baac7b420f43de6\" title=\"2\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/3/395962922defe7f081c1d5f61baac7b420f43de6_2_552x500.png\" alt=\"2\" data-base62-sha1=\"8bkIMR3M9grgJAjMF5freVFHthY\" width=\"552\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/3/395962922defe7f081c1d5f61baac7b420f43de6_2_552x500.png, https://ethresear.ch/uploads/default/original/2X/3/395962922defe7f081c1d5f61baac7b420f43de6.png 1.5x, https://ethresear.ch/uploads/default/original/2X/3/395962922defe7f081c1d5f61baac7b420f43de6.png 2x\" data-dominant-color=\"D2DDD7\"></a></div><p></p>\n<p><strong>Prior Art: Closed Committees:</strong></p>\n<ul>\n<li>Some rollups have opted for a security committee model, such that, a closed group of honest actors would effectively become the final say in state transitions, whereby an honest majority may be able to spot fraud and reverse state transitions.</li>\n<li>Closed committees are typically not permission-less and don’t allow non-committee members to reverse invalid state transitions or defend valid transitions.</li>\n</ul>\n<p><strong>Prior Art: Lowering the Fraud Proving Window (Forking and Sliding Windows):</strong></p>\n<ul>\n<li>Ed Felton proposed an idea of a forked block censorship oracle, where missed blocks are a measure of censorship and thus a window could be extended if missed blocks are detected in the previous epochs. While this is a fine measure of forking censorship, it does not accommodate for the case that block producers could all censor out of protocol (e.g. mempool or p2p layer) preventing a fraud proving transaction from ever being processed for a long period of time, potentially up to many days [9].</li>\n<li>Ayelet Lotem, Sarah Azouvi, Patrick McCorry and Aviv Zohar proposed a sliding window challenge based on a fee oracle which detects if Ethereum is in high demand and extends the window accordingly, thus allowing a shorter window if the fee oracle reports only moderate activity and an extended window under high demand.</li>\n</ul>\n<p>The shortcoming here is that it doesn’t accommodate for large scale censorship (forking) or for when the block space is extremely busy at a norm, thus making it harder to determine an extension of the window [10].</p>\n<ul>\n<li>3Sigma proposed a governance adjusted approach to reducing the finality window using a dynamic challenge window, including rational for a safe minimum window. The shortcoming here is that adding a governance mechanism of this nature would change our base assumption around honest minority in addition to being subject to censorship itself past safe  minimum window (as a majority within the governance body would be required to potentially alter the finality window period transactionally) [12]</li>\n</ul>\n<p><strong>Transacting without Transactions:</strong></p>\n<ul>\n<li>We must assume any censoring party can entirely prevent a fraud proving challenge or any transaction to be submitted and for an extended period of time (close to 7 days on the higher end).</li>\n<li>Under mass censorship, the only mechanism an honest party has is to do nothing.</li>\n<li>This means that a mechanism would need to leverage the idea of “no transaction submission” in order to counter sophisticated forms of censorship on Ethereum.</li>\n<li>While most fraud proving games have used a transactional state transition to signal fraud, they have not used the idea of no state transition as evidence of censorship or fraud.</li>\n<li>With this idea, we define what we call a “check-in” model.</li>\n</ul>\n<p><strong>Check-in based Dynamic Challenge Window:</strong></p>\n<ul>\n<li>Instead of having honest minority validators only submit challenges when an invalid state transition is witnessed during a fixed challenge window (conventionally 7 days past last valid transition), we flip the model on its head, and say that all validators must check-in periodically and if they miss a check-in transaction, this is treated as either censorship or offline inactivity and the challenge window dynamically expands accordingly up until a maximum safe window (e.g. 7 days).</li>\n<li>In a check-in model, validators submit transactions every epoch, say every 64 Ethereum blocks. The check in would be as follows:\n<ul>\n<li><strong>True</strong>, there has been no invalid transitions witnessed over the past epoch</li>\n<li><strong>False</strong>, there has been an invalid transition over the last 64 blocks</li>\n<li><strong>No-report</strong>, my node is down or I am being censored</li>\n</ul>\n</li>\n<li>In the happy path (true), all validators check-in with true, and the state is finalised within 64 blocks.</li>\n<li>In the fraud path (false), all or one validators check-in with a false, and the fraud proving window is opened up in order for a challenge game to commence, the state is finalised once the challenge game is concluded.</li>\n<li>In the no-report path, we assume one of the honest minorities is being censored (even if inactivity is the cause) and thus we keep extending the window every epoch up until the maximum window, in which case the state is finalised either after a check-in before the max window or after the max window has passed.</li>\n<li>The check-in model enables honest minorities to vote without making a transaction, using a dead man switch style mechanism of “no-report”.</li>\n<li>A well funded adversary now has no mechanism in which mass censorship will produce the outcome they are looking for as the system passively reverts back to the safety of long finalisation windows to thwart any censorship attack.</li>\n<li>This empowers a single honest minority to thwart potentially a multi-billion dollar censorship attack on Ethereum, just by doing nothing at all.</li>\n<li>Unlike closed committees, the check-in model is permission-less and thus anyone can enter or leave at will and participate as a validator at will.</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/3/38c357a019d585a01027be7da5a90ff7bc887310.png\" data-download-href=\"https://ethresear.ch/uploads/default/38c357a019d585a01027be7da5a90ff7bc887310\" title=\"3\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/3/38c357a019d585a01027be7da5a90ff7bc887310_2_661x500.png\" alt=\"3\" data-base62-sha1=\"869fWVgkupapQxetKiIdTk8WSZ2\" width=\"661\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/3/38c357a019d585a01027be7da5a90ff7bc887310_2_661x500.png, https://ethresear.ch/uploads/default/original/2X/3/38c357a019d585a01027be7da5a90ff7bc887310.png 1.5x, https://ethresear.ch/uploads/default/original/2X/3/38c357a019d585a01027be7da5a90ff7bc887310.png 2x\" data-dominant-color=\"D5CBCE\"></a></div><p></p>\n<p><strong>Grieving Vector:</strong></p>\n<ul>\n<li>\n<p>Any registered validator of the group may grief the rollup, in the worst case, reverting the rollup back to a 7 day challenge window.</p>\n</li>\n<li>\n<p>This means that even if the entire group is submitting periodically, a single party could prevent finalisation for up to 7 days.</p>\n</li>\n<li>\n<p>Note, this grieving vector outcome alreadys exists with many fraud proving systems today, where finalisation would be typically set to a fixed 7 days anyway.</p>\n</li>\n</ul>\n<p><strong>Reducing or Punishing Grieving:</strong></p>\n<ul>\n<li><strong>Bonding</strong>: each validator can be bonded, such that if their node goes offline, they can be punished by slashing their bond after 7 days have elapsed past their last check in. Similarly, if they don’t submit a check-in and then do not come back online or come back online but have no fraud to report or an invalid fraud challenge, they can be punished.</li>\n<li><strong>Slashing</strong>: the first choice would be to burn either a portion or all their bond depending on the circumstances. A possible option would be to recommend burning half their bond, then distributing the remaining bond to honest participants who are checked in, this acts as a disincentive for validators to be inactive and hold up the finality window. The negative consequence would be that this would incentize validators to censor others, so the game theory would need to be worked out there.</li>\n<li><strong>Holding</strong>: if one of the validators decides to not submit, all other honest participating parties can hold off on submitting a check-in past the last honest check in (the one beyond the lagging validator), either until the lagging validator comes back online or is slashed by the 7 day challenge window. Reducing cost for active validators during grieving.</li>\n<li><strong>Time-based penalties</strong>: we can consider multiple time based response mechanisms, such that there are different slashing punishments the longer the lagging validator is inactive. Such that, there is more and more incentive to come back online with a check in.</li>\n<li><strong>MPC</strong>: to reduce honest inactivity, validators could use MPC based setups over different nodes to ensure that even if a minority of their nodes go offline, that their validator continues to publish check-ins. This setup is commonly used in proof of stake validator blockchains.</li>\n<li><strong>Stake-weighting</strong>: the bonding process could be such that lower bonded validators could be slashed earlier, where higher bonded validators can extend the window longer. The risk here is that smaller honest minorities are treated worse in the slashing game, but they would also have less bond at stake. An example would be if you’re bonded 10 ETH, then you can extend the finality window up until 7 days, but if you’re bonded 1 ETH you can only extend it to 3 days (but this should only be lowered to a safe lower bound such as 24 hours). If multiple honest minorities go offline, then you can add the stake of all lagging minorities together which would result in a larger window than any single lagging minority.</li>\n</ul>\n<p><strong>Costs:</strong></p>\n<ul>\n<li>Check-in’s would be a relatively simple transaction. Likely only the base cost + a single state modification and a few minor checks. Despite this, having all validators check-in and do so very frequently, would result in potentially high costs for honest validation.</li>\n<li>If a validator were to check-in every hour, with an average transaction costing an average of ~3 USD on Ethereum [11], that would be 8760 check-ins a year costing around $26,280 USD, if we have multiple validators, say 5, this could cost closer to $131,400 per year in Ethereum L1 fees if the mechanism is naively constructed.</li>\n<li>While certain batching techniques could be applied, this is still a very high cost penalty per rollup without batching or aggregation.</li>\n</ul>\n<p><strong>Reducing Check-in Cost:</strong></p>\n<ul>\n<li><strong>Zk-Aggregation</strong>: this could be used to aggregate the check-ins of many roll ups into a single check-in proof, drastically reducing the check-in cost for all participating rollups in the happy path case. Of course, in some unhappy paths, you would fallback to normal more expensive check-ins.</li>\n<li><strong>MPC Multi-Party Signatures</strong>: MPC over ECDSA signatures could reduce cost significantly for honest validators when posting check-ins to Ethereum, such that they could both ensure better uptime for nodes going down and signature batching between honest and lively validators.\n<ul>\n<li>Validators could split submission costs, such that 26,280k per year could be split between potentially 100s of honest validators. If we take the cost of an hourly check-in $26,280 USD and divide it by 100, that’s an annual cost of 262.8 per validator, a much lighter on-chain cost.</li>\n<li>MPC in the highest secure setting should require an N of N or as close to N of N construction, such that any honest minority can prevent a valid group signature. In which case, the group can reform and remove the lagging validator, likely with some additional bonding and punishment.</li>\n</ul>\n</li>\n<li><strong>Block Submission Check-ins</strong>: Check-in submissions could potentially be included within block submission, such that the necessary state elements are changed during by honest builders. This would also significantly reduce submission cost, depending on the implementation.</li>\n<li>Expanding the check-in window length, if the window is a little longer, say 4 hours, this would reduce the cost 4x from initial 1 hour estimates.</li>\n<li><strong>Request based check-ins</strong>: the reporting window could also be less frequent and only when enough withdrawals are requested. If an honest minority who is staked fails to report, the window extends until the 7 day mark.</li>\n<li><strong>Passive Validation</strong>: validators may still chose to validate without a check-in which would be effectively free of on-chain cost, this would mean that they are subject to the very small submission window, but so long as Ethereum remains not under a censorship attack, even a 1 hour window should be enough to get a regular submission in. This does not cover the most adversarial cases, but does cover some happy path cases of validation. Rollups could choose a mixture of check-in based and passive validation nodes to ensure security, both from censorship and from invalid state transitions.</li>\n<li><strong>Offshoring to Based Rollup for Censorship Resistance</strong>: many roll ups could also off-shore the security assumptions to a third-party rollup, such that so long as one honest minority in that rollup attempts to submit the request transactions and cannot, the window is extended, whereby many other rollups can then look to this censorship rollup for their own finality window. If any rollup is being censored, all rollups finalisation is extended. Validators of that rollup would have a very high bonding and slashing scheme and again, validators would need to have a safe transport mechanism and mempool to ensure they can receive all request fraud proving transactions quickly.</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/2X/2/2909fbbda891bc7b09f770c4716d7d7d6d1a4122.png\" data-download-href=\"https://ethresear.ch/uploads/default/2909fbbda891bc7b09f770c4716d7d7d6d1a4122\" title=\"4\"><img src=\"https://ethresear.ch/uploads/default/optimized/2X/2/2909fbbda891bc7b09f770c4716d7d7d6d1a4122_2_684x500.png\" alt=\"4\" data-base62-sha1=\"5R2XRyg0vmMTFBTmbJfmOKXXSrE\" width=\"684\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/2X/2/2909fbbda891bc7b09f770c4716d7d7d6d1a4122_2_684x500.png, https://ethresear.ch/uploads/default/original/2X/2/2909fbbda891bc7b09f770c4716d7d7d6d1a4122.png 1.5x, https://ethresear.ch/uploads/default/original/2X/2/2909fbbda891bc7b09f770c4716d7d7d6d1a4122.png 2x\" data-dominant-color=\"C4D1D5\"></a></div><p></p>\n<p><strong>Rewards Systems and Staking:</strong></p>\n<ul>\n<li>Honest validators could be rewarded similar to any staking model, whereby random stake weighted rewards could be applied overtime.</li>\n<li>Transaction fees could be used to pay the additional security budget for honest minorities during block production.</li>\n<li>Newly minted rollup tokens could be dispersed for those who validate and are active to incentivize honest validation.</li>\n</ul>\n<p><strong>Downsides of Short Check-in Windows:</strong></p>\n<ul>\n<li>\n<p>Short check-in windows do come with a potential downside, such that if there is slashing applied to a very time sensitive window, it could negatively impact honest validators if they are censored.</p>\n</li>\n<li>\n<p>Censoring, in this case, can be used against validators if the penalty is overly sensitive and the minimum slashing period set too short without any recourse mechanisms.</p>\n</li>\n<li>\n<p>A possible solution to this would be based around the already existing prior art we have, such as using the base fee, detection of forked blocks, a governance mechanism such as a token DAO or committee, or a third-party shared sequencer to help better calibrate the slashing penalties during more time sensitive windows, namely for offline or inactive validators.</p>\n</li>\n<li>\n<p>These mechanisms in this case would only be used to calibrate penalties and not the window itself, ensuring that users have enough time / warning to withdraw if penalties are unreasonable for validators. If they are applied poorly, honest validators could make their case to the community and people would have enough time to withdraw in this event if nothing can be done.</p>\n</li>\n<li>\n<p>In the event a rollup applies too harsh a penalty/requirement without appropriate recourse, validators would be disincentivised to validate the rollup which would be a negative outcome for the rollup itself. This follows a similar concept to any blockchain in regard to operational cost and penalties, so Rollups will want to set these carefully.</p>\n</li>\n</ul>\n<p><strong>Bugs</strong>:</p>\n<ul>\n<li>There are many reasons to have a longer challenge window outside of censorship, one of these are bugs.</li>\n<li>If there is a bug in the state transition function, the only possibility would be for an honest actor to identify this and proceed with no-report.</li>\n<li>This bug surface area also exists in zk-rollups as well and with the same potential level of consequence.</li>\n<li>Watchtower based validators could be implemented to reduce mass withdrawal scenarios.</li>\n<li>An added benefit of the check-in model is that, if a validator panics during a state transition function, the validator could be designed to not report, and thus allow up to 7 days for developers to fix the issue.</li>\n<li>A hyper aware validator may be able to hit pause in time, thus giving a security council the 7 days to upgrade state transition function.</li>\n</ul>\n<p><strong>Bribery and Collusion</strong>:</p>\n<ul>\n<li>In the case of the check-in model, an adversary would likely attempt to bribe or collude with validators. Which is why it’s important that for each rollup at least one well aligned honest validator is operating, however, unlike large honest majority validator set style mechanisms, only one honest validator is needed for proper state transition enforcement.</li>\n<li>Bribery exists within operating ORs today. A well funded adversary could collude with all the known rollup validators and have them pretend to validate until it is too late for anyone to react (post 7 day finalisation). In conventional 7 day window designs, it may be the case that someone could notice this and spin up their own nodes or intervene with a security council thus countering this form of attack.</li>\n<li>Another notable benefit conferred from a 7 day window non-checkin model is that it is harder to see who is validating on chain as there are no check-ins, and thus hard to collude on exactly all the parties involved. There would also be more time to react or notice invalid state transitions and apply a patch or spin up an honest node to defend the chain. However, understanding how much that really helps the situation is hard to quantify. In all ORs, the main security assumption of a single honest party is still required and this still holds true in the check-in model or longer conventional ORU designs.</li>\n</ul>\n<p><strong>Universal Interoperation between Optimistic Rollups using Zk Aggregation for Check-ins</strong>:</p>\n<ul>\n<li>If we maintain that for each rollup there is one honest minority, that all validators must check-in in the happy path and many roll ups check-into a single universal aggregated zk-proof creation process, we can assert the state of finalisation for different rollups is true across rollups based upon the complete last check in and validator set, enabling better cross-rollup interoperation for ORs.</li>\n<li>This enables faster bridging between ORs in the happy path cases.</li>\n<li>In the unhappy path, bridging or finalisation acceptance may still take up to 7 days for certain lagging ORs.</li>\n<li>A note that zk-aggregation of check-ins would be significantly less proving computation than proving rollup execution for all rollups and likely far cheaper than posting all votes on Ethereum.</li>\n</ul>\n<p><strong>Conclusion</strong>:</p>\n<ul>\n<li>In this post we conclude that a check-in based challenge design enables an honest minority to thwart mass censorship attacks, reducing the overall finality time of Optimistic Rollups significantly from 7 days to only a few blocks (depending on the security budget) while still defending against all censorship attacks.</li>\n<li>We believe this dead man switch mechanism might also have applications elsewhere where time based operations are required that may be subject to censorship attacks, such as lending CDPs, time sensitive voting, or broadly time sensitive decentralised applications.</li>\n<li>We also believe that this no-report mechanism is useful in events of panicking state transition functions, watchtowers, bugs or mass withdrawals and improved cross-chain interoperation for ORs while giving projects and developers enough time for an emergency upgrade if need be.</li>\n</ul>\n<p>Explain Like I’m 5 (EL15):</p>\n<ol>\n<li>Censorship can stop validators signalling fraud in Optimistic Rollups (ORs).</li>\n<li>To counteract censorship, ORs have a 7-day period where challenges can be made. This long window makes it unprofitable to censor fraud signals on Ethereum.</li>\n<li>However, validators in ORs can also regularly signal that everything is correct.</li>\n<li>If the only thing that can be censored is the signalling of correctness, we can also assume no signalling means censorship or fraud. A short challenge window can then increase passively until the challenge is submitted, up to a safe maximum of 7 days.</li>\n<li>This ensures validators have enough time to overcome any censorship attacks while keeping the challenge and finality window short in the happy path.</li>\n<li>With these measures, the time it takes for transactions in ORs to be considered final can now be safely reduced to just a few Ethereum blocks.</li>\n</ol>\n<p>References:</p>\n<p>[1] <a href=\"https://kelvinfichter.com/pages/thoughts/challenge-periods/\" rel=\"noopener nofollow ugc\">Why is the Optimistic Rollup challenge period 7 days?</a><br>\n[2] <a href=\"https://ethresear.ch/t/non-attributable-censorship-attack-on-fraud-proof-based-layer2-protocols/6492\">Non-attributable censorship attack on fraud-proof-based Layer2 protocols</a><br>\n[3] <a href=\"https://medium.com/offchainlabs/fighting-censorship-attacks-on-smart-contracts-c026a7c0ff02\" rel=\"noopener nofollow ugc\">Fighting censorship attacks on smart contracts</a><br>\n[4] <a href=\"https://ethresear.ch/t/nearly-zero-cost-attack-scenario-on-optimistic-rollup/6336\">Nearly-zero cost attack scenario on Optimistic Rollup - Layer 2 - Ethereum Research</a><br>\n[5] <a href=\"https://hackingdistributed.com/2016/07/05/eth-is-more-resilient-to-censorship/\" rel=\"noopener nofollow ugc\">Ethereum is Inherently Secure Against Censorship</a><br>\n[6] <a href=\"https://ethresear.ch/t/responding-to-51-attacks-in-casper-ffg/6363\">Responding to 51% attacks in Casper FFG - Proof-of-Stake - Ethereum Research</a><br>\n[7] <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4727999\" rel=\"noopener nofollow ugc\">Breaking BFT: Quantifying the Cost to Attack Bitcoin and Ethereum</a><br>\n[8] <a href=\"https://medium.com/offchainlabs/whats-up-with-rollup-db8cd93b314e\" rel=\"noopener nofollow ugc\">What’s up with Rollup</a><br>\n[9] <a href=\"https://ethresear.ch/t/reducing-challenge-times-in-rollups/14997\">Reducing challenge times in rollups - Layer 2 - Ethereum Research</a><br>\n[10] <a href=\"https://arxiv.org/abs/2201.09009\" rel=\"noopener nofollow ugc\">Sliding Window Challenge Process for Congestion Detection</a><br>\n[11] <a href=\"https://ycharts.com/indicators/ethereum_average_gas_price/chart/\" rel=\"noopener nofollow ugc\">YCharts Ethereum Average Gas Prices</a><br>\n[12] <a href=\"https://ethresear.ch/t/challenging-periods-reimagined-road-to-dynamic-challenging-periods/15077/2\">Challenging Periods Reimagined: Road to dynamic challenging periods</a></p>\n            <p><small>6 posts - 6 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/why-wait-a-week-fast-finality-optimistic-rollups/18868\">Read full topic</a></p>","link":"https://ethresear.ch/t/why-wait-a-week-fast-finality-optimistic-rollups/18868","pubDate":"Mon, 04 Mar 2024 20:48:44 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-18868"},"source":{"@url":"https://ethresear.ch/t/why-wait-a-week-fast-finality-optimistic-rollups/18868.rss","#text":"Why wait a week? Fast Finality Optimistic Rollups"},"filter":false}]}}}