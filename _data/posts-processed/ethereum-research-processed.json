{"rss":{"@version":"2.0","@xmlns:discourse":"http://www.discourse.org/","@xmlns:atom":"http://www.w3.org/2005/Atom","@xmlns:dc":"http://purl.org/dc/elements/1.1/","channel":{"title":"Ethereum Research - Latest topics","link":"https://ethresear.ch/latest","description":"Latest topics","lastBuildDate":"Mon, 13 May 2024 16:14:42 +0000","atom:link":{"@href":"https://ethresear.ch/latest.rss","@rel":"self","@type":"application/rss+xml"},"item":[{"title":"User-Defined Penalties: Ensuring Honest Preconf Behavior","dc:creator":"jonahb27","category":"Economics","description":"<p><em>Thank you <a href=\"https://twitter.com/drakefjustin\">Justin Drake</a> and <a href=\"https://twitter.com/sproule_\">Ryan Sproule</a> for the help.</em></p>\n<p><strong>tl;dr:</strong> <em>Allow users to specify their preferred penalty when requesting a preconf, enabling the market to naturally establish preconf cryptoeconomic security parameters, rather than setting parameters upfront.</em></p>\n<p>As the community settles on a design for preconfs, a critical choice arises: how can we ensure crypto-economic security for preconfs? Specifically, what incentives exist to prevent safety or liveness faults? I’ll present a high-level overview of the current solutions before proposing an alternative.</p>\n<p>Here are the current mechanisms (they can be used in combination):</p>\n<ol>\n<li><strong>Basic Slashing:</strong> If a proposer is responsible for a safety or liveness fault, they are slashed.\n<ul>\n<li><strong>Open Question:</strong> How much should be slashed, and what amount of stake should the proposer put up?</li>\n</ul>\n</li>\n<li><strong>Freezing:</strong> The proposer’s stake is frozen, causing them to lose the time value of their money.\n<ul>\n<li><em><a href=\"https://twitter.com/drakefjustin\">Justin Drake</a> suggested during the <a href=\"https://github.com/ethereum/pm/issues/1035\">Ethereum sequencing call #7</a> that this approach could help ease the adoption of the preconf protocol since preconfs introduce new behaviors the market needs to adjust to.</em></li>\n<li><strong>Open Question:</strong> How much stake should be frozen, and for how long?</li>\n</ul>\n</li>\n<li><strong>Dynamic Reputation Slashing:</strong> Each fault by a validator results in a progressively stricter penalty; for instance, they might be slashed more or locked up longer.\n<ul>\n<li><strong>Open Question:</strong> What should the penalty curve look like? Should it be time-based and reset after a period of honest behavior?</li>\n</ul>\n</li>\n<li><strong>Insurance:</strong> Proposers must compensate users whose preconfs fail due to faults. Effectively, users’ preconfs are insured.\n<ul>\n<li><strong>Open Question:</strong> How much insurance should be offered?</li>\n</ul>\n</li>\n</ol>\n<p>All these mechanisms require us to know ex ante what preconf users want. Inevitably, this will be opinionated and lead to deadweight loss, as some users who might want a preconf could feel uncomfortable with the setup. Moreover, some proposers might feel uncomfortable with the parameterization and choose not to offer preconfs. The best solution is to allow users to work with proposers to agree on the appropriate level of crypto-economic security.</p>\n<p>My Solution: <strong>User-Defined Penalties</strong></p>\n<p>Users should be able to specify their desired level of crypto-economic security by attaching a penalty structure to their preconf. This structure will detail the consequences of a fault.</p>\n<p>For instance:</p>\n<pre><code class=\"lang-rust\">// Here, users can define a penalty associated with any specific fault,\n// and the system is generic enough to allow for arbitrarily complex rules.\nstruct PreconfAgreement&lt;C: Condition, P: Penalize&gt; {\n    faults: Vec&lt;Fault&lt;C, P&gt;&gt;,\n}\n\nstruct Fault&lt;C: Condition, P: Penalize&gt; {\n    condition: C,\n    penalties: Vec&lt;P&gt;,\n}\n\ntrait Condition {\n    fn should_penalize(...) -&gt; bool;\n}\n\ntrait Penalize {\n    fn penalize(...);\n}\n</code></pre>\n<p><em>Note: There is a DoS vector associated with unbounded compute when evaluating conditions. Some gas metering should be used, or conditions should be constructed as succinct statements (e.g., a SNARK).</em></p>\n<p>This solution is unbiased and allows the market to determine the appropriate parameters naturally. Users can decide the level of security they want rather than leaving it up to the protocol to estimate, while proposers can choose their risk-reward profile. Heavier penalties will likely result in higher costs for users.</p>\n<p><strong>Complexity Concerns:</strong></p>\n<ul>\n<li><strong>Proposers’ Perspective:</strong> With preconfs, we already assume that proposers (or their gateways) are sophisticated, and giving them the ability to manage their own risk profiles should benefit them. Inexperienced proposers can set a simple threshold for the maximum penalty they are willing to incur and, as they gain experience, adjust it more systematically.</li>\n<li><strong>Users’ Perspective:</strong> This approach shouldn’t add complexity, as wallets can easily abstract the penalty decision, much like they abstract gas fee choices. Fine-grained choices can be offered as an opt-in feature for more advanced users.</li>\n</ul>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/user-defined-penalties-ensuring-honest-preconf-behavior/19545\">Read full topic</a></p>","link":"https://ethresear.ch/t/user-defined-penalties-ensuring-honest-preconf-behavior/19545","pubDate":"Mon, 13 May 2024 16:14:42 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19545"},"source":{"@url":"https://ethresear.ch/t/user-defined-penalties-ensuring-honest-preconf-behavior/19545.rss","#text":"User-Defined Penalties: Ensuring Honest Preconf Behavior"},"filter":false},{"title":"Embedded fee markets and ERC-4337 (part 1)","dc:creator":"DavideRezzoli","category":"Economics","description":"<p>by: Davide Rezzoli (<a class=\"mention\" href=\"https://ethresear.ch/u/daviderezzoli\">@DavideRezzoli</a>) and Barnabé Monnot (<a class=\"mention\" href=\"https://ethresear.ch/u/barnabe\">@barnabe</a>)</p>\n<p>Many thanks to Yoav Weiss (<a class=\"mention\" href=\"https://ethresear.ch/u/yoavw\">@yoavw</a>) for introducing us to the problem, Dror Tirosh (<a class=\"mention\" href=\"https://ethresear.ch/u/drortirosh\">@drortirosh</a>) for helpful comments on the draft, and the 4337 team for support. Reviews ≠ endorsements, all errors are the authors’ own.</p>\n<p>This work was done for <a href=\"https://efdn.notion.site/ROP-7-Economic-models-of-signature-aggregation-in-account-abstraction-ec5390efab864ed49a8535e8bdfff182\" rel=\"noopener nofollow ugc\">ROP-7</a></p>\n<hr>\n<p>Transaction fee mechanisms have become the workhorse models to understand the intermediation of block producers between users wishing to transact and “the chain” (or “the protocol”) which users transact on. Given the ability to use some of the supply provided by the chain, the block producers must arbitrate which users will have ability to use the scarce resource of on-chain execution, and at what cost. On Ethereum, for the question of cost, block producers are constrained by the EIP-1559 fee mechanism, which dynamically sets a reserve price block-to-block, called “base fee”. The base fee is a price, expressed per units of gas, which a user transaction must pay to be included and executed. The user may provide so-called “priority fees” beyond the base fee, to further incentivise the block producers in times of congestion.</p>\n<p>In this note, we investigate the question of <em>embedded fee markets</em>, i.e., fee markets which “live” within other fee markets. This question was discussed in a different context in a recent paper by Maryam Bahrani, Pranav Garimidi and Tim Roughgarden, “<a href=\"https://eprint.iacr.org/2024/331\" rel=\"noopener nofollow ugc\">Transaction Fee Mechanism Design in a Post-MEV World</a>”. In this paper, the authors model the use of searchers, further intermediating access to the chain between users and block producers. Block producers receive “hints” from searchers, embodied by atomic bundles of transactions to be included by the chain. The fee market of searchers is driven by the maximisation objective of a quantity known as MEV, or maximal extractable value.</p>\n<p>In our setting, users wish to access the chain but do not express their demand using protocol-legible transactions. Instead, users produce “operations”, to be bundled by entities known as “bundlers”, who then originate a protocol-legible transaction packing the operations together towards execution. Thus, to the EIP-1559 fee mechanism, bundlers are the users of the chain, yet the actual users must first obtain inclusion in the bundle of a bundler before they may gain inclusion to the chain. In other words, we may see this setting as part of the larger question of <a href=\"https://ethresear.ch/t/uncrowdable-inclusion-lists-the-tension-between-chain-neutrality-preconfirmations-and-proposer-commitments/19372#comparison-between-inclusion-lists-pepc-and-multiple-concurrent-block-producers-3\">block co-creation</a>, which arises with (partial) builders/searchers as well as inclusion lists.</p>\n<p>Our hope is for these dynamics to be as transparent as possible, such that there is not either more cognitive overhead or opportunities for the user to be unduly exploited by the bundler, compared to going on-chain directly. We hope for even stronger results, cases where indeed the users benefit from the bundler intermediation, when amortised costs allow users to enjoy greater welfare.</p>\n<p>To investigate the distinction between direct fee markets and their embedded (sub-)mechanisms, we must precise the economic constraints which a bundler abides by. In the following section, we offer a simple model of the bundler cost function motivated by the practice, in particular bundlers participating in the ERC-4337 protocol, which we briefly recapitulate.</p>\n<h2><a name=\"model-1\" class=\"anchor\" href=\"https://ethresear.ch#model-1\"></a>Model</h2>\n<h3><a name=\"bundling-in-erc-4337-2\" class=\"anchor\" href=\"https://ethresear.ch#bundling-in-erc-4337-2\"></a>Bundling in ERC-4337</h3>\n<p>A user wishing to perform some activity on-chain via bundlers issues a User Operation (UserOp, or operation). This UserOp is emitted from the user’s wallet, e.g., after interacting with a DApp. Once the UserOp is broadcast, some bundler receiving the operation may decide to include it in a bundle. A bundle is an “externally-owned account” (EOA) meta-transaction, which writes the data of the included UserOps in its <code>bundle.calldata</code> field. The bundler issues the bundle towards inclusion in a block by a block producer (we discuss the relation between bundler and block producer in a future note).</p>\n<p>Once the bundle is included in the block, and the block makes its way to the chain, the bundle is executed along with other transactions in the block. Essentially, the bundle execution steps are as follows:</p>\n<ul>\n<li><strong>Pre-verification:</strong> A bundler’s EOA transaction will consume 21,000 gas, and the call to the EntryPoint contract will set up key variables to keep track of the execution of the operations in the operation loop.</li>\n<li><strong><a href=\"https://eips.ethereum.org/EIPS/eip-4337#required-entry-point-contract-functionality\" rel=\"noopener nofollow ugc\">Operation loop</a>:</strong> For each operation included in the bundle, the following two steps take place:\n<ul>\n<li><strong>Verification step:</strong> UserOps perform operations containing a verification step, which is encoded in a “smart contract wallet” deployed initially by the user (during an initial UserOp). The verification step may simply check a signature, or perform more complex operations to “grant” the right for the UserOp to proceed with its execution. The verification step is metered by <code>op.verificationGasLimit</code>.</li>\n<li><strong>Execution step:</strong> The core of the UserOp, the execution step performs the operation described in <code>op.callData</code>. This step is also metered, using <code>op.callGasLimit</code>.</li>\n</ul>\n</li>\n</ul>\n<p>As is made clear by the previous decomposition, the pre-verification step is executed once, offering the possibility to amortise the pre-verification costs across all included users. When the bundle is executed, all costs (e.g., <code>block.basefee</code> + priority fees paid by the bundler to the block producer including them) are charged to the bundler, who must ensure that user operations compensate her enough for the costs incurred. We make these statements precise in the following section.</p>\n<h3><a name=\"fee-market-model-for-bundles-3\" class=\"anchor\" href=\"https://ethresear.ch#fee-market-model-for-bundles-3\"></a>Fee market model for bundles</h3>\n<p>We attempt to remain consistent with classic fee markets models. A user <span class=\"math\">t</span> who wishes to emit an operation has some value <span class=\"math\">v_t</span> for the execution of the operation. We assume all operations to have the same size <span class=\"math\">S</span> (i.e., same gas used for the verification and execution steps), and we thus express all quantities as prices per unit of gas.</p>\n<p>Users express their wish to be included by emitting a bid <span class=\"math\">b_t</span> along with their operation. For now, we do not assume a specific grammar for the user to express their bid for inclusion, e.g., the ability to express a max fee and priority fee along with their operation, as they would with EIP-1559. User operations are collected in a mempool <span class=\"math\">\\mathbf{M}</span>, representing the pending status of these operations until inclusion.</p>\n<p>The EIP-1559 fee market exposes a reserve price <span class=\"math\">r</span> known as “base fee”, which bundlers must incur when their bundle is executed. If the bundle contains <span class=\"math\">n</span> operations, the bundler must then expense at least <span class=\"math\">n \\times S \\times r</span>. Additionally, since the bundle consumes “pre-verification gas”, say, some quantity <span class=\"math\">F</span>, the bundler will additionally pay <span class=\"math\">F \\times r</span>. The operations included in the bundle are given by the set <span class=\"math\">\\mathbf{B}</span>.</p>\n<h3><a name=\"bundler-cost-functions-4\" class=\"anchor\" href=\"https://ethresear.ch#bundler-cost-functions-4\"></a>Bundler cost functions</h3>\n<p>We now consider the costs incurred by bundlers for the inclusion of their bundles in the block.</p>\n<p><strong>On-chain cost function:</strong> A bundler issuing bundle <span class=\"math\">\\mathbf{B}</span> when the base fee is <span class=\"math\">r</span> expends a cost:</p>\n<div class=\"math\">\nC_\\text{on-chain}(\\mathbf{B}, r) = F \\times r + n \\times S \\times r\n</div>\n<p>The bundler problem mirrors the block producer problem expressed in <a href=\"https://arxiv.org/abs/2106.01340\" rel=\"noopener nofollow ugc\">[Roughgarden 2021]</a>. There, the block producer had to ensure the provision of some value <span class=\"math\">\\mu</span> compensating her for the cost of including an additional transaction to their block (e.g., <span class=\"math\">\\mu</span> may compensate for the extra load of the block, which delays its propagation and thus increases re-org risk). The block-level fee market must then ensure that the block producer is at least compensated for the total cost <span class=\"math\">n \\times S \\times \\mu</span>, should the block producer include <span class=\"math\">n</span> transactions in their block. The bundler-level fee market will require to at least compensate the bundler for exogenous costs <span class=\"math\">C_\\text{on-chain}(\\mathbf{B}, r)</span>  they incur from the larger fee market they are embedded in.</p>\n<p>ERC-4337 offers the possibility amortise costs beyond sharing the pre-verification gas costs. Should all operations employ the same signature scheme for their verification step, the signatures of these operations may be <em><a href=\"https://youtu.be/CgXzDuN5Xqc?si=lnpEZLKPCp6sjKAp&amp;t=2004\" rel=\"noopener nofollow ugc\">aggregated</a></em> by the bundler, such that instead of verifying on-chain <span class=\"math\">n</span> signatures, a single signature may be verified. In this case, the bundler cost function will need to account for the off-chain costs which the bundler incurs when performing the aggregation. In the following, we make the assumption that such costs are linear in the number of operations, a similar assumption to <a href=\"https://arxiv.org/abs/2404.06495\" rel=\"noopener nofollow ugc\">[Wang et al., 2024]</a>, at a marginal cost <span class=\"math\">\\omega</span>.</p>\n<p>We also account for the reduced gas consumption of each operation, due to savings from the aggregation. When aggregated, operations are not required to publish their signature, but they do require an additional pairing operation. On chains where calldata cost is expensive, but pairing operations/computation are cheap, aggregation thus provides per-operation savings. In this case, we denote by <span class=\"math\">S' &lt; S</span> the reduced size of a transaction. We also need to account for the increased pre-verification gas use <span class=\"math\">F' &gt; F</span>, which now contains the publication and verification of the single on-chain aggregated signature.</p>\n<p><strong>Aggregated cost function:</strong> A bundler issuing bundle <span class=\"math\">\\mathbf{B}</span> with aggregated signatures when the base fee is <span class=\"math\">r</span> expends a cost:</p>\n<div class=\"math\">\nC_\\text{agg}(\\mathbf{B}, r) = F' \\times r + n \\times S' \\times r + n \\times \\omega\n</div>\n<p>In this note, we will not go further, but one may also consider the data publication costs which a bundler may need to expend when their bundle settles on a rollup. We suggest two ways of modelling this and leave this question for future work:</p>\n<ul>\n<li>Either the bundler herself is responsible for data publication (e.g., as a sequencer), and thus requires to obtain from users the necessary amount of funds to pay eventual data publication costs.</li>\n<li>Or the bundle-level fee market is embedded in a larger batch-level fee market, via which the rollup exposes to rollup users (including the bundler) the amount they are required to pay due to congestion (e.g., a base fee) and eventual data publication costs. In this case, the rollup is responsible for balancing their own future costs with their present revenues.</li>\n</ul>\n<h3><a name=\"revisiting-fee-market-quantities-5\" class=\"anchor\" href=\"https://ethresear.ch#revisiting-fee-market-quantities-5\"></a>Revisiting fee market quantities</h3>\n<p>We may now formally express the relevant concepts for the bundle-level fee market, deriving them straightforwardly from previous literature, while taking the embedding into account.</p>\n<p><strong>Bundle-level allocation rule:</strong> A (bundle-level) allocation <span class=\"math\">x</span> decides the set of user operations which the bundler includes in their bundle, given the current mempool <span class=\"math\">\\mathbf{M}</span> and the base fee <span class=\"math\">r</span>.</p>\n<div class=\"math\">\nx_t(\\textbf{M}, r) \\in \\{0, 1\\}, \\forall t\n</div>\n<p><strong>Bundle-level payment rule:</strong> Given the set of selected operations <span class=\"math\">\\mathbf{B}</span>, a payment rule assigns to each included user a fee:</p>\n<div class=\"math\">\np_t(\\textbf{B})\n</div>\n<p><strong>User utility function:</strong> <span class=\"math\">u_t(b_t) = v_t - p_t(\\mathbf{B})</span></p>\n<p>In principle, we could allow for the existence of a burning rule <span class=\"math\">q_t(\\mathbf{B})</span> expressing the fact that the bundler may not receive the totality of all included user payments. We do not consider it in this note however.</p>\n<p><strong>(Myopic) bundler utility function:</strong> <span class=\"math\">u(\\mathbf{B}, r) = \\sum_{t \\in \\mathbf{B}} p_t(\\mathbf{B}) - C(\\mathbf{B}, r)</span></p>\n<p>A bundle-level TFM <span class=\"math\">(x, p)</span> is incentive-compatible for myopic bundlers (MBIC) if, for every mempool <span class=\"math\">\\textbf{M}</span> and base fee <span class=\"math\">r</span>, a myopic bundler maximises its utility by following the suggestion of the allocation rule <span class=\"math\">x</span> (i.e., setting <span class=\"math\">\\textbf{B} = x(\\textbf{M}, r)</span>).</p>\n<h3><a name=\"forming-multiple-bundles-6\" class=\"anchor\" href=\"https://ethresear.ch#forming-multiple-bundles-6\"></a>Forming multiple bundles</h3>\n<p>In the preceding section, we’ve only considered the possibility for the bundler to issue a single bundle. However, we may be interested in the possibility for the bundler to make more than one bundle out of the operations available in the mempool. Given the mempool <span class=\"math\">\\mathbf{M}</span>, let <span class=\"math\">P(\\mathbf{M})</span> represent the set of partitions of the mempool, assigning each operation to a single bundle (we may assume that for each partition, there is a set indexed 0 which contains all the operations not assigned to a bundle for inclusion). The allocation rule then returns the index of the set in the partition to which the operation is assigned.</p>\n<div class=\"math\">\nx(\\textbf{M}, r) \\in P(\\textbf{M})\n</div>\n<p>We can write the set of bundles output by the partition <span class=\"math\">x(\\textbf{M}, \\beta)</span> as <span class=\"math\">\\mathcal{B}(x(\\textbf{M}, r))</span>. Intuitively, these bundles are made up from the operations which do not belong to the set indexed 0. Given a set of bundles <span class=\"math\">\\mathcal{B}</span>, the payment rule is then:</p>\n<div class=\"math\">\np_t(\\mathcal{B})\n</div>\n<p>The user utility function becomes:</p>\n<div class=\"math\">\nu_t(b_t) = v_t - p_t(\\mathcal{B})\n</div>\n<p>and the bundler utility function becomes:</p>\n<div class=\"math\">\nu(\\mathcal{B}, r) = \\sum_{B \\in \\mathcal{B}} \\sum_{t \\in {B}} p_t(\\mathcal{B}) - C(\\mathcal{B}, r)\n</div>\n<h2><a name=\"the-bundler-game-7\" class=\"anchor\" href=\"https://ethresear.ch#the-bundler-game-7\"></a>The bundler game</h2>\n<p>Inclusion of transactions in blocks must remunerate some quantity <span class=\"math\">\\mu</span> to the block producers, which is assumed to be linear in the transaction size in e.g., <a href=\"https://arxiv.org/abs/2106.01340\" rel=\"noopener nofollow ugc\">[Roughgarden, 2021]</a>. This quantity denotes the opportunity cost for the block producer to add an extra transaction to their block, e.g., increasing their gossiping delay and thereby increasing their chances of the block getting re-orged. In Proof-of-Stake, even though the protocol’s schedule allows for enough time to propagate a full block, <a href=\"https://ethresear.ch/t/timing-games-implications-and-possible-mitigations/17612#impact-on-blob-inclusion-ht-dankrad-for-mentioning-this-10\">timing games</a> have induced “last-second” propagation dynamics which have once again made this <span class=\"math\">\\mu</span> parameter relevant.</p>\n<p>In any case, we may observe that the cost-sharing problem at block-level and at bundle-level are very different. At block-level, a transaction need not know what else is going on inside the block to devise its inclusion bid according to EIP-1559 (it may want to know what is going on with respect to MEV <a href=\"https://eprint.iacr.org/2024/331\" rel=\"noopener nofollow ugc\">[Bahrani et al., 2024]</a>, but we’ll consider this a separate issue for now). At bundle-level, bundle overhead costs are no longer linear in the number of transactions, but a fixed overhead may be amortised by many transactions. Further, should the aggregation cost of the user operations be non-linear in the number of transactions (e.g., some proofs are effectively sub-linear in the size being proven), offering the possibility to amortise the total cost over many users.</p>\n<p>This leads to the following game: The bundler wishes for users to place their bids as if they were bidding for the <em>worst case</em>, where the user is alone in the bundle and must compensate by themselves the full overhead gas <span class=\"math\">F</span>. Practically, the user would be faced with the problem of setting three relevant parameters on their operation:</p>\n<ul>\n<li><code>op.maxPriorityFeePerGas</code> and <code>op.maxFeePerGas</code> may be set according to the heuristics a user would use under EIP-1559, i.e., given some estimate amount of gas their operation plans to consume, the user would set these attributes to calibrate how much they are willing to pay in the worst case (<code>maxFee</code>) and how much they are willing to top up in order to pay the eventual block producer (<code>maxPriority</code>). But how should the user estimate the gas?</li>\n<li><code>op.preVerificationGas</code> is an attribute of the UserOperation which must be set to indicate the amount of “extra gas” the user’s operation plans to consume. In our model, we let <span class=\"math\">F</span> denote this “fixed gas overhead”. If <span class=\"math\">n</span> users were included in the bundle, each user ought to set <code>preVerificationGas = F / n</code>. However, should the user prepare their operation with a worst-case scenario in mind, they would set <code>preVerificationGas = F</code>.</li>\n</ul>\n<p><code>preVerificationGas</code> is then the principal vector via which users mediate their bid and attempt to account for the amortisation of costs by the bundler. Assume <span class=\"math\">n</span> users do come to the market with their operations, and all are convinced by the bundler to bid in the worst-case of being alone in the bundle. We’ll also assume that the users are setting their <code>maxPriorityFeePerGas</code> to zero for the sake of this example. Then these <span class=\"math\">n</span> users are all setting <code>preVerificationGas = F</code>, and the bundler is able to output a bundle remunerating them with:</p>\n<div class=\"math\">\nn \\times F \\times r\n</div>\n<p>while they must incur a cost:</p>\n<div class=\"math\">\nF \\times r\n</div>\n<p>once they publish the bundle bundling all <span class=\"math\">n</span> operations together in a block. This yields the bundler a profit <span class=\"math\">\\pi = (n-1) \\times F \\times r</span>.</p>\n<p>This situation may be represented by a two-stage game, where the users first produce their user operations, and the bundler subsequently decides to bundle them. We assume that users do not possess information about the current amount of pending users, and so are unable to estimate the bundler’s true ability to amortise their fixed costs.</p>\n<p>In the first stage, users send their operations, which commit to their attributes such as <code>preVerificationGas</code>. In the second stage, the bundler having received all user transactions decides to output a bundle or set of bundles. Interestingly, even if the users know how many other users will play in the first stage, i.e., even if <span class=\"math\">n</span> is common knowledge across all users, the bundler may be able to force the users into setting the worst-case <code>preVerificationGas = F</code> by threatening to play <span class=\"math\">\\mathcal{B}_\\text{pessimistic} = \\{ \\{ 1 \\}, \\{ 2 \\}, \\{ 3 \\}, \\dots, \\{ n \\} \\}</span>, i.e., threatening to keep every user in their own separate bundle and charging them the maximum amount gas <span class=\"math\">F</span>.</p>\n<p>Note that this threat may not be credible, as users would expect the bundler to prefer playing <span class=\"math\">\\mathcal{B}_\\text{ideal} = \\{ \\{1, 2, 3, \\dots, n \\} \\}</span>, i.e., output a single bundle with all operations included there, realising <span class=\"math\">\\pi</span>. However, users may not have access to the true value of <span class=\"math\">n</span>, and thus they are unable to set their <code>preVerificationGas</code> in a way that forces the bundler to ideally bundle all of them.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/2/a/2a1e3be0f917af5d2d8d3fa4c487a848543c76b2.png\" data-download-href=\"https://ethresear.ch/uploads/default/2a1e3be0f917af5d2d8d3fa4c487a848543c76b2\" title=\"Ideal case: costs are split between the users in the bundle. Pessimistic case: users overpay, and costs are not split.\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/2/a/2a1e3be0f917af5d2d8d3fa4c487a848543c76b2_2_484x500.png\" alt=\"Ideal case: costs are split between the users in the bundle. Pessimistic case: users overpay, and costs are not split.\" data-base62-sha1=\"60APqXwY3iqF7PWcsFuLI0CLetc\" width=\"484\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/2/a/2a1e3be0f917af5d2d8d3fa4c487a848543c76b2_2_484x500.png, https://ethresear.ch/uploads/default/optimized/3X/2/a/2a1e3be0f917af5d2d8d3fa4c487a848543c76b2_2_726x750.png 1.5x, https://ethresear.ch/uploads/default/original/3X/2/a/2a1e3be0f917af5d2d8d3fa4c487a848543c76b2.png 2x\" data-dominant-color=\"F3EEF1\"></a></div><p></p>\n<p>An extension of this model may consider the Bayesian case, where the users have access to a distribution over <span class=\"math\">n</span>, i.e., they may anticipate some random variable <span class=\"math\">n</span> users to show up at any given time step, according to some distribution (e.g., Poisson arrivals), even if they do not know in advance the outcome of the random variable. This may lead to inefficient outcomes, as the following example shows:</p>\n<blockquote>\n<p>Alice expects 9 other users to show up besides herself, and so she sets her <code>preVerificationGas</code> to 1 as she knows <span class=\"math\">F = 10</span>. Alice’s value and the value of all other users is compatible with them setting <code>preVerificationGas = 3</code>, but she attempts to pay the least amount possible for her inclusion. As it turns out, only 5 users appear on the market, who have all set their <code>preVerificationGas</code> to 1 too. The bundler will not be compensated for <code>F = 10</code> units of gas, thus the bundler does not output a bundle and users receive 0 utility. This is obviously suboptimal, as the users could have all set <code>preVerificationGas = 2</code> for instance and receive 1 utility (the maximum <code>preVerificationGas</code> they were willing to set minus the actual <code>preVerificationGas</code> they paid to be included).</p>\n</blockquote>\n<h2><a name=\"future-work-8\" class=\"anchor\" href=\"https://ethresear.ch#future-work-8\"></a>Future work</h2>\n<p>As the bundler game shows, an allocation problem faces the user wishing to be included by the bundler. In the next note, we will address different approaches to recovering “good UX” for the user to prevent them from overpaying a bundler who is better informed about the demand for its bundle capacity. The next exploration will require an understanding of the market structure tying users, bundlers and builders/block producers together.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/embedded-fee-markets-and-erc-4337-part-1/19542\">Read full topic</a></p>","link":"https://ethresear.ch/t/embedded-fee-markets-and-erc-4337-part-1/19542","pubDate":"Mon, 13 May 2024 09:55:52 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19542"},"source":{"@url":"https://ethresear.ch/t/embedded-fee-markets-and-erc-4337-part-1/19542.rss","#text":"Embedded fee markets and ERC-4337 (part 1)"},"filter":false},{"title":"Sandwitch attacks on ePBS","dc:creator":"potuz","category":"Consensus","description":"<p>In this quick note we analyze the special case of ex-anti and sandwitch attacks on ePBS vs the current implementation. We show that with the proposed values for proposer and builder’s boosts as in <a href=\"https://ethresear.ch/t/payload-boosts-in-epbs/18769\" class=\"inline-onebox\">Payload boosts in ePBS</a> the situation is actually an improvement over the status quo.</p>\n<p>This short note contains the raw numbers and it’s meant to be a quick update, no fancy diagrams, for such I recommend looking at the design notes in <a href=\"https://hackmd.io/@potuz/rJ9GCnT1C\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ePBS specification notes - HackMD</a> or even the forkchoice implementation notes in <a href=\"https://hackmd.io/@potuz/SJdXM43x0\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ePBS Forkchoice annotated spec - HackMD</a></p>\n<h2><a name=\"ex-anti-reorgs-the-need-for-proposer-boost-1\" class=\"anchor\" href=\"https://ethresear.ch#ex-anti-reorgs-the-need-for-proposer-boost-1\"></a>Ex-anti reorgs, the need for proposer boost</h2>\n<p>The classical 1-slot ex-anti reorg goes like this. The proposer of slot <code>N</code> plans to reorg the block of <code>N+1</code>. For this they withhold their block during their time. After the proposer of <code>N+1</code> reveals his block (based on <code>N-1</code>) the attacker reveals their block <code>N</code> together with <span class=\"math\">\\beta</span> attestations for it. The attack is succesful if</p>\n<p><span class=\"math\"> \\beta &gt; PB </span>.</p>\n<p>Which in the current situation makes us resilient to these attacks up to a 40% adversary.</p>\n<h2><a name=\"ex-anti-on-epbs-2\" class=\"anchor\" href=\"https://ethresear.ch#ex-anti-on-epbs-2\"></a>Ex anti on ePBS</h2>\n<p>On ePBS the situation for an ex-anti attack changes due to the (block, slot) voting nature of fork choice. The attack goes as follows.</p>\n<ul>\n<li>Since the proposer of <code>N</code> wants to get their payload included, they can’t simply reveal their block after <code>N+1</code> does. They have to have a timely payload so that the PTC votes for it.</li>\n<li>They therefore reveal their consensus block targeting a split view of the attesters at 1/4 of a slot. <span class=\"math\">1-x</span> of the committee votes for <code>N-1</code>, as they didn’t see the block on time, and <span class=\"math\">x - \\beta</span> vote for <code>N</code> (the adversary withholds their attestations).</li>\n<li>The builder of <code>N</code> reveals on time and the PTC attests to the builder’s presence.</li>\n<li>The proposer of <code>N+1</code> will reveal a block based on <code>N-1</code> only if<br>\n<span class=\"math\"> 1 - 2x &gt; RB - \\beta</span><br>\nwhere RB is the reveal boost that the builder of <code>N</code> received.</li>\n<li>The attacker now reveals their attestations for <code>N</code>.</li>\n</ul>\n<p>The attack is successful if <span class=\"math\"> RB &gt; PB + 1 - 2x</span> But given the above inequality this implies <span class=\"math\">RB &gt; PB + RB - \\beta</span>. Therefore we obtain as in the current status quo <span class=\"math\">\\beta &gt; PB</span>.</p>\n<p>Since in ePBS the proposer boost PB is set to <span class=\"math\">20%</span>. One is inclined to think that we have ex-anti reorg protections only up to 20%, a considerable downgrade from the current implementation. But notice that <strong>the payload of N+1 is not reorged</strong>. In fact, the builder of N+1 will not reveal their block since the head is N when the attack is successful, and because of the <em>builder withholding safety</em>, their bid payment will not be necessary. In fact, in order to reorg the payload as well in ePBS, we would require a sandwich attack.</p>\n<h2><a name=\"sandwich-attacks-the-classical-case-3\" class=\"anchor\" href=\"https://ethresear.ch#sandwich-attacks-the-classical-case-3\"></a>Sandwich attacks the classical case</h2>\n<p>A sandwich attack is very similar to an ex-anti one, but now the adversary is proposing slots N and N+2 and plans to reorg the block N. Their setup is just as in the ex-anti attack: they reveal the block N late, together with <span class=\"math\">\\beta</span> attestations for it. Block <code>N+1</code> is early and receives <span class=\"math\">1 - \\beta</span> attestations (the attacker votes for <code>N</code> during N+1. The attacker then reveals N+2 based on N, obtaining proposer boost and attempting to reorg N+1. The attack is successful if<br>\n<span class=\"math\">2 \\beta + PB &gt; 1 - \\beta \\Leftrightarrow 3\\beta &gt; 1 - PB</span><br>\nFrom where with the current values we obtain protection against this attack against validators up to <span class=\"math\">\\beta = 20\\%</span>.</p>\n<h2><a name=\"sandwich-attack-in-epbs-4\" class=\"anchor\" href=\"https://ethresear.ch#sandwich-attack-in-epbs-4\"></a>Sandwich attack in ePBS</h2>\n<p>In ePBS the sandwich attack starts also as an ex-anti setup. In particular, to get the proposer of N+1 to base their block on N-1, the setup requires<br>\n<span class=\"math\"> 1 - 2x &gt; RB - \\beta</span><br>\nas above. The consensus block of N+1 receives <span class=\"math\">1 - \\beta</span> votes just as in the current implementation, and the builder of N+1 reveals timely obtaining a builder’s boost: this is the main difference, <strong>the builder’s boost makes this sandwich attack much more difficult</strong>.<br>\nThe attacker then reveals their N+2 block based on N. Obtaining proposer boost. The attacker’s branch then has weight <span class=\"math\">PB + \\beta + x</span>. While the canonical branch has weight <span class=\"math\">RB + 1 - \\beta + 1 - x</span>. The attack is successful then if<br>\n<span class=\"math\"> PB + 2\\beta &gt; RB + 1 + (1 - 2x) </span><br>\nWhich according to the inequality above implies <span class=\"math\">PB + 2 \\beta &gt; 2RB + 1 - \\beta</span>, from where<br>\n<span class=\"math\">\\beta &gt; \\frac{2RB +  1 - PB}{3}</span><br>\nWhich with the proposed values of RB = 40% and PB = 20% gives protection against this attack by an attacker up to 50%, a significant improvement over the current situation.</p>\n<p>Multiple slot post-anti reorgs become worse in ePB. To give some numbers, in the current implementation we are resistant to 60% attackers for 1 slot post-anti reorgs and 53% for 2 slots post-anti-reorgs. On ePBS these numbers become 40% and 37%.</p>\n            <p><small>3 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/sandwitch-attacks-on-epbs/19538\">Read full topic</a></p>","link":"https://ethresear.ch/t/sandwitch-attacks-on-epbs/19538","pubDate":"Mon, 13 May 2024 07:18:54 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19538"},"source":{"@url":"https://ethresear.ch/t/sandwitch-attacks-on-epbs/19538.rss","#text":"Sandwitch attacks on ePBS"},"filter":false},{"title":"FullDAS: towards massive scalability with 32MB blocks and beyond","dc:creator":"cskiraly","category":"Sharding","description":"<p>Author: Csaba Kiraly, in collaboration with Leonardo Bautista-Gomez and Dmitriy Ryajov, from the <a href=\"https://codex.storage\" rel=\"noopener nofollow ugc\">Codex.storage</a> research team.</p>\n<p><em>Note: this document describes the current state of our thinking, result of a collaborative effort and numerous discussions with other teams. It would not had been possible without the contribution and ideas of <a class=\"mention\" href=\"https://ethresear.ch/u/dankrad\">@dankrad</a> , <a class=\"mention\" href=\"https://ethresear.ch/u/djrtwo\">@djrtwo</a> , <a class=\"mention\" href=\"https://ethresear.ch/u/fradamt\">@fradamt</a> , <a class=\"mention\" href=\"https://ethresear.ch/u/agemanning\">@AgeManning</a> , <a class=\"mention\" href=\"https://ethresear.ch/u/nashatyrev\">@Nashatyrev</a> , <a class=\"mention\" href=\"https://ethresear.ch/u/matt\">@matt</a> , <a class=\"mention\" href=\"https://ethresear.ch/u/pop\">@pop</a> , and <a class=\"mention\" href=\"https://ethresear.ch/u/echo\">@Echo</a> .</em></p>\n<h2><a name=\"tldr-1\" class=\"anchor\" href=\"https://ethresear.ch#tldr-1\"></a>TL;DR</h2>\n<ul>\n<li>Danksharding was planned for 32MB blocks, but our current <strong>networking stack</strong> can’t handle that, <strong>becoming the bottleneck</strong>. With HW accelerated KZG on the horizon for the block encoding, our networking stack will have to scale even more.</li>\n<li>DAS encompasses two different concepts: <strong>Data Availability</strong> achieved by dispersal to custody, and <strong>Sampling</strong> from custody. We can use this distinction to our advantage, designing an efficient dispersal, and an efficient sampling protocol.</li>\n<li><strong>liteDAS</strong> is our sampling protocol, designed to provide low-latency, bandwidth efficient, and robust sampling.</li>\n<li><strong>Dispersal</strong> can be done with protocols similar to GossipSub, but changes are required.</li>\n<li>With the combination of deterministic custody assignments and <strong>Topic Routing</strong>, we can find peers fast enough for dispersal and for sampling.</li>\n<li>To enable sampling, we should also enable <strong>Ephemeral Connect</strong>, not supported by the current stack.</li>\n<li><strong>2D encoding</strong> (or some other locally repairable code) is required for in-network-repair, the key to <strong>availability amplification</strong>.</li>\n</ul>\n<h2><a name=\"introduction-2\" class=\"anchor\" href=\"https://ethresear.ch#introduction-2\"></a>Introduction</h2>\n<p>The <a href=\"https://notes.ethereum.org/@dankrad/new_sharding\" rel=\"noopener nofollow ugc\">original Danksharding proposal</a> targeted 32 MB blocks, and this size was mainly chosen due to compute constraints for the KZG commitments used in the <a href=\"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963#sampling-over-the-das-encoding-3\">DAS data structure</a>. Since then, there have been various iterations on the network design, aiming to efficiently make the block data available in a P2P structure, and to let nodes sample from this structure. However, none of these constructs convincingly support blocks of 32MB, making the performance of the networking solution the real bottleneck. In this post we look at these networking issues and propose solutions, with the aim of making 32 MB (and possibly beyond) achievable.</p>\n<h3><a name=\"das-data-availability-vs-sampling-3\" class=\"anchor\" href=\"https://ethresear.ch#das-data-availability-vs-sampling-3\"></a>DAS: Data Availability vs. Sampling</h3>\n<p>The goal of DAS (Data Availability Sampling) is to ascertain, with high probability, that a given block of data was made available to anyone interested, and to do this without requiring any single node in the system to hold - or even to temporarily receive - the whole block of data.</p>\n<p>That is, we want to keep the bandwidth requirements of individual nodes at the levels of the current (after EIP-4844) Ethereum network, while handling orders of magnitude larger blocks. From the networking perspective, this is an important constraint on the Ethereum DAS design.</p>\n<p>For our discussion on networking aspects, it is important to emphasize that there are two distinct parts to DAS: the “making data available” part, and the “sampling” part.</p>\n<h4><a name=\"making-data-available-4\" class=\"anchor\" href=\"https://ethresear.ch#making-data-available-4\"></a>Making Data Available</h4>\n<p>First, the data has to be made available. Availability means that nodes in the system get “enough” data to be able together to reconstruct the original block. Thus, availability is a <strong>system level property</strong>: data is either available to the whole system, or not available. At least, this is what we want to achieve.</p>\n<p>As in P2P systems in general, the amount of data should better not be “just enough”, but it should be <strong>overwhelmingly enough</strong>, meaning the data can be reconstructed even if there is churn, network partitioning, or a large portion of malicious nodes. Only at this point we can say that the data was made available.</p>\n<p>In other words, we should design a protocol that makes sure that there are no borderline situations: the data is either not available (wasn’t released by the source), or it is overwhelmingly available. As we will see later, it is the interplay between erasure coding and a robust and largely redundant P2P networking structure that makes this possible.</p>\n<h4><a name=\"sampling-5\" class=\"anchor\" href=\"https://ethresear.ch#sampling-5\"></a>Sampling</h4>\n<p>Second, there is sampling. Sampling is an individual node’s view of what was made available to the system. It is a single node convincing itself that the data was made available, and the technique it uses for this is to retrieve a few pieces of the block. The <strong>sample</strong> is the (typically random) selection of the pieces to retrieve, while <strong>sampling</strong> is the retrieval of these pieces. If this sampling is successful, and with a few <strong>independence assumptions</strong>, the node can convince itself that the data was indeed made available.</p>\n<p>Importantly, in the background, behind the <a href=\"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963#setting-the-sample-size-7\">probabilistic calculations</a>, there are those nasty independence assumptions. Essentially it is assumed that data is (was, or will be) released independent of what sample was selected. This unfortunately can be gamed, and the more the block producer and its “accomplices” knows about the sample selection of a node, the easier it is to introduce correlation and thus fool someone. Thus, <strong>sample selection</strong> and <strong>limited disclosure of the selection</strong> is an important part of the security guarantees.</p>\n<p><em>Note that in this writeup we call the pieces of the block <strong>segments</strong>, but they also go by the name “column” when 1D encoding is used, “cell” when 2D encoding is used, or sometimes the confusing term “sample” is used both the segment as well as for the list of selected segments.</em></p>\n<p><em>Also note that sampling is not the only technique we could use to “convince ourselves” that the data was available. Without trying to list all possibilities, we could use succinct proofs, trusted friends, etc. Sampling, however, is trustless and spreads segments of data to all nodes in the system, eventually enabling an extra layer of redundancy and thus recovery.</em></p>\n<h3><a name=\"interest-custody-vs-sample-6\" class=\"anchor\" href=\"https://ethresear.ch#interest-custody-vs-sample-6\"></a>Interest: Custody vs. Sample</h3>\n<p>Both when data is made available, and when nodes are sampling, segments of the block get delivered to nodes. Which segments get to which node, however, is driven by different objectives in these two phases.</p>\n<p>We call the segments that should be delivered to a node it’s <strong>interest</strong>. We have two types of interest in the system. They are similar, but serve different purposes and have some fundamental differences:</p>\n<ul>\n<li><strong>custody</strong>: segments getting to nodes as part of making the data available are taken into custody and used to serve sampling. That is, custody has a double goal: providing availability and at the same time serving as a sampling infrastructure.</li>\n<li><strong>sample</strong>: sampling is for the individual node to convince itself about availability with very high probability by checking that enough segments are in custody.</li>\n</ul>\n<p>Interest can <strong>change over time</strong>, for example from epoch to epoch for the sample selection, or with some other time granularity for custody. Changing it or keeping it fixed has both security and network efficiency implications.</p>\n<h3><a name=\"das-phases-dispersal-vs-sampling-7\" class=\"anchor\" href=\"https://ethresear.ch#das-phases-dispersal-vs-sampling-7\"></a>DAS Phases: Dispersal vs. Sampling</h3>\n<p>The fundamental differences between the requirements of data availability and sampling, and between the properties of custody and sample, are also reflected in the network design. We can differentiate between two phases of segment distribution:</p>\n<ul>\n<li><strong>dispersal</strong>: in which segments of the block are distributed in the P2P network to provide overwhelming availability and custody.</li>\n<li><strong>sampling</strong>: in which nodes collect a random sample of segments from custody.</li>\n</ul>\n<p>The two phases can use different P2P network constructs. In what follows we focus on these network constructs, deriving fast, robust, and bandwidth efficient protocols both for dispersal and for sampling.</p>\n<h2><a name=\"fulldas-networking-8\" class=\"anchor\" href=\"https://ethresear.ch#fulldas-networking-8\"></a>FullDAS networking</h2>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/9/3/93a404791201f9e15e8d268a971c5fd54d120d7c.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/93a404791201f9e15e8d268a971c5fd54d120d7c\" title=\"FullDAS - LossyDAS - liteDAS - Data Availability Sampling Components\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/9/3/93a404791201f9e15e8d268a971c5fd54d120d7c_2_666x500.jpeg\" alt=\"FullDAS - LossyDAS - liteDAS - Data Availability Sampling Components\" data-base62-sha1=\"l45APdFj7UJvsf2ot9sCYQImCKM\" width=\"666\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/9/3/93a404791201f9e15e8d268a971c5fd54d120d7c_2_666x500.jpeg, https://ethresear.ch/uploads/default/original/3X/9/3/93a404791201f9e15e8d268a971c5fd54d120d7c.jpeg 1.5x, https://ethresear.ch/uploads/default/original/3X/9/3/93a404791201f9e15e8d268a971c5fd54d120d7c.jpeg 2x\" data-dominant-color=\"E4E2E4\"></a></div><p></p>\n<p>In the original design, GossipSub was planned to be used for dispersal, with many (512 for columns and 512 for rows) topics, distributing columns and rows into custody at all the staked nodes (beacon nodes with validators). For sampling, instead, a separate DHT was planned to be set up from all full nodes (staked and non-staked), serving all the sampling queries. This presented several challenges, as we have <a href=\"https://ethresear.ch/t/scalability-limitations-of-kademlia-dhts-when-enabling-data-availability-sampling-in-ethereum/18732\">highlighted in our related post and paper</a>.</p>\n<p>As a consequence, in <strong>PeerDAS</strong> and in <strong>SubnetDAS</strong> we have tweaked the design to provide intermediate solutions, with compromises both in functionality and in scalability.</p>\n<p>In <a href=\"https://ethresear.ch/t/peerdas-a-simpler-das-approach-using-battle-tested-p2p-components/16541\">PeerDAS</a>, we have modified custody assignment and dispersal to be based on the NodeID, making all full nodes (both staked and non-staked) be part of the custody system. Sampling is then made from this P2P structure using a Request/Response (Req/Resp) protocol from existing peers of a node, adding more pressure on peer discovery (Discv5), and the number of peers that nodes have to sustain.</p>\n<p><a href=\"https://ethresear.ch/t/subnetdas-an-intermediate-das-approach/17169\">SubnetDAS</a>, instead, modifies the design to use the GossipSub distribution mechanism all the way, both for dispersal and for sampling. This, again, limits our possibilities, also sacrificing the unlinkability of sampling queries.</p>\n<p>A fast and bandwidth-efficient implementation of these proves to be challenging even for blocks of a few MBs, and this is where we arrive to our main topic: <strong>how to make DAS work for blocks of 32 MB and beyond?</strong></p>\n<p>In what follows, we introduce the important parts of the stack:</p>\n<ol>\n<li>liteDAS: fast and efficient sampling from custody</li>\n<li>Finding custody peers fast:\n<ul>\n<li>Custody allocation and sample selection</li>\n<li>Topic Routing</li>\n<li>Ephemeral Connect</li>\n</ul>\n</li>\n<li>Making dispersal more efficient</li>\n<li>Availability amplification with 2D encoding</li>\n</ol>\n<h3><a name=\"litedas-the-sampling-protocol-9\" class=\"anchor\" href=\"https://ethresear.ch#litedas-the-sampling-protocol-9\"></a>liteDAS: The Sampling Protocol</h3>\n<p>Sampling, at an abstract level, sounds very simple:</p>\n<ul>\n<li>take a random sample of block segment IDs</li>\n<li>ask for these from nodes custodying it</li>\n<li>wait for all the responses</li>\n<li>declare success if received all, otherwise declare failure</li>\n</ul>\n<p>In reality, however, things get much more complicated. We have dealt with some of the complication related to sample selection in <a href=\"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963\">our post on LossyDAS, IncrementalDAS, and DiDAS</a>, but we have left out all the networking aspects from that post.</p>\n<p>To design a sampling protocol, besides the draft outline above, we should also answer questions like:</p>\n<ul>\n<li>when should a node start sampling?</li>\n<li>when should sampling end, what should be the timeout?</li>\n<li>if we know several nodes custodying the same segment, which one to ask?</li>\n<li>what to do if we do not know a node that is supposed to custody a segment?</li>\n<li>what to do when sampling fails?</li>\n<li>etc.</li>\n</ul>\n<p>Moreover, we should design a protocol that is fast, robust, and bandwidth efficient.</p>\n<p>For sampling, we propose <strong>liteDAS</strong>, a new Req/Resp protocol that aims to provide close to minimum sampling latency with close to minimal bandwidth usage, avoiding excessive resource utilization both on the happy path and when there are issues. The key observations behind liteDAS are:</p>\n<ul>\n<li>There is no “perfect time” to ask for a segment, since we don’t know when dispersal will make these arrive to custody. It is better to ask early and wait.</li>\n<li>Sampling nodes have some time (the dispersal time) to prepare for sampling. We can use this time to make sampling efficient.</li>\n<li>We can hide our sample better if we ask for segments one-by-one, asking each from distinct nodes.</li>\n<li>Having a fast and efficient way of finding custody nodes for a given segment is essential.</li>\n</ul>\n<p>The <strong>messaging primitive</strong> we propose is a <strong>request with an explicit timeout</strong>. This simple primitive allows us to start sampling early (at slot start), and build a nice dynamic behavior with an initial period to handle the happy path, and a second period to handle potential issues, as outlined below:</p>\n<ul>\n<li>At <span class=\"math\">T_0</span> (slot start, or when public parameters are known)\n<ul>\n<li>decide sample (list of segment IDs) using local randomness\n<ul>\n<li>eventually consider using <a href=\"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963#didas-steering-away-from-uniform-random-sampling-11\">DiDAS</a> and/or <a href=\"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963#lossydas-accept-partial-sampling-9\">LossyDAS</a></li>\n</ul>\n</li>\n<li>For each ID:\n<ul>\n<li>select list of candidate peers who might custody given ID</li>\n<li>select <span class=\"math\">P=1</span> of these candidate nodes (eventually ranking), and send request with long timeout (e.g. 4s). <span class=\"math\">P</span> is the “parallelism” parameter.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>at <span class=\"math\">T_1=4s</span>\n<ul>\n<li>for every ID still missing\n<ul>\n<li>select next candidate node and send request with short timeout</li>\n<li>use timeouts and the number of outstanding queries to avoid traffic explosion</li>\n<li>eventually search for new candidates (this can also be done proactive, before <span class=\"math\">T_1</span>)</li>\n</ul>\n</li>\n<li>Optionally, extend sample according to <a href=\"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963#incrementaldas-dynamically-increase-the-sample-size-10\">IncrementalDAS</a> with some period</li>\n</ul>\n</li>\n</ul>\n<p>Intuitively, this is fast (reducing sampling latency close to minimum), bandwidth efficient (sending only one Req/Resp per segment on the happy path), and robust (explores sampling options once custody should be there).</p>\n<h4><a name=\"fast-10\" class=\"anchor\" href=\"https://ethresear.ch#fast-10\"></a>Fast</h4>\n<p>Sending the sampling query at the start of the slot allows us to minimize the sampling delay to a single 1-hop latency on top of the dispersal delay (the delay with which the node we queried received the segment or column).</p>\n<p>In more detail, to understand why this is fast, let’s see what would be the fastest. The minimum sampling latency would be what one could achieve if the node asks, right when it knows what samples it needs, all its peers that could have the sample, i.e. all peers that plan to custody that segment. That would however mean that our node receives lots of responses, adding considerable extra load. In liteDAS we only ask <span class=\"math\">P</span> of them at a time, as a compromise between bandwidth efficiency and latency. <a href=\"https://ethresear.ch/t/big-block-diffusion-and-organic-big-blocks-on-ethereum/17346\">Our data on GossipSub latency distribution</a> and our initial simulations show that the latency compromise is very limited on the happy path.</p>\n<h4><a name=\"bandwidth-efficient-11\" class=\"anchor\" href=\"https://ethresear.ch#bandwidth-efficient-11\"></a>Bandwidth efficient</h4>\n<p>The protocol receives every sample (or column) exactly once on the happy path. On the unhappy path, we can set a compromise between following a sequential logic with short timeouts, or allowing some parallelism (and thus duplicates) by changing <span class=\"math\">P</span> in later phases. Importantly, if the data is actually not available, it will still keep resource utilization in limits.</p>\n<h4><a name=\"robust-12\" class=\"anchor\" href=\"https://ethresear.ch#robust-12\"></a>Robust</h4>\n<p>In the first phase, the robustness of the above sampling relies on the robustness of dispersal. Dispersal already employs a number of reliability techniques, thus, when on the happy path, it is very likely that for every ID, the single node we select will receive the sample.<br>\nIf instead there are issues with dispersal, our sampling also becomes less latency sensitive: sampling should anyway not succeed until availability through custody is widespread. Thus, we have time to “hunt” for segments. After the initial phase, our protocol will cycle through potential other peers taking in custody the same segment. At that point, we can also send more queries in parallel, eventually allowing some duplicates.</p>\n<h4><a name=\"possible-extensions-13\" class=\"anchor\" href=\"https://ethresear.ch#possible-extensions-13\"></a>Possible Extensions</h4>\n<p>Beyond the basic idea, there are a few other tweaks that can improve dynamic behaviour:</p>\n<ol>\n<li>Nodes can anticipate the hunt for peers that could provide a given segment. These are the peers that will eventually custody that segment.</li>\n<li>Since sample selection is done using local randomness, nodes could even prepare the sample (list of IDs) well in advance, anticipating the search for candidate peers with slots or even epochs.</li>\n<li>To avoid traffic explosion, we recommend controlling the number of outstanding queries, together with timeouts. This practically means that we can avoid traffic explosion even in cases of an unavailable block or when there are large-scale network issues.</li>\n<li>As mentioned before, asking one node at a time works, but this can be increased to e.g asking 2 or 3 in parallel. These are compromises that can even be made locally, we can’t really mandate them anyway. (Actually we could, using Rate Limiting Nullifier-like techniques, but that’s out of scope here and seems too complex for the scope).</li>\n<li>We could also include a HAVE list in the query response message. The reason behind this is that it gives extra information on the state of diffusion that we would not have otherwise. Given the 1D or 2D structure, a have list can be compressed very well to a small bitmap, so it is not much extra bytes. Eventually, to stay generic on the message primitive, we can make it a flag in the request whether we want a HAVE list in the response.</li>\n<li>Finally, it is worth mentioning that sampling nodes could use the information collected during protocol execution to feed back segments into custody. One way to make this happen is to introduce a Resp with a NACK (negative acknowledgment) style message when the timeout has passed and the segment is not in custody. This “counter-request” would mean that the sampling node should send the segment when it receives it from someone.</li>\n</ol>\n<h3><a name=\"finding-custody-peers-fast-14\" class=\"anchor\" href=\"https://ethresear.ch#finding-custody-peers-fast-14\"></a>Finding Custody Peers Fast</h3>\n<p>liteDAS still assumes we have the peers to send requests to. What if we don’t have these, and we have to search for peers taking in custody a given ID? What if we also have to go through a convoluted connection procedure before requesting the segment?</p>\n<p>As highlighted before, we can anticipate the search for custody peers. Still, there are times when we need to search for peers with a specific ID in custody, and that should be relatively fast. This is a crucial point currently being debated in discussions around DAS. As part of the sampling process, we need to find and connect to peers in a few seconds, which seems to be one of the main limiting factors. There is a similar problem for dispersal as well when building the mesh for efficient column(/row) based distribution, although in that case timing is less critical.</p>\n<p>The problem with the current protocol stack was summarized well by AgeManning as <a href=\"https://github.com/ethereum/consensus-specs/pull/3623#issuecomment-2046558155\" rel=\"noopener nofollow ugc\">part of the PeerDAS spec discussions</a>. Getting individual segments from new peers is awfully inefficient with the current stack. Clearly, the problem has two mayor underlying aspects:</p>\n<ul>\n<li><strong>Finding nodes is inefficient</strong>: currently this would be done using Discv5. Since Discv5 has no topic search, it is a random walk on the DHT, enumerating peers in hope of finding a good one. We propose to make this faster by deterministic custody assignment and by what we call “topic routing”.</li>\n<li><strong>Connecting to nodes is inefficient</strong>: we intend to solve this by introducing “ephemeral connect”.</li>\n</ul>\n<h4><a name=\"deterministic-nodeid-based-custody-assignment-15\" class=\"anchor\" href=\"https://ethresear.ch#deterministic-nodeid-based-custody-assignment-15\"></a>Deterministic NodeID-based Custody Assignment</h4>\n<p>First, lets see what we can do to find custody nodes for a given segment fast. The key to this is to assign custody wisely.</p>\n<p>Since we need system-wide availability, custody has to be assigned properly, providing good coverage. A randomized local selection would provide this, but fortunately we can do better. A nice property of custody is that we <strong>do not need to hide who takes custody of what</strong>. We can use this property to make our system more efficient by <strong>deriving custody interest from the NodeID</strong> (eventually with some rotation scheme and using public randomness) in a deterministic way. It seems that publicly exposing the interest of nodes for the purposes of custody does not influence security. It also seems that we can expose interest publicly well ahead of time (keeping custody stable in time, similar to what we do with attestations today), and it still does not create security issues.</p>\n<p><em>Note that there is a catch here: by using the NodeID, we are not binding custody to validators, as in the original proposal, but to full nodes (including non-staked nodes). Thus, we cannot really mandate custody. We currently do not want to mandate it anyway, but later we might introduce <a href=\"https://dankradfeist.de/ethereum/2021/09/30/proofs-of-custody.html\" rel=\"noopener nofollow ugc\">Proof Of Custody</a>, and then we have to revisit this point. It is also relatively easy to create a large amount of new NodeIDs, potentially exposing custody to targeted sybil attacks.</em></p>\n<p>By deriving custody from NodeID, we can speed up search in two ways. First, we can derive custody from the leftmost bits of the nodeID. This allows us to use the Discv5 Kademlia search instead of a random walk, since Kademlia is prefix-based. Second, if a custody rotation scheme is used (nodes changed what columns/rows they custody with some period), we can calculate this from the NodeID, without contacting the peers or getting updated ENR records from Discv5.</p>\n<p>We already use this NodeID-based deterministic custody assignment trick in PeerDAS: it allows us to make the dispersal more efficient, and it also allows sampling peers to find custody nodes easier, just based on the NodeID and a small amount of metadata published together with the NodeID in the ENR.</p>\n<p><em>Note that the same is not true for the sample, where we do care about making it relatively hard to figure out what will be sampled by a given node, as otherwise we would make it easy to attack individual nodes. Hence, we cannot derive the sample (list of segment IDs) from the NodeID, and we also better avoid sending the whole sample (again, the list of IDs, not the data) to a single node. We thus prefer nodes to request samples on-the-fly, preferably exposing interest selectively to a diverse set of peers, limiting the exposure of the individual node.</em></p>\n<h4><a name=\"topic-routing-efficiently-finding-nodes-for-a-given-column-or-row-16\" class=\"anchor\" href=\"https://ethresear.ch#topic-routing-efficiently-finding-nodes-for-a-given-column-or-row-16\"></a>Topic Routing: efficiently finding nodes for a given column or row</h4>\n<p>Another option we have at hand is to introduce custody node search into our protocol directly, circumventing the difficulties with Discv5.</p>\n<p>In current proposals, columns are distributed using GossipSub, with different column IDs mapping to different GossipSub topics. The column ID space is seen as a flat list, without considering any relation between topics.</p>\n<p>However, <strong>we do not have to handle the row and column ID space as a flat ID space</strong>. We can use almost any distance metric, and require nodes to keep a few neighbors with column/row ID close to them, enabling faster search. Some examples to explain how this might work:</p>\n<ul>\n<li>Circlular: if a node participates in column C, it has to keep at least N neighbors from the range <code>[C-D, C+D] mod NUM_COLS</code>, for some selected <code>D</code> and <code>N</code></li>\n<li>Hypercube style:  if a node participates in column C, it has to keep at least N neighbors from columns <code>C+2^j mod NUM_COLS</code>, for <code>j in [0..log2(NUM_COLS}-1]</code></li>\n<li>Kademlia style:  if a node participates in column C, it has to keep at least N neighbors from the set of columns we get by keeping the j MSB (most significant) bits fixed, and flipping bit j+1</li>\n</ul>\n<p>The first one (circular) is not really scalable, the last two have good scalability. However, if we keep the number of columns(and rows) sufficiently low, any of these will work good enough to find potential neighbors for any C. If we want to scale higher, we might want to pick the best one for our use.</p>\n<p>Note that the topic ID space is dense, with each ID representing a topic, and the number of nodes we have is expected to be higher than the number of topics. In a typical DHT, instead, keys and node IDs are sparse in the ID space. Having a dense ID space allows us to simplify search.</p>\n<p>It is relatively easy to extend any of the above techniques to handle both rows and columns, either by handling the column and row ID spaces as two separate ID spaces, or by defining a joint distance function.</p>\n<p>Also note that when sampling in 2D, we look for either a node in column C, or a node in row R. Any of these will suffice, making our search faster.</p>\n<h4><a name=\"ephemeral-connect-fast-connect-for-sample-retrieval-17\" class=\"anchor\" href=\"https://ethresear.ch#ephemeral-connect-fast-connect-for-sample-retrieval-17\"></a>Ephemeral Connect: fast connect for sample retrieval</h4>\n<p>Connection issues are mainly due to a protocol limitation, since we are operating over libP2P, which was not originally designed for fast connectivity. We make our life even harder by keeping a hard limit on connected peers, and by keeping that full in many nodes (it is known that some peers do not accept connections, and as a consequence others are overloaded, having their peer count full, and thus also not accepting new connections).</p>\n<p>We list a few possible solutions to this problem below:</p>\n<ul>\n<li>libP2P-ephemeralConnect: we could add a new flag to the connect request indicating a connection that is timing out fast. This can be allowed-in on the receiving side above peer count limits.</li>\n<li>Use a new primitive outside of the libp2p framework: If we want a secure encrypted protocol, but something more efficient than a libP2P modification, we can e.g. easily reuse our Discv5 DHT primitives here, making a Discv5 request in a handshake. That employs only 2 roundtrips, and a few bytes of data. Of course, this is just an example of what could be achieved. Other custom protocols with similar latency/bandwidth characteristics can be derived as well.</li>\n<li>Query forwarding: If we really do not want to add another primitive, we can introduce query forwarding. A node can take one of it’s peers according to Topic Routing, and send to it a forwardable query. We’ll get the sample in 2 hops.</li>\n</ul>\n<p>In our opinion these can solve the connectivity issues, getting us to FullDAS. However, more testing is needed.</p>\n<h3><a name=\"the-dispersal-protocol-18\" class=\"anchor\" href=\"https://ethresear.ch#the-dispersal-protocol-18\"></a>The Dispersal Protocol</h3>\n<p>Initially, we intend to use GossipSub for dispersal for several reasons. It is a publish-subscribe protocol which maps well to the erasure coding structure, creating separate topics per column (if using 1D erasure coding), or for columns and rows (if using 2D erasure coding). It is also battle-tested, already used in Ethereum for block diffusion and attestation aggregation, showing fast and reliable operation, as discussed in <a href=\"https://ethresear.ch/t/big-block-diffusion-and-organic-big-blocks-on-ethereum/17346\">our related post measuring latency distributions for block diffusion and for attestations</a>.</p>\n<p>In PeerDAS, we are in fact mapping the dispersal problem to a relatively small number of GossipSub topics. While this is a reasonable first approach, there are aspects that are specific to the dispersal problem and allow for performance optimisations.</p>\n<h4><a name=\"making-dispersal-even-more-efficient-19\" class=\"anchor\" href=\"https://ethresear.ch#making-dispersal-even-more-efficient-19\"></a>Making dispersal even more efficient</h4>\n<p>We have studied how fast blocks can be dispersed into custody using <a href=\"https://github.com/codex-storage/das-research\" rel=\"noopener nofollow ugc\">our custom DAS simulator</a>, both for the 1D and for the 2D erasure coded case. If column and row topics are correctly populated, GossipSub can disperse the block data in seconds to thousands of nodes, as shown in <a href=\"https://www.youtube.com/watch?v=M-xkP4FzYMQ\" rel=\"noopener nofollow ugc\">our presentations at EthereumZurich’23</a>, and in later presentations (<a href=\"https://www.youtube.com/watch?v=N1e_LDrKxZg\" rel=\"noopener nofollow ugc\">EDCON’23</a>, <a href=\"https://www.youtube.com/watch?v=Al7Jns8bCO4\" rel=\"noopener nofollow ugc\">EthPrague’23</a>, <a href=\"https://www.youtube.com/watch?v=pUAVEbzLHLk\" rel=\"noopener nofollow ugc\">EthCC’23</a>). However, in these talks we are speaking of GossipSub-like pub-sub protocols for a reason. GossipSub, as is, is not the best option we have. We argue that we can make a few changes to make it work well for our case. Eventually, we might also derive a custom protocol specifically targeting the dispersal use case.</p>\n<p>In what follows, we list a few of the possible differences between a generic use of GossipSub topics and dispersal, showing some of the optimizations that can be made. We will dedicate a separate post to extend these in detail.</p>\n<h5><a name=\"compared-to-generic-gossipsub-topics-das-topics-are-extremely-structured-20\" class=\"anchor\" href=\"https://ethresear.ch#compared-to-generic-gossipsub-topics-das-topics-are-extremely-structured-20\"></a>Compared to generic GossipSub topics, DAS “topics” are extremely structured</h5>\n<ul>\n<li>With the 512 row and 512 column structure, we can map any ID into 1+9 bits (1 bit for direction, 9 bits for position) or a 9+9 bits ID space.</li>\n<li>We can then use compact representations in message IDs, in HAVE lists, and in many other data structures.</li>\n<li>Given the compact representation, we can also piggyback diffusion state information easily on other messages.</li>\n</ul>\n<h5><a name=\"with-many-topics-finding-peers-can-become-hard-21\" class=\"anchor\" href=\"https://ethresear.ch#with-many-topics-finding-peers-can-become-hard-21\"></a>With many topics, finding peers can become hard</h5>\n<ul>\n<li>We address this partly by deterministic NodeID-based custody assignment,</li>\n<li>Partly by “topic routing” based on a topic ID proximity metric.</li>\n</ul>\n<h5><a name=\"das-segments-of-a-single-block-are-unordered-which-we-can-use-to-our-benefit-22\" class=\"anchor\" href=\"https://ethresear.ch#das-segments-of-a-single-block-are-unordered-which-we-can-use-to-our-benefit-22\"></a>DAS segments of a single block are unordered, which we can use to our benefit</h5>\n<ul>\n<li>We can speed up delivery by “rarest first” like techniques. Rarest-fist scheduling is used in P2P dissemination to cut the tail of the latency distribution, equalizing the diffusion of different pieces. Rarest-first requires precise “have” information, which is typically not available, but it can be approximated. For example, a technique we have already implemented in our simulators is to make sure a copy of every single segment is sent out by the block producer before sending the second copy to another neighbor. We start by sending only one copy of each segment, to one of the block producer’s column/row neighbors (randomized), seeding segments in random places in the network. Only then we send second copies to other peers. Later, one can use information collected through e.g. IHAVE bitmaps to estimate which segments are diffusing well, and which seem to be hindered, prioritizing further seeding based on this information.</li>\n<li>We might also speed up delivery by “node coloring” techniques: these techniques are about specializing nodes to prioritize the distribution of a specific subset of the ID space they custody.</li>\n</ul>\n<h5><a name=\"with-tcp-we-pay-the-cost-of-in-order-reliable-delivery-23\" class=\"anchor\" href=\"https://ethresear.ch#with-tcp-we-pay-the-cost-of-in-order-reliable-delivery-23\"></a>With TCP we pay the cost of in-order reliable delivery</h5>\n<ul>\n<li>GossipSub defaults to using in-order reliable delivery over the topic mesh, which has an inherent cost.</li>\n<li>DAS does not need ordering between segments (to the contrary, shuffling is beneficial).</li>\n<li>DAS does not need P2P link-level reliability (because of all the other reliability techniques we have: multi-path, EC, pull, cross-feeding between columns and rows).</li>\n<li>Thus, using e.g. randomized best-effort delivery can provide overall better performance.</li>\n</ul>\n<h5><a name=\"duplicate-reduction-techniques-24\" class=\"anchor\" href=\"https://ethresear.ch#duplicate-reduction-techniques-24\"></a>Duplicate reduction techniques</h5>\n<ul>\n<li>With GossipSub we pay a relatively high bandwidth cost of duplicate deliveries. Basically, every single message traverses every topic mesh link at least once. Sometimes even twice, when sent from both sides close in time (less than the one-way latency). This means that on average, every message is sent at least D/2 times, where D is the target mesh degree.</li>\n<li>These duplicates serve multiple purposes: contribute to robustness, to low latency, and to protecting sender identity. However, in DAS, we do not need to protect sender identity. Because of the traffic volume, we can’t protect it. This, and the fact that we are distributing over several topics in parallel, allow for optimizations.\n<ul>\n<li>One option is to use the IDONTWANT proposal</li>\n<li>We might also use techniques based on diffusion state, such as Push-Pull phase transition: in the first steps of the diffusion of a segment, there are almost no duplicates. Most of the duplicate load is happening in the last steps of the diffusion. One can emphasize Push at the beginning (even with an increased degree, sending to more nodes than without this technique), and change to gossip-and-pull towards the end of diffusion if the diffusion state can be estimated.</li>\n</ul>\n</li>\n</ul>\n<h3><a name=\"the-need-for-2d-encoding-availability-amplification-through-in-network-repair-25\" class=\"anchor\" href=\"https://ethresear.ch#the-need-for-2d-encoding-availability-amplification-through-in-network-repair-25\"></a>The Need for 2D Encoding: availability amplification through in-network repair</h3>\n<p>Erasure Coding is mostly presented in DAS descriptions as a tool to make sampling more robust. However, we should highlight that we can use Erasure Coding in the design for two purposes:</p>\n<ol>\n<li>to amplify sampling, and</li>\n<li>to amplify availability.</li>\n</ol>\n<p><strong>Amplifying sampling</strong> is well understood: using a code with rate R=K/N (e.g. 1/2), and assuming the data is not available, every single sampled segment reduces the chance of not finding out about the data not being available by a factor of 2. We like this exponential property, letting us reach a very low False Positive (FP) rate with only a few segments sampled. In the 2D case, the ratio is not as good as in 1D, but we still have a nice exponential property.</p>\n<p><strong>Amplifying availability</strong> is a different thing. It means that if data is available, we make it overwhelmingly available. We achieve this by using <strong>in-network repair</strong> during dispersal. This is where our erasure coding structure and dispersal network structure meet. By organising nodes according to the code, in columns and rows, and by making nodes participate in both rows and columns, we enable local repair in the EC structure during dispersal.</p>\n<p>This in-network repair is then amplifying availability, as we have shown in simulations already in our <a href=\"https://youtu.be/M-xkP4FzYMQ?t=1183\" rel=\"noopener nofollow ugc\">EthereumZuri.ch’23 talk</a>. Basically, if not enough samples are released, segment count stays below 75%, while row/coumn-based sampling (SubnetSampling), used by validators, and also by full nodes in many of our constructs, remains very low. Segment sampling also remains very low.</p>\n<p>If instead one more segment is released, the result is amplified by the dispersal structure to almost 100%.</p>\n<p>For this amplification mechanism to work, however, we need the 2D code. We cannot really do it in the 1D case, because in the 1D case, repair could only be done in nodes having K pieces, basically the whole block. In the 2D case, instead, we can repair in any single row or column, so individual nodes can repair and then send. In other words, we need a code with local repair. We can make such a code with 2D RS (there are also other code constructs with local repair capabilities, if we want).</p>\n<p>Thus, while we could map segment IDs to custody in many ways, we do it by columns and rows to enable in-network repair and availability amplification during dispersal.</p>\n<h2><a name=\"conclusions-26\" class=\"anchor\" href=\"https://ethresear.ch#conclusions-26\"></a>Conclusions</h2>\n<p>Our aim with this post was to outline the main building blocks of FullDAS, the networking stack we propose for DAS with blocks of 32MB and beyond. We have discussed new ways to disperse the block data into a P2P structure of custody, while also amplifying data availability. We have also shown the tools needed to implement fast, bandwidth-efficient, and robust sampling from this structure, using liteDAS, Topic Routing, and improved connection mechanisms.</p>\n<p>At this point it might be evident to many that we are building something similar to a DHT, a point raised a few times in the past: the difference between dispersal to custody and sampling, and the generic concept of a DHT is not that big. With FullDAS we are in fact building a special purpose distributed storage structure, with custom seeding, repair, and retrieval, each optimized for the DAS encoding and for the DAS purpose.</p>\n<p>Some of the above techniques have already been evaluated in simulation, while others are work in progress. We have written two simulators for this purpose:</p>\n<ul>\n<li><a href=\"https://github.com/codex-storage/das-research\" rel=\"noopener nofollow ugc\">one with a high-level of abstraction</a>, written in Python, for large scale experiments. Here protocol behaviour is approximated, but a larger parameter space can be explored.</li>\n<li><a href=\"https://github.com/cskiraly/das-simulator-nim\" rel=\"noopener nofollow ugc\">one with a lower level abstraction</a>, using directly the nim-libP2P stack. We already used this simulator to simulate diffusion with a modified GossipSub implementation with 128 columns and 128 rows, and to run sampling from this structure to 1000s of nodes.</li>\n</ul>\n<p>Both of these are work in progress, and we plan to release further posts as the refinement of FullDAS protocols and the evaluation goes on.</p>\n<h2><a name=\"references-27\" class=\"anchor\" href=\"https://ethresear.ch#references-27\"></a>References</h2>\n<p><a href=\"https://notes.ethereum.org/@dankrad/new_sharding\" rel=\"noopener nofollow ugc\">The original Danksharding proposal</a></p>\n<p><a href=\"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963#sampling-over-the-das-encoding-3\">The DAS data structure</a></p>\n<p><a href=\"https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963\">Our previous post on sampling techniques: LossyDAS, IncrementalDAS, and DiDAS</a></p>\n<p><a href=\"https://ethresear.ch/t/scalability-limitations-of-kademlia-dhts-when-enabling-data-availability-sampling-in-ethereum/18732\">Scalability limitations of using Kademlia DHTs for DAS</a></p>\n<p><a href=\"https://ethresear.ch/t/peerdas-a-simpler-das-approach-using-battle-tested-p2p-components/16541\">The PeerDAS proposal</a></p>\n<p><a href=\"https://ethresear.ch/t/subnetdas-an-intermediate-das-approach/17169\">The SubnetDAS proposal</a></p>\n<p><a href=\"https://ethresear.ch/t/big-block-diffusion-and-organic-big-blocks-on-ethereum/17346\">Our post on measuring GossipSub latency distribution, both when used for blocks and when used for attestations</a></p>\n<p><a href=\"https://github.com/ethereum/consensus-specs/pull/3623#issuecomment-2046558155\" rel=\"noopener nofollow ugc\">Part of PeerDAS specification discussions about peer search and connectivity issues</a></p>\n<p><a href=\"https://dankradfeist.de/ethereum/2021/09/30/proofs-of-custody.html\" rel=\"noopener nofollow ugc\">Proof Of Custody</a></p>\n<p>Our previous presentations on DAS:</p>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=M-xkP4FzYMQ\" rel=\"noopener nofollow ugc\">EthereumZurich’23</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=N1e_LDrKxZg\" rel=\"noopener nofollow ugc\">EDCON’23</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=Al7Jns8bCO4\" rel=\"noopener nofollow ugc\">EthPrague’23</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=pUAVEbzLHLk\" rel=\"noopener nofollow ugc\">EthCC’23</a>)</li>\n</ul>\n<p>Our DAS simulators:</p>\n<ul>\n<li><a href=\"https://github.com/codex-storage/das-research\" rel=\"noopener nofollow ugc\">DAS simulator with a high-level of abstraction</a>, written in Python, for large scale experiments</li>\n<li><a href=\"https://github.com/cskiraly/das-simulator-nim\" rel=\"noopener nofollow ugc\">DAS simulator with a lower level abstraction</a>, using directly the nim-libP2P stack over Shadow</li>\n</ul>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/fulldas-towards-massive-scalability-with-32mb-blocks-and-beyond/19529\">Read full topic</a></p>","link":"https://ethresear.ch/t/fulldas-towards-massive-scalability-with-32mb-blocks-and-beyond/19529","pubDate":"Sat, 11 May 2024 15:29:25 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19529"},"source":{"@url":"https://ethresear.ch/t/fulldas-towards-massive-scalability-with-32mb-blocks-and-beyond/19529.rss","#text":"FullDAS: towards massive scalability with 32MB blocks and beyond"},"filter":false},{"title":"MACI with mostly-off-chain \"happy path\"","dc:creator":"vbuterin","category":"Cryptography","description":"<p><em>For background, see: <a href=\"https://ethresear.ch/t/minimal-anti-collusion-infrastructure/5413\" class=\"inline-onebox\">Minimal anti-collusion infrastructure</a></em></p>\n<p>One of the challenges of MACI is that it requires data to be submitted on-chain for each vote, which incurs significant transaction fees. This post suggests a mechanism by which votes can be off-chain by default; a vote would only need to go on-chain if the coordinator is actively attempting to censor voters.</p>\n<p>When a user makes a vote, take the following steps:</p>\n<ol>\n<li>The user locally generates their vote, as in regular MACI</li>\n<li>The user sends their vote to the coordinator</li>\n<li>The coordinator replies with a signature, specifying which position of the next batch the user’s vote will be included at</li>\n<li>When the coordinator submits their next batch, they submit only a hash to the chain. The hash must be the Merkle root of all votes that have been sent to the coordinator since the previous round. The coordinator is also required to publish the full set of votes (eg. on IPFS).</li>\n</ol>\n<p>The ZK-SNARK enforces that the final vote results are the result of processing all submitted votes, including votes that have been published via this hash mechanism.</p>\n<p>Users have access to two extra features:</p>\n<ol>\n<li>They have the ability to submit a vote directly to chain. The coordinator is forced to process both types of votes (included via hash, and included directly)</li>\n<li>If a user has a cryptographically signed promise, they can publish that promise on chain as a challenge. Any future message that the coordinator submits (a batch or the final result) must come with a SNARK proving that the correct votes have been included at the provided challenge positions.</li>\n</ol>\n<p>This ensures the following properties:</p>\n<ul>\n<li>Assuming an honest coordinator, onchain costs go down to O(1) per batch period</li>\n<li>Censorship resistance is maintained, because users can go onchain worst-case</li>\n<li>We mitigate attacks where the coordinator pretends to accept a vote, but then fails to include it, hoping that most voters will not notice or re-open any kind of software daemon, by providing signatures:\n<ul>\n<li>If a user fails to get a signature immediately, they go straight to voting onchain.</li>\n<li>If a user gets a signature, and they do come back to check after the batch, they can check on IPFS (or ask the coordinator) for the Merkle branch associated with their vote; if it does not match their signature, they can publish their signature to chain, which effectively halts the entire vote and prevents it from giving a result. Hence, trying to censor even one voter becomes extremely risky for a coordinator.</li>\n</ul>\n</li>\n</ul>\n            <p><small>5 posts - 4 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/maci-with-mostly-off-chain-happy-path/19527\">Read full topic</a></p>","link":"https://ethresear.ch/t/maci-with-mostly-off-chain-happy-path/19527","pubDate":"Sat, 11 May 2024 10:47:47 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19527"},"source":{"@url":"https://ethresear.ch/t/maci-with-mostly-off-chain-happy-path/19527.rss","#text":"MACI with mostly-off-chain \"happy path\""},"filter":false},{"title":"Based proposer commitments - Ethereum’s marketplace for proposer commitments","dc:creator":"DrewVanderWerff","category":"Block proposer","description":"<p><em>As always, humbled by the Ethereum community’s willingness to review / provide feedback. Thank you <a href=\"https://twitter.com/barnabemonnot\" rel=\"noopener nofollow ugc\">Barnabe</a>, <a href=\"https://twitter.com/cshg0x\" rel=\"noopener nofollow ugc\">Chris</a>, <a href=\"https://twitter.com/ConorMcMenamin9\" rel=\"noopener nofollow ugc\">Conor</a>, <a href=\"https://twitter.com/ellierdavidson\" rel=\"noopener nofollow ugc\">Ellie</a>, <a href=\"https://twitter.com/jasnoodle\" rel=\"noopener nofollow ugc\">Jason</a>, <a href=\"https://twitter.com/mempirate\" rel=\"noopener nofollow ugc\">Jonas</a>, <a href=\"https://twitter.com/drakefjustin\" rel=\"noopener nofollow ugc\">Justin</a>, <a href=\"https://twitter.com/_julianma\" rel=\"noopener nofollow ugc\">Julian</a>, <a href=\"https://twitter.com/kubimensah\" rel=\"noopener nofollow ugc\">Kubi</a>, <a href=\"https://twitter.com/0xkydo\" rel=\"noopener nofollow ugc\">Kydo</a>, <a href=\"https://twitter.com/mikeneuder\" rel=\"noopener nofollow ugc\">Mike</a>, <a href=\"https://twitter.com/pascalstichler\" rel=\"noopener nofollow ugc\">Pascal</a>, and <a href=\"https://twitter.com/sjerniganIV\" rel=\"noopener nofollow ugc\">Sam</a> for the feedback and review.</em></p>\n<p><strong>Introduction:</strong></p>\n<p>Over the last year, proposer commitments have become more widely discussed. I personally became proposer-commitment-pilled on the back of the <a href=\"https://ethresear.ch/t/based-rollups-superpowers-from-l1-sequencing/15016\">based sequencing</a> / <a href=\"https://ethresear.ch/t/based-preconfirmations/17353\">preconf</a> posts and <a href=\"https://docs.google.com/document/d/1FG3nKQdUNb_YHCp_IzSDkC_r7A6HOT11O2YNUjCX-6s/edit#heading=h.2whbk0my4lq5\" rel=\"noopener nofollow ugc\">weekly calls</a> being held by Justin as well as discussions and <a href=\"https://frontier.tech/ethereums-blockspace-future\" rel=\"noopener nofollow ugc\">research</a> about blockspace futures. With this interest came the dive down the rabbit hole and as always within the Ethereum community, I found many others already there or willing to join. This post I hope contributes to the ideas already out there and continues to push the discussion around proposer commitments and engagement across the community.</p>\n<p>Below we introduce a concept focused on standardizing the last mile of communication between a proposer and a third party and how proposers may register and make / receive commitments. We see proposer commitments as another promising evolution of Ethereum’s original core vision that will expand the infinite garden to not just be THE marketplace for credible blockspace, but also THE marketplace for credible proposer commitments. We outline some background and motivation, design principles, an initial high-level design, and some open questions. We plan to continue to expand on this with more detailed specs as we gather more feedback and input!</p>\n<p><strong></strong></p><div class=\"lightbox-wrapper\"><strong><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/f/9/f9246140c4556cd087cf9de5c5a0f5e363ada4b4.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/f9246140c4556cd087cf9de5c5a0f5e363ada4b4\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/f/9/f9246140c4556cd087cf9de5c5a0f5e363ada4b4_2_475x316.jpeg\" alt=\"\" data-base62-sha1=\"zy0PGLz0KLAEgJlSwhqSwUcfetK\" width=\"475\" height=\"316\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/f/9/f9246140c4556cd087cf9de5c5a0f5e363ada4b4_2_475x316.jpeg, https://ethresear.ch/uploads/default/optimized/3X/f/9/f9246140c4556cd087cf9de5c5a0f5e363ada4b4_2_712x474.jpeg 1.5x, https://ethresear.ch/uploads/default/original/3X/f/9/f9246140c4556cd087cf9de5c5a0f5e363ada4b4.jpeg 2x\" data-dominant-color=\"AF938A\"></a></strong></div><p></p>\n<p><strong>TL;DR:</strong></p>\n<ul>\n<li>Proposer commitments have been an important part of Ethereum’s history and could continue to be a powerful unlock for Ethereum</li>\n<li>The potential impact of proposer commitments are best captured in a quote from Barnabe’s <a href=\"https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ\" rel=\"noopener nofollow ugc\">recent post</a>; the “…proposer creates the specs, or the template, by which the resulting block must be created, and the builders engaged by the proposer are tasked with delivering the block according to its specifications”</li>\n<li>Over the last year, there have been multiple ideas around proposer commitments. For instance, even in a short period, we have seen multiple implementations of proposer commitments related to preconfs</li>\n<li>While powerful, if multiple standards arise around how proposers register and make / receive commitments, we run the risk of fragmentation that could increase risks to Ethereum</li>\n<li>We present a potential design referred to as “Commitment Boost”[1] inspired by research around PEPC, EigenLayer, and the broader Ethereum community</li>\n<li>This envisions leveraging existing pipes to allow proposers to register and make / receive commitments and remain fully backward compatible</li>\n<li>The principles we embrace for this require open source / development in the open, contemplate modularity and self-containment for safety, integrate a robust suite of testing / off-the-shelf alerts / data APIs, and be performant / efficient</li>\n<li>We end with a few items around risks / additional questions we want to engage with the community on</li>\n</ul>\n<p>Last, we want to note that this post is focused on the last mile of communication to allow proposers to register / send / receive commitments. We do not discuss how the proposer commitment protocols may work or how they may be enforced.</p>\n<p><strong>Related Work:</strong></p>\n<ul>\n<li><a href=\"https://ethresear.ch/t/unbundling-pbs-towards-protocol-enforced-proposer-commitments-pepc/13879\">Unbundling PBS</a></li>\n<li><a href=\"https://efdn.notion.site/PEPC-FAQ-0787ba2f77e14efba771ff2d903d67e4\" rel=\"noopener nofollow ugc\">PEPC FAQ</a></li>\n<li><a href=\"https://hackmd.io/@bchain/BJkarrEWp\" rel=\"noopener nofollow ugc\">PEPC-Boost</a></li>\n<li><a href=\"https://ethresear.ch/t/pepc-dvt-pepc-with-no-changes-to-the-consensus-protocol/16514\">PEPC-DVT</a></li>\n<li><a href=\"https://ethresear.ch/t/the-preconfirmation-gateway-unlocking-preconfirmations-from-user-to-preconfer/18812\">Preconfirmation Gateway</a></li>\n<li><a href=\"https://ethresear.ch/t/based-preconfirmations/17353\">Based preconfirms</a><br>\n<a href=\"https://www.youtube.com/watch?v=2IK136vz-PM\" rel=\"noopener nofollow ugc\">Presentation </a>on preconfs and <a href=\"https://docs.google.com/presentation/d/1v429N4jdikMIWWkcVwfjMlV2LlOXSawFCMKoBnZVDNU/edit#slide=id.p\" rel=\"noopener nofollow ugc\">slides</a></li>\n<li><a href=\"https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ\" rel=\"noopener nofollow ugc\">More Pictures about Proposers and Builders</a></li>\n<li><a href=\"https://ethresear.ch/t/towards-an-implementation-of-based-preconfirmations-leveraging-restaking/19211\">Preconfirm protocol</a></li>\n<li><a href=\"https://ethresear.ch/t/uncrowdable-inclusion-lists-the-tension-between-chain-neutrality-preconfirmations-and-proposer-commitments/19372\">Chain Neutrality and Uncrowdable Inclusion Lists</a></li>\n<li><a href=\"https://ethresear.ch/t/grounded-relay-superpowers-from-relay-coordination/18601\">Grounded Relay</a></li>\n</ul>\n<p><strong>Background:</strong></p>\n<p>Nearly half a decade ago, <a href=\"https://arxiv.org/abs/1904.05234\" rel=\"noopener nofollow ugc\">Flashboys 2.0</a> was published highlighting how arbitrage bots were challenging the promise of blockchains. On the back of this, some of the authors and community members started a research collective to provide solutions to tackle these challenges. In the end, these efforts created a product more broadly known as <a href=\"https://github.com/flashbots/mev-boost\" rel=\"noopener nofollow ugc\">MEV-boost</a>.</p>\n<p>MEV-boost is a middleware that allows the proposer to make a wholesale commitment that outsources block building to a sophisticated actor called a block builder. In return, these block builders pack a block to earn the highest fee for the proposer. Today, <a href=\"https://mevboost.pics/\" rel=\"noopener nofollow ugc\">over 90% of blocks</a> are built with proposer commitments.</p>\n<p><strong>Proposer Commitments:</strong></p>\n<p>On the back of a few developments[2] and some research by the EF[3], the concept of proposer commitments has begun to be more commonly discussed spurring the question; could proposers make commitments that would unlock a significant design space for Ethereum? And, could this be a mechanism to allow validators “…to provide input into block production, even when they decide to delegate building.”[4] In the last year, multiple proposals have come forward that rely on or could greatly benefit from proposer commitments, some examples include:</p>\n<ul>\n<li>Inclusion lists: Proposer commitment where part of the block / a set of transactions will be included / can’t be censored or removed by a third party, including the proposer</li>\n<li>Preconf: Proposer commitment to in advance, guarantee inclusion of data / certain transaction or group of transactions in a block</li>\n<li>Partial block auctions: Proposer commitment to auction off the top-of-block and the rest-of-block</li>\n<li>Blockspace / blob futures: Proposer commitment to sell part of their block now, but deliver that part of the block in the future</li>\n</ul>\n<p>The proposals range in complexity but are underpinned by the same simple idea–a proposer’s commitment to do something with or for a third party. We also note that proposers may not need to make commitments at this level of granularity (i.e., continue to use wholesale block auctions). However, we believe this is an avenue worth exploring as it may help preserve things like chain neutrality “by allowing them to provide input into block production, even when they decide to delegate building”[4] and if they choose, give some autonomy back to the proposer.</p>\n<p><strong>Challenge:</strong></p>\n<p>On the surface, this all seems great and is an incredibly exciting development. But, in the undercurrents, we are potentially on a perilous path if we can’t agree on a standard of how proposers register and make / receive commitments. We see multiple risks including, but not limited to:</p>\n<ul>\n<li>Increased fragmentation: While diversity of standards can create unlock more innovation, multiple standards (particularly in the last mile of communication) could compromise the security integrity of the entire Ethereum network through fragmentation of how proposer commitment protocols speak to proposers (i.e., proposers may need to make client adjustments for each variation of proposer commitments)</li>\n<li>Development complexity: If there is no standard, teams may more commonly make client adjustments to opt into proposer commitments. This could exponentially inflate the burden on core developers tasked with executing / testing major network upgrades increasing risks for the network around hard forks</li>\n<li>Limited transparency: With multiple software and standards, transparency around what proposers are opting into as well as bugs and taking quick actions may be challenging when something does go wrong</li>\n</ul>\n<p>These risks are likely to only increase as more and more proposer commitments get proposed and adopted. We also note that longer-term there are potential ideas to enshrine various mechanisms helping to reduce these risks.</p>\n<p><strong>Proposal:</strong></p>\n<p>We propose an out-of-protocol, open-source public good that is backward compatible to help standardize the communication mechanism for the last mile of proposers’ commitments. The goal is to develop, adopt, and then sustain one standard software that will limit fragmentation and reduce complexity for core devs. We currently call this Commitment Boost.</p>\n<p><strong>Design Principles:</strong></p>\n<p>Below are a few design principles when initially envisioning Commitment Boost.</p>\n<ul>\n<li>Open source / open development: This should be developed in the open and under open source licensing such as MIT / Apache-2.0</li>\n<li>Safety and reducing risks: Should be backwards compatible not changing existing pipes that support current proposer commitments and be built to isolate each proposer commitment. We also note that Commitment Boost should be continuously managed for future forks / upgrades in the Ethereum ecosystem</li>\n<li>Same overhead as today: The software will not have more overhead to run than existing proposer commitments (and ideally it is even more efficient!). Note: there may be additional overhead for facilitating the actual proposer commitments, but it is not the focus of this discussion and will be up to the proposer commitment protocol itself around how to potentially outsource any complexity</li>\n<li>Transparency: There will need to be robust functionality to understand which commitments a proposer is opting into as well as performing rigorous testing to quickly identify bugs. There should also be data APIs to increase information and transparency for the community and to strengthen alerting systems when bugs happen</li>\n</ul>\n<p><strong>Initial High-Level Potential Design of Commitment Boost:</strong></p>\n<ul>\n<li>Proposer wishing to register for commitments runs Commitment Boost. This will be backward compatible with consensus clients using the same messaging mechanisms that exist today. Commitment Boost is just focused on standardizing the last mile communication between a proposer choosing to register and then send / receive messages to / from a proposer commitment protocol</li>\n<li>Once running Commitment Boost, the proposer will need to register for each commitment they wish to make</li>\n<li>Each proposer commitment across the Commitment Boost stack is likely to be modular and isolated. The rationale that if there is a bug in one module this will not impact the rest of the proposer commitments / block construction. We also note that safeguards should be put in place to protect consensus (i.e., fallback mechanisms to local block building etc.)</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/4/3/43f7a19652088da9f139f178060df90293cebbf0.png\" data-download-href=\"https://ethresear.ch/uploads/default/43f7a19652088da9f139f178060df90293cebbf0\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/4/3/43f7a19652088da9f139f178060df90293cebbf0_2_531x309.png\" alt=\"\" data-base62-sha1=\"9HgxlLbVleMaHdQv1FfJFNrhzvq\" width=\"531\" height=\"309\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/4/3/43f7a19652088da9f139f178060df90293cebbf0_2_531x309.png, https://ethresear.ch/uploads/default/optimized/3X/4/3/43f7a19652088da9f139f178060df90293cebbf0_2_796x463.png 1.5x, https://ethresear.ch/uploads/default/original/3X/4/3/43f7a19652088da9f139f178060df90293cebbf0.png 2x\" data-dominant-color=\"B3BAB2\"></a></div><br>\n<strong>Near-Term Focus:</strong><p></p>\n<p>As noted above, Commitment Boost should lean towards being modular to potentially allow any proposer commitment. However, we initially plan to focus on designing standards for Commitment Boost to be backward compatible and to support commitments such as preconf protocols and inclusion lists. If other projects require proposer commitments and are interested in thinking through designs please reach out.</p>\n<p><strong>Open Questions:</strong></p>\n<p>Below is a list of questions that we need to consider and engage around. We note that some of these are not specific to Commitment Boost, but proposer commitments more broadly.</p>\n<ul>\n<li>Added risks: While there are general risks and questions on enabling proposer commitment protocols, we are particularly interested to engage on additional risks / the initial high-level design of Commitment Boost and whether the design can reduce risks to any commitment potentially impacting consensus</li>\n<li>Standardization is not always good: While in most cases standardization helps align the market and reduces the risk of fragmentation, it can also limit innovation and create “tech debt” that can confine the design space given how early we are and what we know about proposer commitments today</li>\n<li>How modules are added: It is not clear the best path forward around a process, or lack of, for how new proposer commitment modules are added</li>\n<li>Coordination during Ethereum upgrades: Likely will need to coordinate a group to ensure there is a process to perform testing / changes related to forks</li>\n<li>Upgrades to commitment boost: Similar to the point directly above, if we identify some upgrades needed we will need to test and manage any code changes required</li>\n<li>Economics: Are there any considerations around how proposers are paid for committing and how this could impact Ethereum / could these be internalized to Ethereum in the future</li>\n<li>Centralization: What are the impacts Commitment Boost could have on centralization and does it impact at-home stakers or geographical dispersion of validators</li>\n</ul>\n<p><strong>Conclusion:</strong></p>\n<p>The garden of Ethereum is infinite and the potential for proposer commitments could set off a wave of ways to use Ethereum. Proposer commitments could enable things like preconfs that are critical for based sequencing as well as other applications not currently envisioned. To help the community and entrepreneurs build on Ethereum, Commitment Boost is an initial idea to standardize the way proposers register and send / receive commitments. We look forward to discussing, refining, developing, and working with the community on proposer commitments. We plan to keep gathering feedback on the back of this post and continue to work with the community to push this idea as well as assist in other efforts around proposer commitments.</p>\n<p><strong>References:</strong></p>\n<p>[1] Fun Fact: The initials for Commitment Boost are shared with <a href=\"https://en.wikipedia.org/wiki/Citizens_band_radio\" rel=\"noopener nofollow ugc\">CB Radios</a>, “a system allowing short-distance one-to-many bidirectional voice communication among individuals.”</p>\n<p>[2] With the excitement around <a href=\"https://ethresear.ch/t/based-rollups-superpowers-from-l1-sequencing/15016\">based sequencing</a> introduced by Justin Drake, there has been a push <a href=\"https://www.youtube.com/watch?v=XSsKFINj710\" rel=\"noopener nofollow ugc\">by a few teams</a> to build proposer commitments such as preconfs. Developments like EigenLayer and restaking generally have also expanded developers / teams imagination of what proposers can make commitments around.</p>\n<p>[3] Generally, we are referring to PEPC and inclusion lists research as noted in the “Related Work” section of this post.</p>\n<p>[4]<a href=\"https://ethresear.ch/t/uncrowdable-inclusion-lists-the-tension-between-chain-neutrality-preconfirmations-and-proposer-commitments/19372\" class=\"inline-onebox\">Uncrowdable Inclusion Lists: The Tension between Chain Neutrality, Preconfirmations and Proposer Commitments</a></p>\n            <p><small>2 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/based-proposer-commitments-ethereum-s-marketplace-for-proposer-commitments/19517\">Read full topic</a></p>","link":"https://ethresear.ch/t/based-proposer-commitments-ethereum-s-marketplace-for-proposer-commitments/19517","pubDate":"Thu, 09 May 2024 19:58:17 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19517"},"source":{"@url":"https://ethresear.ch/t/based-proposer-commitments-ethereum-s-marketplace-for-proposer-commitments/19517.rss","#text":"Based proposer commitments - Ethereum’s marketplace for proposer commitments"},"filter":false},{"title":"[Research report] Allowing validators to share client information privately---a project by Nethermind Research","dc:creator":"jorem321","category":"Consensus","description":"<p>Dear ethresear.ch readers,</p>\n<p>As part of the Ethereum Foundation’s <a href=\"https://esp.ethereum.foundation/data-collection-grants\" rel=\"noopener nofollow ugc\">Data Collection Grants Round 2023 </a> which ran between last September and October, an interdisciplinary team involving Nethermind Research and Nethermind core developers received a grant to work on the project “Allowing validators to provide client information privately”. Below, we attach our submission in fulfillment of the objectives behind the project. In this deliverable, we have provided the necessary motivation and background for the problem of measuring client diversity, which we then use to propose and analyze three different approaches for validators to privately share their client diversity data—each with their strengths and weaknesses.</p>\n<p><a href=\"https://nethermind.notion.site/Allowing-validators-to-provide-client-information-privately-bfea6436bfe246d28afdcda125d9049c\" rel=\"noopener nofollow ugc\">Deliverable: Allowing validators to provide client information privately </a></p>\n<h2><a name=\"executive-summary-of-the-proposed-approaches-1\" class=\"anchor\" href=\"https://ethresear.ch#executive-summary-of-the-proposed-approaches-1\"></a>Executive summary of the proposed approaches.</h2>\n<p>We briefly summarize the key ideas behind the three approaches before. The reader is referred to the deliverable for a full exposition.</p>\n<h3><a name=\"h-1-client-diversity-data-on-the-graffiti-field-2\" class=\"anchor\" href=\"https://ethresear.ch#h-1-client-diversity-data-on-the-graffiti-field-2\"></a>1. Client diversity data on the graffiti field</h3>\n<p>As the first approach, we have discussed a method for measuring validator client diversity by posting data directly on the graffiti field. We note that this approach has been discussed by the community before. We have outlined necessary changes, such as creating an EngineAPI method for CL clients to retrieve EL client details and agreeing on encoding standards for the data. We have also discussed challenges with this method including dealing with parties that do not participate, multiplexed architectures, and distinguishing between proposer and attester duties.</p>\n<p>We have also discussed statistical significance, i.e., how many client data reports are needed to accurately estimate the client distribution from graffiti field data alone. We confirmed that the analyzed method can reach statistical significance quickly (in the order of days) assuming a reasonable participation rate. We discuss these assertions quantitatively in the deliverable.</p>\n<p>Finally, we have assessed the feasibility of anonymizing graffiti field reports, concluding that existing methods like encryption or zero-knowledge proofs are impractical to use due to the sequential nature of data collection and the limited space in the graffiti field.</p>\n<h3><a name=\"h-2-allowing-nodes-to-listen-to-client-diversity-data-through-the-gossip-network-using-nullifiers-to-hide-the-identity-of-validators-3\" class=\"anchor\" href=\"https://ethresear.ch#h-2-allowing-nodes-to-listen-to-client-diversity-data-through-the-gossip-network-using-nullifiers-to-hide-the-identity-of-validators-3\"></a>2. Allowing nodes to listen to client diversity data through the gossip network + using nullifiers to hide the identity of validators</h3>\n<p>As the second approach, we have examined a potential modification to Ethereum’s P2P layer to enable crawlers to obtain validator distribution for client diversity. We have explored using a dedicated channel in the GossipSub protocol to share client diversity data efficiently. We have proposed a method that periodically selects validators at random to submit their client diversity data, which is then shared through GossipSub. Each validator forms its client diversity data into a <strong>ClientData</strong> object and publishes it via a designated topic. Then, the nodes in this designated topic can receive those objects, verify their authenticity, and aggregate them for the final result. We have also discussed the challenges around this method, particularly concerning network overload.</p>\n<p>Furthermore, we have explored anonymizing P2P reports to ensure validators’ privacy. We have discussed potential approaches such as encrypting client data or anonymizing the voters’ identities using nullifiers and zero-knowledge proofs. We have proposed an approach that uses BLS signatures, nullifiers, and zero-knowledge proofs to hide validators’ identities and prevent double submissions. Validators submit encoded client data along with proofs to a P2P network. We have discussed potential deanonymization vectors such as P2P traffic analysis and proposed mitigation strategies like mixnets and approaches based on Dandelion and Dandelion++.</p>\n<p>Implementing these strategies may face challenges such as increased latency and complexity. We have stressed our interest in community input regarding the concern level over potential attack vectors and the feasibility of mitigation strategies.</p>\n<h3><a name=\"h-3-dedicated-voting-scheme-for-client-data-collection-4\" class=\"anchor\" href=\"https://ethresear.ch#h-3-dedicated-voting-scheme-for-client-data-collection-4\"></a>3. Dedicated voting scheme for client data collection</h3>\n<p>As the third and last approach, we have proposed a voting protocol aimed at collecting data from validators securely and verifiably, avoiding issues like obscurity and centralization found in existing survey methods. We have examined the use of public bulletin boards (PBBs) or blockchains for collecting votes, drawing insights from Vitalik’s analysis of blockchains’ limitations in elections and the advantages of using blockchains as bulletin boards. Due to its decentralization and cost-efficiency, we have proposed to utilize a blockchain, specifically Ethereum’s Holesky Testnet. Regarding how validators submit their votes, we have considered having validators encrypt their client data and share it through a P2P network, and using a trusted committee—called decryption authorities—to receive the encrypted data, submit the received data to a smart contract, and finally, aggregate and decrypt the encrypted client data.</p>\n<p>This third method addresses some of the traffic analysis concerns in the second method by leveraging homomorphic encryption of the votes, which requires a trusted committee.</p>\n<h1><a name=\"a-call-for-feedback-5\" class=\"anchor\" href=\"https://ethresear.ch#a-call-for-feedback-5\"></a>A call for feedback</h1>\n<p>As the next stage of this research project, we look forward to disseminating and discussing the aforementioned approaches through various channels, including this forum and community calls. Thus, we welcome discussions with the Ethereum community to gauge the impressions on the most suitable approach. For example,</p>\n<ul>\n<li>In the deliverable above, we have provided a rubric that ranks the downsides of each method according to their severity as perceived by the team. From the team’s perspective, this analysis positions the second method as the most favorable. Should this rubric be challenged in any way?</li>\n<li>Does the reader see any additional concerns with the proposed methods?</li>\n<li>Are there any variations or suggestions the reader can think of to build upon the methods herein?</li>\n</ul>\n<p>We look forward to your impressions and comments!</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/research-report-allowing-validators-to-share-client-information-privately-a-project-by-nethermind-research/19506\">Read full topic</a></p>","link":"https://ethresear.ch/t/research-report-allowing-validators-to-share-client-information-privately-a-project-by-nethermind-research/19506","pubDate":"Thu, 09 May 2024 00:58:51 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19506"},"source":{"@url":"https://ethresear.ch/t/research-report-allowing-validators-to-share-client-information-privately-a-project-by-nethermind-research/19506.rss","#text":"[Research report] Allowing validators to share client information privately---a project by Nethermind Research"},"filter":false},{"title":"Triadic Consensus: A Fast and Resilient Consensus Mechanism for Sharded Blockchains","dc:creator":"cryptskii","category":"Consensus","description":"<p><strong>TL;DR</strong> We introduce a novel triadic consensus mechanism that achieves fast and resilient agreement on transaction ordering and state updates within shards, with <span class=\"math\">O(n \\log_3 n)</span> message complexity and <span class=\"math\">O(\\log_3 n)</span> time complexity.</p>\n<p><strong>Background</strong> Traditional consensus protocols like PBFT suffer from high communication overhead, requiring <span class=\"math\">O(n^2)</span> message complexity for <span class=\"math\">n</span> validators. This limits scalability and performance in sharded blockchain architectures. There is a need for a more efficient and resilient consensus mechanism that can scale to large networks while maintaining security guarantees.</p>\n<p><strong>Proposal</strong> Triadic consensus organizes validator nodes within each shard into a fractal data structure based on the Sierpinski triangle. Nodes are grouped into recursive “triads” of three nodes each. Consensus is reached by propagating votes across these triads in a recursive manner. Each node independently validates transactions and broadcasts votes to its triad peers. Votes are aggregated at each triad level, with the majority outcome determining the triad vote. If a triad commits a transaction, the vote is propagated to the parent triad at the level above. This process continues recursively until either a triad rejects the transaction or the root triad is reached, indicating global consensus.</p>\n<p>Let <span class=\"math\">v_{i,j} \\in \\{0,1\\}</span> denote the vote of node <span class=\"math\">j</span> in triad <span class=\"math\">\\tau_i</span>, and let <span class=\"math\">V_{\\tau_i} \\in \\{0,1\\}</span> denote the aggregated triad-level vote for <span class=\"math\">\\tau_i</span>, defined as:</p>\n<p><span class=\"math\">V_{\\tau_i} = \\begin{cases}\n1 &amp; \\text{if } \\sum_{j=1}^3 v_{i,j} \\geq 2 \\\\\n0 &amp; \\text{otherwise}\n\\end{cases}</span></p>\n<p>The overall shard-level consensus outcome <span class=\"math\">V_{\\text{shard}}</span> for transaction <span class=\"math\">tx</span> is determined as:</p>\n<p><span class=\"math\">V_{\\text{shard}}(tx) = \\prod_{i=0}^r V_{\\tau_i}</span></p>\n<p>where <span class=\"math\">r = \\log_3 n</span> is the height of the fractal topology for a shard with <span class=\"math\">n</span> nodes.</p>\n<p><strong>Advantages</strong> Triadic consensus offers several advantages over traditional consensus protocols:</p>\n<ol>\n<li>\n<p>Reduced message complexity: With <span class=\"math\">O(n \\log_3 n)</span> message complexity, triadic consensus significantly reduces communication overhead compared to <span class=\"math\">O(n^2)</span> in PBFT.</p>\n</li>\n<li>\n<p>Fast convergence: The recursive vote propagation allows the shard to reach a <span class=\"math\">2/3</span> supermajority consensus on each transaction with minimal rounds of communication, achieving <span class=\"math\">O(\\log_3 n)</span> time complexity.</p>\n</li>\n<li>\n<p>Byzantine fault tolerance: Triadic consensus ensures agreement on all valid transactions with up to <span class=\"math\">f &lt; n/3</span> Byzantine nodes, maintaining the same security threshold as PBFT.</p>\n</li>\n</ol>\n<p>Triadic consensus ensures agreement on all valid transactions with up to <span class=\"math\">f &lt; n/3</span> Byzantine nodes.</p>\n<p>Consider a triad <span class=\"math\">\\tau_i</span> with an honest supermajority (<span class=\"math\">\\geq 2</span> out of <span class=\"math\">3</span> nodes). If <span class=\"math\">tx</span> is valid, then at least <span class=\"math\">2</span> nodes will broadcast <span class=\"math\">v_{i,j} = 1</span>, and thus <span class=\"math\">V_{\\tau_i} = 1</span>. Conversely, if <span class=\"math\">tx</span> is invalid, then at least <span class=\"math\">2</span> nodes will broadcast <span class=\"math\">v_{i,j} = 0</span>, and thus <span class=\"math\">V_{\\tau_i} = 0</span>.</p>\n<p>Since at most <span class=\"math\">f</span> nodes are Byzantine, and each triad requires <span class=\"math\">\\geq 2</span> matching votes to commit, a Byzantine node can only stall consensus if paired with <span class=\"math\">\\geq 1</span> other Byzantine node in a triad. With <span class=\"math\">f &lt; n/3</span>, Byzantine nodes can comprise at most <span class=\"math\">1</span> node in each triad. Thus every honest supermajority triad will reach agreement, and every mixed triad (<span class=\"math\">1</span> Byzantine node) will either agree with the honest nodes or deadlock. Any deadlocked triads will be eventually resolved by honest majority triads in subsequent rounds.</p>\n<p><strong>Applications</strong> Triadic consensus can be applied in sharded blockchain architectures to enable fast and secure consensus within each shard. It is particularly well-suited for high-throughput applications that require low latency and scalability, such as decentralized finance (DeFi), supply chain management, and Internet of Things (IoT) networks. By organizing validators into a fractal topology and propagating votes efficiently, triadic consensus allows shards to process transactions in parallel while maintaining global consistency.</p>\n<p><strong>Conclusion</strong> The triadic consensus mechanism introduced represents a significant advancement in consensus protocols for sharded blockchains. By achieving <span class=\"math\">O(n \\log_3 n)</span> message complexity and <span class=\"math\">O(\\log_3 n)</span> time complexity, it enables fast and resilient agreement on transaction ordering and state updates within shards. This scalable and efficient consensus mechanism has the potential to unlock new possibilities for high-performance decentralized applications and pave the way for more robust and scalable blockchain networks.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/triadic-consensus-a-fast-and-resilient-consensus-mechanism-for-sharded-blockchains/19504\">Read full topic</a></p>","link":"https://ethresear.ch/t/triadic-consensus-a-fast-and-resilient-consensus-mechanism-for-sharded-blockchains/19504","pubDate":"Wed, 08 May 2024 21:05:05 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19504"},"source":{"@url":"https://ethresear.ch/t/triadic-consensus-a-fast-and-resilient-consensus-mechanism-for-sharded-blockchains/19504.rss","#text":"Triadic Consensus: A Fast and Resilient Consensus Mechanism for Sharded Blockchains"},"filter":false},{"title":"Client-Side Ordinal Transaction Ordering (COTO)","dc:creator":"cryptskii","category":"Sharding","description":"<p><strong>TL;DR</strong> ProofChain introduces a novel transaction ordering mechanism called Client-Side Ordinal Transaction Ordering (COTO) that achieves deterministic and uniform ordering of transactions across shards without requiring shard synchronization, enhancing the scalability of the system.</p>\n<p><strong>Background</strong> Existing blockchain systems face scalability challenges due to limitations in their transaction ordering approaches. Global consensus ordering suffers from limited throughput, while shard-level consensus ordering requires cross-shard synchronization. DAG-based approaches face challenges in transaction finality and handling conflicting transactions.</p>\n<p><strong>Proposal</strong> COTO assigns a unique ordinal rank to each transaction based on the sender’s shard ID, logical clock value, timestamp, and hash of the serialized transaction. The ordinal rank determines the order in which transactions are processed. Each shard validates and processes transactions independently using the assigned ordinal ranks. Processed transactions are propagated to other shards through either Client View (signed transactions) or Global View (Global State Proofs) propagation modes.</p>\n<p>Let <span class=\"math\">\\mathcal{T}</span> denote the set of all transactions in the ProofChain network. Each transaction <span class=\"math\">tx \\in \\mathcal{T}</span> is represented as a tuple:</p>\n<p><span class=\"math\">tx = (s, r, a, n, t)</span></p>\n<p>where:<br>\n<span class=\"math\">s</span> is the sender’s address<br>\n<span class=\"math\">r</span> is the recipient’s address<br>\n<span class=\"math\">a</span> is the transaction amount<br>\n<span class=\"math\">n</span> is the transaction nonce<br>\n<span class=\"math\">t</span> is the transaction timestamp</p>\n<p>The ordinal rank of a transaction <span class=\"math\">tx</span> is calculated as follows:</p>\n<p><span class=\"math\">OrdinalRank(tx) = (Shard(tx), LogicalClock(tx), Timestamp(tx), Hash(Serialize(tx)))</span></p>\n<p>where:<br>\n<span class=\"math\">Shard(tx) = hash(tx.s) \\bmod m</span>, with <span class=\"math\">hash</span> being a cryptographic hash function and <span class=\"math\">m</span> the total number of shards<br>\n<span class=\"math\">LogicalClock(tx)</span> is the current logical clock value of the client<br>\n<span class=\"math\">Timestamp(tx)</span> is the current timestamp at the time of transaction submission<br>\n<span class=\"math\">Hash(Serialize(tx))</span> is the cryptographic hash of the serialized transaction</p>\n<p><strong>Illustration</strong> Consider two transactions, <span class=\"math\">tx_1</span> and <span class=\"math\">tx_2</span>, submitted to the ProofChain network. The ordinal ranks of these transactions are calculated as follows:</p>\n<p><span class=\"math\">OrdinalRank(tx_1) = (Shard(tx_1), LogicalClock(tx_1), Timestamp(tx_1), Hash(Serialize(tx_1)))</span><br>\n<span class=\"math\">OrdinalRank(tx_2) = (Shard(tx_2), LogicalClock(tx_2), Timestamp(tx_2), Hash(Serialize(tx_2)))</span></p>\n<p>Assuming <span class=\"math\">tx_1</span> and <span class=\"math\">tx_2</span> belong to different shards and have unique ordinal ranks, they can be processed independently by their respective shards without requiring cross-shard synchronization.</p>\n<p><strong>Advantages</strong> COTO offers several advantages over alternative transaction ordering approaches:</p>\n<ol>\n<li>Scalability: COTO allows parallel transaction processing across shards, eliminating the need for global consensus or cross-shard synchronization.</li>\n<li>Deterministic Ordering: COTO ensures a deterministic ordering of transactions based on their unique ordinal ranks, providing consistency across shards.</li>\n<li>Efficient Propagation: COTO supports efficient propagation of shard states through succinct Global State Proofs (GSPs) or signed transactions.</li>\n</ol>\n<p><strong>Applications</strong> COTO can be applied in various scenarios where scalability and deterministic ordering of transactions are crucial:</p>\n<ol>\n<li>High-Throughput Payment Systems: COTO enables fast and parallel processing of transactions, making it suitable for large-scale payment networks.</li>\n<li>Decentralized Exchanges: COTO ensures a consistent ordering of trades across shards, facilitating efficient and fair execution of orders.</li>\n<li>Supply Chain Management: COTO can be used to track and order events in supply chain networks, ensuring data integrity and consistency across participants.</li>\n</ol>\n<p><strong>Conclusion</strong> Client-Side Ordinal Transaction Ordering (COTO) introduces a scalable and deterministic approach to transaction ordering in sharded blockchain networks. By assigning unique ordinal ranks to transactions and enabling parallel processing across shards, COTO addresses the scalability challenges faced by existing transaction ordering approaches. The mathematical formalisms, algorithms, and proofs presented demonstrate the correctness and scalability properties of COTO, making it a promising solution for various applications requiring high throughput and consistent transaction ordering.</p>\n            <p><small>6 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/client-side-ordinal-transaction-ordering-coto/19503\">Read full topic</a></p>","link":"https://ethresear.ch/t/client-side-ordinal-transaction-ordering-coto/19503","pubDate":"Wed, 08 May 2024 19:16:11 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19503"},"source":{"@url":"https://ethresear.ch/t/client-side-ordinal-transaction-ordering-coto/19503.rss","#text":"Client-Side Ordinal Transaction Ordering (COTO)"},"filter":false},{"title":"Optimized Decentralized Coded Computing with Binary Field SNARKs","dc:creator":"cryptskii","category":"zk-s[nt]arks","description":"<p><strong>TL;DR</strong> Building off of existing theory and practical, we propose a novel protocol for efficient and robust decentralized coded computing using binary field SNARK constructions and coding-theoretic techniques. It leverages a tower of binary field extensions to natively capture various data types and utilizes block-level polynomial commitments and PLONKish arithmetization for efficient verification of the computations.</p>\n<p><strong>Background</strong> Coded computing has emerged as a promising approach for injecting redundancy into decentralized computations for robustness against faults and stragglers. However, existing solutions using zero-knowledge proofs (ZKPs) and fully homomorphic encryption (FHE) face challenges in terms of efficiency, flexibility and scalability.</p>\n<p><strong>Proposal</strong> Our protocol synergistically combines state-of-the-art techniques from binary field SNARKs and coding theory:</p>\n<ul>\n<li>Use a tower of binary field extensions <span class=\"math\">\\mathbb{F}_2 \\subseteq \\mathbb{F}_{2^2} \\subseteq \\mathbb{F}_{2^4} \\subseteq \\mathbb{F}_{2^8} \\subseteq \\cdots \\subseteq \\mathbb{F}_{2^{128}}</span> to efficiently work with various data types</li>\n<li>Apply a block-level polynomial commitment scheme to commit coded boolean data with optimal rate and polylogarithmic proof size</li>\n<li>Adapt PLONKish techniques like product and permutation arguments over the binary fields to support expressive computations</li>\n<li>Introduce a shifting virtual polynomial for efficient rotations of coded data chunks</li>\n<li>Reconcile the different components via an interactive proof system with tower field arithmetic</li>\n</ul>\n<p><strong>Illustration</strong> Consider a multilinear polynomial <span class=\"math\">f \\in \\mathbb{F}_2[X_1, \\ldots, X_d]</span> of degree <span class=\"math\">\\leq d</span>. We encode the coefficients block-wise into a vector <span class=\"math\">\\vec{f} \\in \\mathbb{F}_{2^{\\lceil \\log{d} \\rceil}}^{2^d/d}</span> as follows:</p>\n<ol>\n<li>Partition the coefficients into <span class=\"math\">2^d/d</span> blocks <span class=\"math\">\\{\\vec{c}_i \\in \\mathbb{F}_2^d\\}_{i=1}^{2^d/d}</span></li>\n<li>For each block <span class=\"math\">i</span>, evaluate <span class=\"math\">g_i(X) = \\sum_{j=0}^{d-1} c_{i,j} X^j</span> at a fixed element <span class=\"math\">\\alpha_i \\in \\mathbb{F}_{2^{\\lceil \\log{d} \\rceil}}</span></li>\n<li>Define <span class=\"math\">\\vec{f} = (g_1(\\alpha_1), g_2(\\alpha_2), \\ldots, g_{2^d/d}(\\alpha_{2^d/d}))</span></li>\n</ol>\n<p>To avoid embedding overhead when committing, we directly work with the block-encoded vector <span class=\"math\">\\vec{f}</span> which has length <span class=\"math\">O(2^d/d)</span> over the extension field instead of the full coefficient vector over <span class=\"math\">\\mathbb{F}_2</span> of length <span class=\"math\">2^d</span>.</p>\n<p><strong>Advantages</strong> Our protocol achieves several advantages over prior works:</p>\n<ul>\n<li>Efficient prover times of <span class=\"math\">\\widetilde{O}(mN^2)</span> field operations and verifier times of <span class=\"math\">\\widetilde{O}(N^2/\\rho)</span> field operations for <span class=\"math\">N</span> constraints and rate-<span class=\"math\">\\rho</span> encoding, where <span class=\"math\">m</span> is the number of variables per constraint</li>\n<li>Proof sizes of <span class=\"math\">O(N^2 \\log k / \\rho)</span> bits that compares favorably to FRI-based systems like STARKs and RedShift</li>\n<li>Flexibility to work with multiple binary extension fields and compute natively over <span class=\"math\">\\mathbb{F}_2</span>, enabling more compact constraint systems in some cases compared to R1CS</li>\n<li>Short structured reference strings that can be generated transparently without extra setup assumptions</li>\n</ul>\n<p><strong>Applications</strong> This protocol can be applied to efficiently verify computations expressed as boolean or arithmetic circuits in a decentralized setting with redundancy against faults. Potential use cases include:</p>\n<ul>\n<li>Privacy-preserving outsourcing of computations to a network of untrusted workers</li>\n<li>Scalable and robust multi-party computation (MPC) with low online communication</li>\n<li>Transparent and succinct proof systems for general computations with purely algebraic security assumptions</li>\n</ul>\n<p><strong>Conclusion</strong> We presented a high-performance cryptographic protocol for decentralized coded computing by combining binary field SNARKs and coding theory in a novel way. Our construction overcomes challenges in prior works and achieves asymptotic and concrete efficiency for a wide class of computations. This work expands the capabilities of zero-knowledge proof systems and enables exciting applications in privacy-enhancing technologies.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/optimized-decentralized-coded-computing-with-binary-field-snarks/19499\">Read full topic</a></p>","link":"https://ethresear.ch/t/optimized-decentralized-coded-computing-with-binary-field-snarks/19499","pubDate":"Wed, 08 May 2024 16:54:10 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19499"},"source":{"@url":"https://ethresear.ch/t/optimized-decentralized-coded-computing-with-binary-field-snarks/19499.rss","#text":"Optimized Decentralized Coded Computing with Binary Field SNARKs"},"filter":false},{"title":"Empirical Analysis of the Impact of EIP-4844 on Ethereum's Ecosystem","dc:creator":"wanify","category":"Economics","description":"<h2><a name=\"empirical-analysis-of-the-eip-4844s-impact-on-ethereum-1\" class=\"anchor\" href=\"https://ethresear.ch#empirical-analysis-of-the-eip-4844s-impact-on-ethereum-1\"></a>Empirical Analysis of the EIP-4844’s Impact on Ethereum</h2>\n<p><em>by <a href=\"https://twitter.com/seongwan_eth\" rel=\"noopener nofollow ugc\">Seongwan Park</a>, <a href=\"https://twitter.com/1004YUKICHAN\" rel=\"noopener nofollow ugc\">Bosul Mun</a></em></p>\n<p>Draft paper : <a href=\"https://arxiv.org/abs/2405.03183\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[2405.03183] Impact of EIP-4844 on Ethereum: Consensus Security, Ethereum Usage, Rollup Transaction Dynamics, and Blob Gas Fee Markets</a></p>\n<h3><a name=\"summary-2\" class=\"anchor\" href=\"https://ethresear.ch#summary-2\"></a>Summary</h3>\n<p>On March 13, 2024, Ethereum implemented EIP-4844 to enhance its capabilities as a data availability layer. While this upgrade has successfully reduced <a href=\"https://l2fees.info/\" rel=\"noopener nofollow ugc\">data posting costs</a> for rollups, it also introduces potential concerns regarding the consensus layer, particularly due to increased propagation sizes. Additionally, the broader impacts on the overall Ethereum ecosystem have not been thoroughly explored until now.</p>\n<p>In our analysis, we’ve examined EIP-4844’s effects in terms of consensus security, Ethereum usage, rollup transaction dynamics, and the blob gas fee mechanism. Our study includes changes in synchronization times, detailed assessments of Ethereum usage, rollup activities, and insights into the blob gas fee mechanism. Our goal is to pinpoint both improvements and potential issues since the upgrade.</p>\n<p><strong>Main Findings:</strong></p>\n<ol>\n<li><strong>Consensus Security</strong></li>\n</ol>\n<ul>\n<li>Increase in fork rates, even excluding periods affected by client issues.</li>\n<li>Sync time has increased by approximately 140ms, from 2267.436ms to 2407.05ms.</li>\n<li>The most significant contributor to increased sync time is receive time, while blob propagation had minimal effects.</li>\n</ul>\n<ol start=\"2\">\n<li><strong>Ethereum Usage</strong></li>\n</ol>\n<ul>\n<li>Marked increase in total data size posted by rollups (+116%).</li>\n<li>Significant reduction in total fees paid by rollups (-71%) and price per MiB for data availability (-82%).</li>\n<li>A substantial decrease in total gas used (-54%).</li>\n</ul>\n<ol start=\"3\">\n<li><strong>Rollup Transactions</strong></li>\n</ol>\n<ul>\n<li>All six rollups studied (Arbitrum One, Optimism, Base, Starknet, zkSync Era, Linea) showed significant increases in transaction volume.</li>\n<li>User delay has notably increased in four of the rollups(except for Arbitrum One and zkSync Era), highlighting the need for blob sharing protocols.</li>\n</ul>\n<ol start=\"4\">\n<li><strong>Blob Gas Fee Market</strong></li>\n</ol>\n<ul>\n<li>Small influence of the gas base fee on the blob gas base fee, with no reciprocal influence detected.</li>\n<li>Higher priority fees for blob transactions compared to non-blob transactions</li>\n<li>The blob gas fee market exhibits greater volatility than the gas fee market, yet it potentially reflects market demands more accurately.</li>\n</ul>\n<p><strong>Figures</strong></p>\n<div class=\"d-image-grid\">\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/5/8/589586f7a2dd9e55913caf25215b3279929be1d3.png\" data-download-href=\"https://ethresear.ch/uploads/default/589586f7a2dd9e55913caf25215b3279929be1d3\" title=\"Screenshot 2024-05-07 at 5.51.53 PM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/5/8/589586f7a2dd9e55913caf25215b3279929be1d3_2_345x204.png\" alt=\"Screenshot 2024-05-07 at 5.51.53 PM\" data-base62-sha1=\"cDEnqXPFLP1od79gwTQcEQkuAuL\" width=\"345\" height=\"204\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/5/8/589586f7a2dd9e55913caf25215b3279929be1d3_2_345x204.png, https://ethresear.ch/uploads/default/optimized/3X/5/8/589586f7a2dd9e55913caf25215b3279929be1d3_2_517x306.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/5/8/589586f7a2dd9e55913caf25215b3279929be1d3_2_690x408.png 2x\" data-dominant-color=\"DFD6CC\"></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/6/f/6fb1d284d34adcfa3b1b2f37dd60a1d882111787.png\" data-download-href=\"https://ethresear.ch/uploads/default/6fb1d284d34adcfa3b1b2f37dd60a1d882111787\" title=\"Screenshot 2024-05-07 at 5.52.11 PM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/6/f/6fb1d284d34adcfa3b1b2f37dd60a1d882111787_2_345x226.png\" alt=\"Screenshot 2024-05-07 at 5.52.11 PM\" data-base62-sha1=\"fW5YS8ixjxAqzsCgAqvYcpdLAnJ\" width=\"345\" height=\"226\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/6/f/6fb1d284d34adcfa3b1b2f37dd60a1d882111787_2_345x226.png, https://ethresear.ch/uploads/default/optimized/3X/6/f/6fb1d284d34adcfa3b1b2f37dd60a1d882111787_2_517x339.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/6/f/6fb1d284d34adcfa3b1b2f37dd60a1d882111787_2_690x452.png 2x\" data-dominant-color=\"F5F0EB\"></a></div><p></p>\n</div>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/8/4/84bf36904da0af478a6813aee89ae47bd80b272d.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/84bf36904da0af478a6813aee89ae47bd80b272d\" title=\"Screenshot 2024-05-07 at 9.03.14 PM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/8/4/84bf36904da0af478a6813aee89ae47bd80b272d_2_393x374.jpeg\" alt=\"Screenshot 2024-05-07 at 9.03.14 PM\" data-base62-sha1=\"iWkHiuZj8oN1CqWmv4DQ4kdcss5\" width=\"393\" height=\"374\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/8/4/84bf36904da0af478a6813aee89ae47bd80b272d_2_393x374.jpeg, https://ethresear.ch/uploads/default/optimized/3X/8/4/84bf36904da0af478a6813aee89ae47bd80b272d_2_589x561.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/3X/8/4/84bf36904da0af478a6813aee89ae47bd80b272d_2_786x748.jpeg 2x\" data-dominant-color=\"F6F6F6\"></a></div><p></p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/b/0/b0a06aed03cdbbe0c2df234e6a85e586326bcfb0.png\" data-download-href=\"https://ethresear.ch/uploads/default/b0a06aed03cdbbe0c2df234e6a85e586326bcfb0\" title=\"스크린샷 2024-05-13 오후 7.47.01\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/b/0/b0a06aed03cdbbe0c2df234e6a85e586326bcfb0_2_517x110.png\" alt=\"스크린샷 2024-05-13 오후 7.47.01\" data-base62-sha1=\"pcvJ8My13G3kP8yLeS0UhRhAN9K\" width=\"517\" height=\"110\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/b/0/b0a06aed03cdbbe0c2df234e6a85e586326bcfb0_2_517x110.png, https://ethresear.ch/uploads/default/optimized/3X/b/0/b0a06aed03cdbbe0c2df234e6a85e586326bcfb0_2_775x165.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/b/0/b0a06aed03cdbbe0c2df234e6a85e586326bcfb0_2_1034x220.png 2x\" data-dominant-color=\"E8E8E8\"></a></div><p></p>\n<p>We will also open our code&amp;dataset soon.<br>\nWe hope to offer a deeper understanding on the post-upgrade effects and encourage discussions that can help improve the Ethereum ecosystem. Your feedback and insights would be greatly appreciated.</p>\n            <p><small>4 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/empirical-analysis-of-the-impact-of-eip-4844-on-ethereums-ecosystem/19486\">Read full topic</a></p>","link":"https://ethresear.ch/t/empirical-analysis-of-the-impact-of-eip-4844-on-ethereums-ecosystem/19486","pubDate":"Tue, 07 May 2024 12:36:11 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19486"},"source":{"@url":"https://ethresear.ch/t/empirical-analysis-of-the-impact-of-eip-4844-on-ethereums-ecosystem/19486.rss","#text":"Empirical Analysis of the Impact of EIP-4844 on Ethereum's Ecosystem"},"filter":false},{"title":"Tiered Commitment Trees to reduce gas costs and offchain complexity","dc:creator":"rymnc","category":"Applications","description":"<p>Discussion post for: <a href=\"https://dev.vac.dev/rlog/rln-light-verifiers/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Verifying RLN Proofs in Light Clients with Subtrees | Vac Research</a><br>\ncross-posted to: <a href=\"https://forum.vac.dev/t/light-rln-verifiers-using-a-tiered-commitment-tree/290\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Light RLN Verifiers using a Tiered Commitment Tree - Vac Rearch Blog Posts - Vac</a></p>\n<p>tl;dr: Implementation of a technique to decrease gas fees associated with a sparse Merkle tree on-chain, while simultaneously minimizing client-side requirements. This solution leverages the segmentation of root computation into subtrees. Notably utilized by projects like Penumbra and Polygon Miden, this approach also facilitates trustless availability of the Merkle tree root on-chain, even when using a zk-friendly hash function that is more costly within the EVM environment.</p>\n<p>Can be used for Semaphore/RLN.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/tiered-commitment-trees-to-reduce-gas-costs-and-offchain-complexity/19484\">Read full topic</a></p>","link":"https://ethresear.ch/t/tiered-commitment-trees-to-reduce-gas-costs-and-offchain-complexity/19484","pubDate":"Tue, 07 May 2024 10:22:56 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19484"},"source":{"@url":"https://ethresear.ch/t/tiered-commitment-trees-to-reduce-gas-costs-and-offchain-complexity/19484.rss","#text":"Tiered Commitment Trees to reduce gas costs and offchain complexity"},"filter":false},{"title":"Sharded Recursive zk-SNARK Proofs","dc:creator":"cryptskii","category":"zk-s[nt]arks","description":"<p><strong>TL;DR</strong> In a sharded blockchain protocol that utilizes recursive zk-SNARK proofs to enable scalable, private cross-shard transactions with constant-size proofs of validity. This architecture allows for horizontal scaling while maintaining strong privacy guarantees.</p>\n<p><strong>Background</strong> Existing blockchain systems face significant challenges in terms of scalability and privacy. Sharding is a promising approach to improve transaction throughput by parallelizing computation across multiple chains. However, cross-shard communication remains a bottleneck, as verifying transactions across shards typically requires expensive cross-shard proofs.</p>\n<p>Zero-knowledge proofs, particularly zk-SNARKs, offer a powerful tool for enhancing privacy by allowing users to prove knowledge of secret information without revealing it. Unfortunately, generating and verifying zk-SNARK proofs incurs high computational overhead, limiting their practicality for large-scale applications.</p>\n<p>Prior solutions have attempted to combine zk-SNARKs with sharding, but fail to fully address the scalability challenges. For example, Zexe uses zk-SNARKs in a sharded setting but requires storing a linear-size “state proof” on-chain. Coda achieves constant-size proofs using recursive composition, but lacks the horizontal scaling benefits of sharding.</p>\n<p><strong>Proposal</strong> We introduce a novel construction that synergistically combines sharding with recursive zk-SNARK proofs for unparalleled scalability and privacy.</p>\n<p>At the core of this DLT is a hierarchy of zk-SNARK proofs that recursively attest to the validity of state transitions within and across shards. Each shard generates succinct proofs, called Zero-Knowledge Balance &amp; Inclusion State Proofs (ZkBISPs), certifying the correctness of their local state updates. These ZkBISPs are then aggregated by a designated coordinator into a global proof, termed a Zero-Knowledge Succinct Nested Global-state Proof (ZkSNGP).</p>\n<p>Crucially, the ZkSNGP is a constant-size proof that recursively verifies the validity of all shard-level ZkBISPs, thereby providing a succinct and efficient means to prove the integrity of the entire cross-shard state transition. Verifying the ZkSNGP requires only logarithmic time in the number of shards, enabling exponential savings compared to naively checking each shard’s proofs individually.</p>\n<p>Formally, we define the intra-shard state transition language <span class=\"math\">\\mathcal{L}_{\\mathsf{ST}}^{(t,i)}</span> for each shard <span class=\"math\">i</span> at epoch <span class=\"math\">t</span> as the set of tuples <span class=\"math\">(x, w)</span> where:</p>\n<ul>\n<li>The statement <span class=\"math\">x = (\\mathsf{shardID}_i, \\mathsf{root}_i^{(t-1)}, \\mathsf{root}_i^{(t)}, B_i^{(t)})</span> includes the shard ID, starting and ending state roots, and final account balances.</li>\n<li>The witness <span class=\"math\">w = (\\mathsf{txs}_i^{(t)}, \\mathcal{T}_i^{(t-1)}, \\mathcal{T}_i^{(t)})</span> contains the list of transactions, along with the initial and final account state trees.</li>\n<li><span class=\"math\">(x, w) \\in \\mathcal{L}_{\\mathsf{ST}}^{(t,i)} \\Leftrightarrow \\mathsf{root}_i^{(t-1)} = H(\\mathcal{T}_i^{(t-1)}) \\wedge \\mathsf{root}_i^{(t)} = H(\\mathcal{T}_i^{(t)}) \\wedge \\text{transition}(\\mathcal{T}_i^{(t-1)}, \\mathsf{txs}_i^{(t)}) \\rightarrow \\mathcal{T}_i^{(t)}</span>, i.e, the roots match the account trees and the final tree results from applying valid transactions to the initial tree.</li>\n</ul>\n<p>Similarly, we define the cross-shard state transition language <span class=\"math\">\\mathcal{L}_{\\mathsf{CST}}^{(t)}</span> for epoch <span class=\"math\">t</span> as the set of tuples <span class=\"math\">(x, w)</span> where:</p>\n<ul>\n<li>The statement <span class=\"math\">x = (\\mathsf{root}_G^{(t-1)}, \\mathsf{root}_G^{(t)})</span> consists of the starting and ending global state roots.</li>\n<li>The witness <span class=\"math\">w = \\left(\\left\\{\\left(\\mathsf{shardID}_i, \\pi_{\\mathsf{ST},i}^{(t)}, \\mathsf{root}_i^{(t-1)},\\mathsf{root}_i^{(t)}, B_i^{(t)}\\right)\\right\\}_{i=1}^{\\ell},\\mathcal{T}_G^{(t-1)},\\mathcal{T}_G^{(t)}\\right)</span> includes the shard IDs, ZkBISPs, local roots and balances, and global account trees.</li>\n<li><span class=\"math\">(x, w) \\in \\mathcal{L}_{\\mathsf{CST}}^{(t)} \\Leftrightarrow \\forall i: \\mathsf{Verify}_{\\mathsf{ST}}(\\mathsf{vk}_{\\mathsf{ST}}, x_i, \\pi_{\\mathsf{ST},i}^{(t)}) \\wedge \\mathsf{root}_G^{(t-1)} = H(\\mathcal{T}_G^{(t-1)}) \\wedge \\mathsf{root}_G^{(t)} = H(\\mathcal{T}_G^{(t)}) \\wedge \\text{merge}(\\mathcal{T}_G^{(t-1)}, \\{\\mathsf{root}_i^{(t)}, B_i^{(t)}\\}_{i=1}^{\\ell}) \\rightarrow \\mathcal{T}_G^{(t)}</span>, i.e., the ZkBISPs verify w.r.t. their shards, the Merkle roots match, and the final global tree is the result of correctly merging the shards’ final local trees and balances.</li>\n</ul>\n<p>A shard’s ZkBISP for epoch <span class=\"math\">t</span> is generated as <span class=\"math\">\\pi_{\\mathsf{ST},i}^{(t)} \\leftarrow \\mathsf{Prove}_{\\mathsf{ST}}(\\mathsf{pk}_{\\mathsf{ST}}, x_i, w_i)</span> for <span class=\"math\">(x_i, w_i) \\in \\mathcal{L}_{\\mathsf{ST}}^{(t,i)}</span>, where <span class=\"math\">\\mathsf{pk}_{\\mathsf{ST}}</span> is the proving key for the corresponding zk-SNARK scheme. The coordinator’s ZkSNGP is computed analogously as <span class=\"math\">\\pi_{\\mathsf{CST}}^{(t)} \\leftarrow \\mathsf{Prove}_{\\mathsf{CST}}(\\mathsf{pk}_{\\mathsf{CST}}, x, w)</span> for <span class=\"math\">(x, w) \\in \\mathcal{L}_{\\mathsf{CST}}^{(t)}</span></p>\n<p>The coordinator, randomly selected in each epoch, collects these ZkBISPs along with the shards’ final state roots <span class=\"math\">\\mathsf{root}_i^{(t)}</span> and account balances <span class=\"math\">B_i^{(t)}</span>. It then generates the ZkSNGP <span class=\"math\">\\pi_{\\mathsf{CST}}^{(t)}</span> (green proof) certifying the validity of the overall state transition, including the correct application of all shard-level updates to the global state.</p>\n<p><strong>Advantages</strong> The network simultaneously achieves exceptional horizontal scalability and privacy without sacrificing security or decentralization.</p>\n<p>In terms of scalability, the concept supports an unprecedented number of shards and transactions per second while retaining a constant-size proof of the system’s entire state. Concretely, if there are <span class=\"math\">\\ell</span> shards each processing <span class=\"math\">N</span> transactions, the communication cost per epoch is only <span class=\"math\">O(\\ell)</span> for the coordinator to collect the ZkBISPs, and the ZkSNGP proof adds just <span class=\"math\">O(1)</span> to the blockchain size. Crucially, verifying the ZkSNGP requires <span class=\"math\">O(\\log \\ell)</span> time, an exponential speedup compared to naively verifying all <span class=\"math\">\\ell</span> shards.</p>\n<p>For example, suppose the network is instantiated with <span class=\"math\">\\ell = 2^{10}</span> shards, each processing <span class=\"math\">N = 2^{20}</span> transactions in 2-minute epochs. This configuration could support a peak throughput of roughly 1 billion transactions per epoch, or 500,000 transactions per second, with a ZkSNGP verification time of only <span class=\"math\">10\\log \\ell \\approx 100</span> ms on ordinary hardware. The recursive proof would contribute a mere 1 KB to the blockchain per epoch, maintaining years of history in a highly compact format.</p>\n<p>In terms of privacy, the ZkSNGPs inherit the zero-knowledge property of the underlying zk-SNARK scheme, revealing nothing about the shards’ local transactions beyond the final state roots and balances. An adversary that compromises the coordinator cannot glean any additional information, as the shards’ ZkBISPs are similarly zero-knowledge. Transactional privacy thus holds as long as at least one shard remains honest.</p>\n<p>Compared to prior sharded blockchain designs, the network is the first to achieve sublinear proof sizes and verification times by recursively composing zk-SNARKs. Relative to Zexe, the network attains a qualitative improvement in scalability by eliminating the linear-size “state proof” in favor of constant-size ZkSNGPs. Compared to Coda, the network offers strictly stronger performance due to its sharded architecture, while still leveraging Coda’s core technique of recursive proof composition.</p>\n<p><strong>Applications</strong> the network’s dual emphasis on scalability and privacy renders it a natural foundation for a variety of high-throughput, privacy-centric blockchain applications.</p>\n<p>On the payments front, the network could serve as a backend for a globally-scalable digital currency with strong confidentiality guarantees, concealing both transaction amounts and participants. The subtransactions within each shard could clear near-instantaneously, while cross-shard payments would incur a maximum delay of one epoch (e.g., 2 minutes) before the ZkSNGP confirms finality. This would support a substantially higher payment volume than existing solutions like Zcash without leaking metadata.</p>\n<p>More broadly, the network could function as a privacy-preserving platform for general smart contract execution. Shards would not only process token transfers but also arbitrary state transitions, with the ZkBISPs and ZkSNGP verifying the correctness of all contract logic and dependencies. This would enable complex applications such as private decentralized exchanges, automated market makers, and lending protocols to run at scale, without disclosing individual users’ balances or positions.</p>\n<p>The network’s sharded architecture could also be adapted to specific domains to meet their unique performance requirements. For instance, a decentralized adtech ecosystem that handles billions of micropayments per day could utilize more granular sharding (e.g., <span class=\"math\">\\ell = 2^{20}</span> shards), with each shard perhaps corresponding to a particular geographic region or publisher. A secure messaging app that routes payments alongside packets could likewise tune its cross-shard spanning tree structure based on network topology.</p>\n<p><strong>Conclusion</strong> the network introduces a powerful new paradigm for designing scalable and private blockchain protocols through recursive zk-SNARK proof composition. By strategically combining recursive proofs with sharding, the network enables a significant breakthrough in blockchain performance, supporting over a million transactions per second with sublinear proof sizes and verification times.</p>\n            <p><small>7 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/sharded-recursive-zk-snark-proofs/19480\">Read full topic</a></p>","link":"https://ethresear.ch/t/sharded-recursive-zk-snark-proofs/19480","pubDate":"Mon, 06 May 2024 19:04:30 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19480"},"source":{"@url":"https://ethresear.ch/t/sharded-recursive-zk-snark-proofs/19480.rss","#text":"Sharded Recursive zk-SNARK Proofs"},"filter":false},{"title":"Coded ZKP/FHE, a decentralized, collaborative, robust ZKP/FHE System","dc:creator":"0x1cc","category":"zk-s[nt]arks","description":"<h2><a name=\"tldr-1\" class=\"anchor\" href=\"https://ethresear.ch#tldr-1\"></a>TL;DR</h2>\n<ul>\n<li>\n<p>For a ZKP/FHE task, we can decompose it into several subtasks.</p>\n</li>\n<li>\n<p>We introduce redundancy into subtasks such that the original task’s result can be decoded from a subset of the subtask results, treating uncompleted subtasks as erasures. This is similar to the erasure code design in DA.</p>\n</li>\n<li>\n<p>For a <span class=\"math\">(n,k)</span> coded ZKP/FHE system, we can decompose a ZKP/FHE task into <span class=\"math\">n</span> subtasks, with <span class=\"math\">k \\leq n</span> subtask results, we can obtain the original task’s result.</p>\n</li>\n<li>\n<p>With this coded design, we can design a decentralized, collaborative, robust ZKP/FHE system.</p>\n</li>\n</ul>\n<h2><a name=\"background-2\" class=\"anchor\" href=\"https://ethresear.ch#background-2\"></a>Background</h2>\n<p>ZKP/FHE systems play a pivotal role in blockchain ecosystems, ensuring privacy and enabling cost-effective verification. However, the resource-intensive nature of ZKP generation and FHE computation presents a significant challenge. To address this, numerous distributed algorithms have been devised to enhance scalability and are now integral to ZKP/FHE mining pools.</p>\n<p>For instance, complex ZKP tasks can be subdivided into smaller subtasks, which are then distributed across multiple nodes for parallel processing. However, the efficacy of existing distributed algorithms falls short in ensuring robustness, hindering the realization of decentralized and collaborative systems.</p>\n<p>Consider a scenario where a ZKP task is divided into <span class=\"math\">k</span> subtasks and allocated to <span class=\"math\">k</span> distinct nodes. Should one of these nodes fail to respond promptly, the entire computation process is stalled. While redundancy mechanisms, such as assigning each subtask to two nodes, may mitigate this risk, vulnerabilities persist. Even with this redundancy, if both nodes assigned to a task fail to respond in a timely manner, computational delays ensue.</p>\n<p>In summary, while distributed algorithms offer scalability benefits, their current limitations impede the development of resilient decentralized and collaborative ZKP/FHE systems. Addressing these shortcomings is essential for advancing the efficacy and reliability of such systems within blockchain environments.</p>\n<h2><a name=\"proposal-3\" class=\"anchor\" href=\"https://ethresear.ch#proposal-3\"></a>Proposal</h2>\n<p>In this proposal, we introduce redundancy into subtasks to enhance the robustness of Zero-Knowledge Proof/Fully Homomorphic Encryption (ZKP/FHE) systems, akin to the erasure code design in Distributed Algorithms (DA). Specifically, in a <span class=\"math\">(n,k)</span> coded ZKP/FHE system, a ZKP/FHE task is decomposed into <span class=\"math\">n</span> subtasks, which are then distributed across <span class=\"math\">n</span> nodes. With a minimum of <span class=\"math\">k</span> completed subtask results, where <span class=\"math\">k \\leq n</span>, the original task’s result can be obtained.</p>\n<p>To illustrate this concept, let’s consider a toy model of matrix multiplication in zkML/fheML.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/2/b/2b2ff2f906f369f930c8722e9adf55b9d01aab43.png\" data-download-href=\"https://ethresear.ch/uploads/default/2b2ff2f906f369f930c8722e9adf55b9d01aab43\" title=\"Snipaste_2024-05-05_15-58-22\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/2/b/2b2ff2f906f369f930c8722e9adf55b9d01aab43_2_690x211.png\" alt=\"Snipaste_2024-05-05_15-58-22\" data-base62-sha1=\"6a3ge5hL9EV9Kgttvdh7Uep3tDB\" width=\"690\" height=\"211\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/2/b/2b2ff2f906f369f930c8722e9adf55b9d01aab43_2_690x211.png, https://ethresear.ch/uploads/default/original/3X/2/b/2b2ff2f906f369f930c8722e9adf55b9d01aab43.png 1.5x, https://ethresear.ch/uploads/default/original/3X/2/b/2b2ff2f906f369f930c8722e9adf55b9d01aab43.png 2x\" data-dominant-color=\"E2E1E1\"></a></div><p></p>\n<p>Consider a system comprising three worker nodes and one master node. In this setup, a data matrix <span class=\"math\">A</span> is divided into two submatrices, <span class=\"math\">A_1</span> and <span class=\"math\">A_2</span>. Specifically, node <span class=\"math\">W_1</span> stores <span class=\"math\">A_1</span>, node <span class=\"math\">W_2</span> stores <span class=\"math\">A_2</span>, and node <span class=\"math\">W_3</span> stores the sum <span class=\"math\">A_1 + A_2</span>. Upon receiving input <span class=\"math\">X</span>, each node computes the product of <span class=\"math\">X</span> with the respective stored matrix and transmits the result to the master node. Notably, the master node can reconstruct the product <span class=\"math\">AX</span> upon receiving any two products, thus obviating the need to await the slowest response. For instance, consider a scenario where the master node receives <span class=\"math\">A_1X</span> and <span class=\"math\">(A_1 + A_2)X</span>. Through subtracting <span class=\"math\">A_1X</span> from <span class=\"math\">(A_1 + A_2)X</span>, the master node can deduce <span class=\"math\">A_2X</span> and consequently reconstruct <span class=\"math\">AX</span>.</p>\n<p>We can further adopt an <span class=\"math\">(n,k)</span> MDS code in this matrix multiplication example for generalization. For example, in zkML or fheML, we can adopt the <span class=\"math\">(n,k)</span> coded approach to design a decentralized, collaborative, robust ZKP/FHE System. In zkML, we can decompose the task into <span class=\"math\">n</span> subtasks, with the results and zkp of <span class=\"math\">k</span> subtasks, we can aggravate the zkp of these subtasks with the decoded process. In fheML, we can decompose the task into <span class=\"math\">n</span> subtasks, with the results of <span class=\"math\">k</span> subtasks, we can apply the fhe computation on the decoded process.</p>\n<p>The preceding discussion has focused on a particular use case, namely zkML/fheML. The coded design methodology explored can be extrapolated to the foundational elements of the ZKP/FHE framework, facilitating the creation of a comprehensive coded ZKP/FHE system capable of supporting applications such as zkRollup and fheEVM computation.</p>\n<p>Specifically, this coded approach can be applied to various components of the ZKP system, enabling the development of a distributed coded system. For instance, the R1CS instance in ZKP involves numerous multi-scalar multiplications, which can seamlessly integrate with the coded design. With the distributed computation algorithms applied in current distributed ZKP systems, we can further enhance the efficiency and scalability.</p>\n<p>I may design and implement a PoC version of the coded ZKP system in my free time <img src=\"https://ethresear.ch/images/emoji/facebook_messenger/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<h2><a name=\"advantages-4\" class=\"anchor\" href=\"https://ethresear.ch#advantages-4\"></a>Advantages</h2>\n<ul>\n<li>\n<p>This coded design significantly accelerates computation within ZKP/FHE systems. Theoretically, assuming a node count of <span class=\"math\">n</span> and subtask runtimes with exponential tails, the coded approach could be <span class=\"math\">\\theta(\\log n)</span> times faster than conventional uncoded distributed algorithms.</p>\n</li>\n<li>\n<p>With this coded design, the ZKP/FHE system is more robust. For example, in a <span class=\"math\">(n,k)</span> coded ZKP/FHE system, we can tolerate the downtime or delay of <span class=\"math\">n-k</span> nodes.</p>\n</li>\n</ul>\n<h2><a name=\"applications-5\" class=\"anchor\" href=\"https://ethresear.ch#applications-5\"></a>Applications</h2>\n<p>Utilizing the coded design paradigm, a ZKP/FHE mining pool can be devised, where decentralized agents function as worker nodes engaged in the computation of subtasks. The managerial role within this context is assumed by the manager of the mining pool, serving as the master node responsible for aggregating and decoding the results submitted by the worker nodes. Furthermore, the conventional master node architecture can be supplanted by a smart contract, assuming the duties of result aggregation and decoding. This architectural transformation facilitates the establishment of a decentralized, collaborative, and resilient ZKP/FHE system, wherein cryptographic operations are conducted in a distributed manner, enhancing the system’s robustness and scalability.</p>\n<h2><a name=\"conclusion-6\" class=\"anchor\" href=\"https://ethresear.ch#conclusion-6\"></a>Conclusion</h2>\n<p>By breaking down ZKP/FHE tasks into subtasks and incorporating redundancy, a <span class=\"math\">(n,k)</span> coded ZKP/FHE system emerges, enabling the decomposition of tasks into <span class=\"math\">n</span> subtasks, with <span class=\"math\">k \\leq n</span> subtask results required for task reconstruction. This coded approach facilitates the creation of decentralized, collaborative, and resilient ZKP/FHE systems, promising enhanced efficiency and reliability in cryptographic operations.</p>\n            <p><small>12 posts - 7 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/coded-zkp-fhe-a-decentralized-collaborative-robust-zkp-fhe-system/19459\">Read full topic</a></p>","link":"https://ethresear.ch/t/coded-zkp-fhe-a-decentralized-collaborative-robust-zkp-fhe-system/19459","pubDate":"Sun, 05 May 2024 08:01:38 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19459"},"source":{"@url":"https://ethresear.ch/t/coded-zkp-fhe-a-decentralized-collaborative-robust-zkp-fhe-system/19459.rss","#text":"Coded ZKP/FHE, a decentralized, collaborative, robust ZKP/FHE System"},"filter":false},{"title":"Unbounded Scaling of a fully Decentralized Network without Global Coordination - - May the 4th Be With You","dc:creator":"cryptskii","category":"Consensus","description":"<h1><a name=\"neurodlt-a-neurologically-inspired-distributed-ledger-architecture-1\" class=\"anchor\" href=\"https://ethresear.ch#neurodlt-a-neurologically-inspired-distributed-ledger-architecture-1\"></a>NeuroDLT: A Neurologically-Inspired Distributed Ledger Architecture</h1>\n<p>Brandon “Cryptskii” Ramsay<br>\nMay 4, 2024</p>\n<h2><a name=\"abstract-2\" class=\"anchor\" href=\"https://ethresear.ch#abstract-2\"></a>Abstract</h2>\n<p>We propose a novel distributed ledger architecture drawing inspiration from the information processing principles of biological neural networks. The aim is to develop a highly scalable, efficient, and robust decentralized system for secure data storage and computation. The fundamental processing units are modeled after neurons, maintaining internal state vectors and communicating via synaptic connections. Information encoding and retrieval leverages content-addressable distributed associative memory. Consensus and consistency are achieved through self-organized synchronization of pulse-coupled oscillators. The architecture supports rich computational capabilities including inference, constraint satisfaction, and optimization. The neuromorphic design offers significant advantages in terms of scalability, adaptivity, robustness, and efficiency compared to traditional blockchain approaches.</p>\n<h2><a name=\"h-1-introduction-3\" class=\"anchor\" href=\"https://ethresear.ch#h-1-introduction-3\"></a>1 Introduction</h2>\n<p>Distributed ledger technologies have revolutionized decentralized systems, enabling secure, transparent, and immutable recording of transactions without relying on centralized authorities. However, existing blockchain-based architectures face significant challenges in terms of scalability, efficiency, and adaptability, limiting their widespread adoption and practical utility.</p>\n<p>Biological neural networks, on the other hand, exhibit remarkable capabilities in processing vast amounts of complex, unstructured information with exceptional scalability, efficiency, and robustness. The human brain, consisting of approximately 86 billion neurons connected by 150 trillion synapses, can perform highly sophisticated cognitive tasks while consuming only around 20 watts of power [8].</p>\n<p>Inspired by the computational principles of biological neural networks, we propose a novel distributed ledger architecture that leverages key neurological mechanisms to address the limitations of traditional blockchain systems. By bridging the gap between biological and artificial information processing, this neurologically-inspired architecture aims to create a highly scalable, adaptive, and computationally powerful decentralized system for secure data storage and computation.</p>\n<h2><a name=\"h-2-background-and-related-work-4\" class=\"anchor\" href=\"https://ethresear.ch#h-2-background-and-related-work-4\"></a>2 Background and Related Work</h2>\n<p>The field of distributed ledger technologies has seen significant advancements in recent years, with the emergence of various blockchain platforms such as Bitcoin [20], Ethereum [25], and Hyperledger Fabric [1]. These systems have demonstrated the potential of decentralized, trustless networks for secure and transparent recording of transactions and execution of smart contracts.</p>\n<p>However, existing blockchain architectures suffer from several limitations. The Proof-of-Work (PoW) consensus mechanism used in Bitcoin and Ethereum requires significant computational resources and energy consumption, leading to limited transaction throughput and scalability issues [24]. Alternative consensus mechanisms such as Proof-of-Stake (PoS) and Delegated Proof-of-Stake (DPoS) have been proposed to address these challenges, but they still face trade-offs between security, decentralization, and efficiency [26].</p>\n<p>Recent advancements in neuromorphic computing and brain-inspired architectures have shown promising results in achieving highly efficient and scalable information processing. Neuromorphic chips such as IBM’s TrueNorth [18] and Intel’s Loihi [7] have demonstrated the ability to perform complex cognitive tasks with ultra-low power consumption and high parallelism. These architectures leverage key principles of biological neural networks, such as event-driven computation, distributed memory, and adaptive learning, to achieve remarkable efficiency and robustness.</p>\n<p>Several studies have explored the intersection of neuromorphic computing and blockchain technologies. Baza et al. [2] proposed a blockchain-based framework for secure and efficient data sharing in Internet of Things (IoT) networks using neuromorphic hardware. Calzavara et al. [6] developed a privacy-preserving smart contract execution platform using trusted execution environments and neural networks. However, these approaches focus on integrating neuromorphic components into existing blockchain architectures rather than fundamentally redesigning the distributed ledger system based on neurological principles.</p>\n<p>Our work takes a different approach by proposing a novel distributed ledger architecture that is intrinsically inspired by the computational mechanisms of biological neural networks. By leveraging the inherent scalability, adaptivity, and efficiency of neurological information processing, we aim to create a fundamentally new paradigm for decentralized systems that can overcome the limitations of traditional blockchain approaches.</p>\n<h2><a name=\"h-3-neuron-like-node-architecture-5\" class=\"anchor\" href=\"https://ethresear.ch#h-3-neuron-like-node-architecture-5\"></a>3 Neuron-like Node Architecture</h2>\n<h3><a name=\"h-31-node-dynamics-6\" class=\"anchor\" href=\"https://ethresear.ch#h-31-node-dynamics-6\"></a>3.1 Node Dynamics</h3>\n<p>The fundamental processing units in the proposed architecture are modeled after the functional properties of biological neurons. Each node <span class=\"math\">i</span> maintains an internal state vector <span class=\"math\">s_i \\in \\mathbb{R}^n</span> analogous to the membrane potential of a neuron, encoding its current activation level. The temporal evolution of a node’s state is governed by the dynamical system:</p>\n<p><span class=\"math\">\\frac{ds_i}{dt} = f\\left( \\sum_{j \\in N_i} w_{ij} s_j(t) \\right)</span></p>\n<p>The temporal evolution of a node’s state <span class=\"math\">s_i</span> is governed by this differential equation, where <span class=\"math\">N_i</span> denotes the set of nodes with incoming synaptic connections to node <span class=\"math\">i</span>, <span class=\"math\">w_{ij}</span> is the synaptic weight of the connection from node <span class=\"math\">j</span> to node <span class=\"math\">i</span>, and <span class=\"math\">f(\\cdot)</span> is a non-linear activation function applied element-wise to the weighted sum of inputs. This equation captures the dynamics of how each node’s state changes over time based on the input it receives from its neighbors.</p>\n<p>where <span class=\"math\">N_i</span> denotes the set of nodes with incoming synaptic connections to node <span class=\"math\">i</span>, <span class=\"math\">w_{ij}</span> is the synaptic weight of the connection from node <span class=\"math\">j</span> to node <span class=\"math\">i</span>, and <span class=\"math\">f(\\cdot)</span> is a non-linear activation function applied element-wise to the weighted sum of inputs. Suitable choices for the activation function include the logistic sigmoid <span class=\"math\">f(x) = (1 + \\exp(-x))^{-1}</span> or the rectified linear unit (ReLU) <span class=\"math\">f(x) = \\max(0, x)</span> commonly employed in artificial neural networks.</p>\n<p>When a node’s internal state crosses a firing threshold <span class=\"math\">\\theta_i</span>, it generates an output spike that is transmitted to its downstream neighbors. This thresholding mechanism can be expressed as:</p>\n<p><span class=\"math\">\\textrm{output}_i(t) = \n\\begin{cases}\n1, &amp; \\text{if } s_i(t) \\geq \\theta_i \\\\\n0, &amp; \\text{otherwise}\n\\end{cases}</span></p>\n<p>The firing threshold <span class=\"math\">\\theta_i</span> is dynamically modulated as a function of the node’s recent firing history to implement adaptive homeostatic regulation of its excitability and maintain a target average firing rate.</p>\n<p><strong>Definition 1 (Node State Update).</strong> The internal state of a node <span class=\"math\">i</span> is updated according to the following discrete-time equation:</p>\n<p><span class=\"math\">s_i[t+1] = f\\left( \\sum_{j \\in N_i} w_{ij} s_j[t] \\right)</span></p>\n<p>where <span class=\"math\">t</span> denotes the discrete time step.</p>\n<p><strong>Example 1 (Sigmoid Activation).</strong> For a node with a logistic sigmoid activation function, the state update equation becomes:</p>\n<p><span class=\"math\">s_i[t+1] = \\frac{1}{1 + \\exp\\left(-\\sum_{j \\in N_i} w_{ij} s_j[t]\\right)}</span></p>\n<h3><a name=\"h-32-synaptic-plasticity-7\" class=\"anchor\" href=\"https://ethresear.ch#h-32-synaptic-plasticity-7\"></a>3.2 Synaptic Plasticity</h3>\n<p>Synaptic plasticity refers to the dynamic modification of synaptic strengths based on the correlated activity patterns of pre- and post-synaptic nodes. A well-established synaptic learning rule is spike-timing-dependent plasticity (STDP) [4], which updates the synaptic weight <span class=\"math\">w_{ij}</span> based on the relative timing of spikes between nodes <span class=\"math\">i</span> and <span class=\"math\">j</span>:</p>\n<p><span class=\"math\">\\frac{dw_{ij}}{dt} = \n\\begin{cases}\nA_+ \\exp\\left(-\\frac{|\\Delta t|}{\\tau_+}\\right), &amp; \\text{if } \\Delta t &gt; 0 \\\\\n-A_- \\exp\\left(-\\frac{|\\Delta t|}{\\tau_-}\\right), &amp; \\text{if } \\Delta t &lt; 0\n\\end{cases}</span></p>\n<p>The synaptic weight <span class=\"math\">w_{ij}</span> between nodes <span class=\"math\">i</span> and <span class=\"math\">j</span> is updated according to this spike-timing-dependent plasticity (STDP) learning rule. Here, <span class=\"math\">\\Delta t = t_j - t_i</span> represents the time difference between the spikes of nodes <span class=\"math\">i</span> and <span class=\"math\">j</span>. The parameters <span class=\"math\">\\tau_+</span> and <span class=\"math\">\\tau_-</span> set the time scales for potentiation and depression, respectively, while <span class=\"math\">A_+</span> and <span class=\"math\">A_-</span> control the maximum weight changes. This rule captures the idea that synaptic weights are strengthened when the pre-synaptic neuron fires before the post-synaptic neuron (causality), and weakened when the order is reversed (acausality).</p>\n<p>where <span class=\"math\">\\Delta t = t_j - t_i</span> is the time difference between spikes of nodes <span class=\"math\">i</span> and <span class=\"math\">j</span>, <span class=\"math\">\\tau_+</span> and <span class=\"math\">\\tau_-</span> set the time scales for potentiation and depression, and <span class=\"math\">A_+</span> and <span class=\"math\">A_-</span> control the maximum weight changes. This STDP rule implements a form of Hebbian learning, allowing the network to discover and amplify causal associations between neural activities. Over time, this leads to the self-organized emergence of meaningful connectivity patterns optimized for the particular data streams and computational tasks, without requiring centralized control or explicit programming.</p>\n<p><strong>Definition 2 (STDP Update).</strong> The synaptic weight <span class=\"math\">w_{ij}</span> between nodes <span class=\"math\">i</span> and <span class=\"math\">j</span> is updated according to the STDP rule:</p>\n<p><span class=\"math\">w_{ij} \\leftarrow w_{ij} + \\eta \\Delta w_{ij}</span></p>\n<p>where <span class=\"math\">\\eta</span> is the learning rate and <span class=\"math\">\\Delta w_{ij}</span> is the weight change computed using Equation (??).</p>\n<p><strong>Theorem 1 (Convergence of STDP).</strong> Under suitable conditions on the learning rate <span class=\"math\">\\eta</span> and the STDP time constants <span class=\"math\">\\tau_+</span> and <span class=\"math\">\\tau_-</span>, the synaptic weights converge to a stable equilibrium that maximizes the mutual information between the pre- and post-synaptic activities.</p>\n<p><em>Proof.</em> The proof follows from the analysis of the STDP learning rule as a stochastic gradient descent algorithm on the mutual information objective. See [3] for a detailed derivation.</p>\n<p>In addition to modifying synaptic weights, the network topology itself can adapt by periodically adding or pruning connections based on their long-term statistics and utility for the network’s overall information processing. This dynamic rewiring enables the system to flexibly reconfigure its architecture in response to changing demands and environments.</p>\n<p><strong>Algorithm 1 Adaptive Network Rewiring</strong></p>\n<pre><code class=\"lang-auto\">1: procedure RewireNetwork(G, λ, γ)\n2:   Initialize connectivity graph G = (V, E)\n3:   for each time step t do\n4:     for each node i ∈ V do\n5:       Sample candidate connections (i, j) with probability exp(c_ij/λ)\n6:       Prune connections (i, j) with probability exp(-|w_ij|/γ)\n7:     end for\n8:     Update connectivity graph G with new connections  \n9:   end for\n10: end procedure\n</code></pre>\n<p><span class=\"math\">\\lambda</span> and <span class=\"math\">\\gamma</span> are temperature parameters controlling the exploration-exploitation trade-off in adding and pruning connections, respectively. The algorithm balances the formation of new connections based on their temporal correlations <span class=\"math\">c_{ij}</span> and the removal of weak connections based on their absolute synaptic weights <span class=\"math\">|w_{ij}|</span>.</p>\n<h2><a name=\"h-4-distributed-associative-memory-8\" class=\"anchor\" href=\"https://ethresear.ch#h-4-distributed-associative-memory-8\"></a>4 Distributed Associative Memory</h2>\n<p>Information is encoded and stored in the network using a distributed content-addressable memory scheme. Each data item <span class=\"math\">x \\in \\mathbb{R}^d</span> is assigned a random binary key <span class=\"math\">k \\in \\{0,1\\}^m</span> that serves as a unique identifier. The data item is then encoded by activating the internal state vectors <span class=\"math\">s_i</span> of the nodes whose feature vectors <span class=\"math\">f_i \\in \\{0,1\\}^m</span> have the highest dot product similarity with the key:</p>\n<p><span class=\"math\">S_x = \\{i \\in V \\mid k \\cdot f_i \\text{ is among the top } K \\text{ values}\\}</span></p>\n<p>Here, <span class=\"math\">V</span> denotes the set of all nodes in the network, and <span class=\"math\">K</span> is a sparsity parameter controlling the number of nodes selected for encoding each data item. The feature vectors <span class=\"math\">f_i</span> are fixed random projections that map the high-dimensional keys to a lower-dimensional space, effectively implementing a form of locality-sensitive hashing (LSH) [13].</p>\n<p>To retrieve a data item given its key <span class=\"math\">k'</span>, the nodes with the highest feature similarity to the query key are activated, and the network dynamics are allowed to evolve according to Equation (??). The internal states of the activated nodes will converge to an attractor pattern representing the stored memory associated with the key. The retrieved data item can then be reconstructed from the stable node activations.</p>\n<p>This distributed associative memory scheme provides several advantages in terms of scalability, robustness, and efficiency:</p>\n<ul>\n<li>Scalability: The memory capacity of the network grows linearly with the number of nodes, allowing for massively parallel storage and retrieval of large-scale datasets.</li>\n<li>Robustness: The distributed encoding ensures graceful degradation in the presence of node failures or data corruptions. Partial or noisy keys can still retrieve the correct data item through the network’s pattern completion capabilities.</li>\n<li>Efficiency: The content-addressable nature of the memory enables fast and efficient retrieval of data items without requiring explicit search or indexing structures.</li>\n</ul>\n<p><strong>Definition 3 (Associative Memory Encoding).</strong> A data item <span class=\"math\">x \\in \\mathbb{R}^d</span> with key <span class=\"math\">k \\in \\{0,1\\}^m</span> is encoded in the network by activating the internal states of the nodes in the set <span class=\"math\">S_x</span> defined by:</p>\n<p><span class=\"math\">S_x = \\{i \\in V \\mid k \\cdot f_i \\geq \\theta\\}</span></p>\n<p>where <span class=\"math\">\\theta</span> is a similarity threshold.</p>\n<p><strong>Theorem 2 (Memory Capacity).</strong> For a network with <span class=\"math\">N</span> nodes and an average node degree <span class=\"math\">\\bar{d}</span>, the memory capacity <span class=\"math\">C</span> scales as:</p>\n<p><span class=\"math\">C = O\\left(\\frac{N}{\\bar{d}\\log N}\\right)</span></p>\n<p>assuming a sparse connectivity pattern and independent random feature vectors.</p>\n<p><em>Proof.</em> The proof follows from analyzing the associative memory as a random graph with a constraint on the average degree. The memory capacity is derived using information-theoretic arguments based on the entropy of the stored patterns. See [21] for a detailed analysis.</p>\n<p><strong>Algorithm 2 Associative Memory Retrieval</strong></p>\n<pre><code class=\"lang-auto\"> 1: procedure RetrieveData(k')\n 2:   Initialize query key k'\n 3:   Compute similarity scores k' · f_i for all nodes i ∈ V\n 4:   Select top K nodes with highest similarity scores as the activated set S_k'\n 5:   for each node i ∈ S_k' do\n 6:     Set initial state s_i[0] based on similarity score\n 7:   end for\n 8:   for t = 1 to T do\n 9:     for each node i ∈ S_k' do\n10:       Update state s_i[t] according to Equation (??)\n11:     end for\n12:   end for\n13:   Reconstruct retrieved data item xˆ from stable node states\n14:   return xˆ \n15: end procedure\n</code></pre>\n<p>The retrieval algorithm first computes the similarity scores between the query key <span class=\"math\">k'</span> and the feature vectors <span class=\"math\">f_i</span> of all nodes in the network. The top <span class=\"math\">K</span> nodes with the highest similarity scores are selected as the activated set <span class=\"math\">S_{k'}</span>. The initial states of these nodes are set based on their respective similarity scores, providing a starting point for the network dynamics.</p>\n<p>The algorithm then iteratively updates the states of the activated nodes for a fixed number of time steps <span class=\"math\">T</span> according to the node dynamics equation (??). During this process, the network settles into a stable attractor state that represents the retrieved memory associated with the query key.</p>\n<p>Finally, the retrieved data item <span class=\"math\">\\hat{x}</span> is reconstructed from the stable node states. This reconstruction can be performed using various methods, such as taking the average or weighted sum of the node states, depending on the specific encoding scheme used.</p>\n<p>The properties of the associative memory retrieval process can be formally analyzed using dynamical systems theory and attractor network models [10]. The convergence and stability of the retrieval dynamics depend on factors such as the network connectivity, the choice of activation function, and the noise level in the query key.</p>\n<p><strong>Theorem 3 (Retrieval Convergence).</strong> Under suitable conditions on the network connectivity and the activation function, the associative memory retrieval algorithm converges to a stable attractor state within a finite number of iterations.</p>\n<p><em>Proof.</em> The proof relies on analyzing the retrieval dynamics as a discrete-time dynamical system and showing that the energy function of the network decreases monotonically over time. The convergence to a stable attractor state follows from the existence of a Lyapunov function for the system. See [5] for a detailed analysis.</p>\n<p>The distributed associative memory scheme provides a powerful and efficient mechanism for storing and retrieving large-scale datasets in a decentralized manner. The content-addressable nature of the memory, combined with the robustness and scalability of the distributed encoding, enables fast and reliable access to stored information even in the presence of node failures or partial keyinformation.</p>\n<h2><a name=\"h-5-decentralized-consensus-protocol-9\" class=\"anchor\" href=\"https://ethresear.ch#h-5-decentralized-consensus-protocol-9\"></a>5 Decentralized Consensus Protocol</h2>\n<p>To achieve global consistency and coordination in a fully decentralized setting, the proposed architecture employs a consensus mechanism inspired by the synchronization of pulse-coupled oscillators in biological neural networks [19]. Each node maintains an internal clock variable <span class=\"math\">\\phi_i \\in [0, 2\\pi)</span> that evolves autonomously according to the phase response curve (PRC) model:</p>\n<p><span class=\"math\">\\frac{d\\phi_i}{dt} = \\omega_i + \\sum_{j \\in N_i} K(\\phi_j - \\phi_i)</span></p>\n<p>Here, <span class=\"math\">\\omega_i</span> is the natural frequency of the oscillator, <span class=\"math\">N_i</span> denotes the set of neighboring nodes connected to node <span class=\"math\">i</span>, and <span class=\"math\">K(\\cdot)</span> is the phase response function that determines how the phase of node <span class=\"math\">i</span> is adjusted based on the relative phases of its neighbors.</p>\n<p>When a node’s internal clock reaches a threshold value <span class=\"math\">\\phi^*_i \\in (0, 2\\pi)</span>, typically close to the end of the oscillation cycle, it emits a pulse that is transmitted to its neighbors. Upon receiving a pulse, each neighboring node <span class=\"math\">j</span> updates its own phase according to the PRC:</p>\n<p><span class=\"math\">\\phi_j(t^+) = \\phi_j(t^-) + K(\\phi_j(t^-))</span></p>\n<p>where <span class=\"math\">t^-</span> and <span class=\"math\">t^+</span> denote the times immediately before and after the pulse reception, respectively. The PRC is designed to promote synchronization among the oscillators by pulling the phases of the lagging nodes forward and slowing down the advancing nodes. A common choice for the PRC is the sinusoidal function <span class=\"math\">K(\\phi) = \\varepsilon \\sin(\\phi)</span>, where <span class=\"math\">\\varepsilon</span> is a coupling strength parameter.</p>\n<p>Under suitable conditions on the coupling strength and the natural frequencies, the pulse-coupled oscillator network will self-organize and achieve global synchronization, with all the nodes firing in unison [23]. This emergent consensus enables the decentralized coordination and agreement among the nodes without requiring explicit global communication or centralized control.</p>\n<p><strong>Definition 4 (Pulse-Coupled Oscillator Network).</strong> A pulse-coupled oscillator network is a dynamical system consisting of <span class=\"math\">N</span> oscillators, each characterized by a phase variable <span class=\"math\">\\phi_i \\in [0, 2\\pi)</span> and a natural frequency <span class=\"math\">\\omega_i</span>. The evolution of the phases is governed by the equations:</p>\n<p><span class=\"math\">\\frac{d\\phi_i}{dt} = \\omega_i + \\sum_{j \\in N_i} K(\\phi_j - \\phi_i)</span></p>\n<p><span class=\"math\">\\phi_i(t^+) = \\phi_i(t^-) + K(\\phi_i(t^-)) \\quad \\text{if } \\phi_j(t^-) = \\phi^*_j</span></p>\n<p>where <span class=\"math\">K(\\cdot)</span> is the phase response function and <span class=\"math\">\\phi^*_j</span> is the firing threshold.</p>\n<p><strong>Theorem 4 (Global Synchronization).</strong> For a pulse-coupled oscillator network with <span class=\"math\">N</span> nodes, if the coupling strength <span class=\"math\">\\varepsilon</span> satisfies <span class=\"math\">0 &lt; \\varepsilon &lt; \\frac{1}{N-1}</span> and the natural frequencies <span class=\"math\">\\omega_i</span> are sufficiently close to each other, then the network will achieve global synchronization asymptotically, i.e., <span class=\"math\">\\lim_{t \\to \\infty} |\\phi_i(t) - \\phi_j(t)| = 0</span> for all <span class=\"math\">i,j \\in \\{1, \\ldots, N\\}</span>.</p>\n<p><em>Proof.</em> The proof relies on analyzing the stability of the synchronized state using the linearized dynamics of the phase differences. The condition on the coupling strength ensures that the synchronized state is stable, while the assumption on the natural frequencies guarantees that the oscillators can be pulled into synchronization. The detailed proof can be found in [19].</p>\n<p>The pulse-coupled oscillator synchronization mechanism provides a robust and scalable foundation for achieving decentralized consensus in the proposed distributed ledger architecture. By associating special meanings to the firing events occurring at specific phases of the oscillation cycle, such as validating transactions or committing checkpoints, the network can coordinate and maintain consistent state in a fully decentralized manner.</p>\n<p><strong>Algorithm 3 Decentralized Consensus Protocol</strong></p>\n<pre><code class=\"lang-auto\">1: procedure RunConsensus\n2:   Initialize oscillator phases ϕ_i and frequencies ω_i for all nodes i ∈ V\n3:   while not synchronized do\n4:     for each node i ∈ V do\n5:       Update phase ϕ_i according to the oscillator dynamics\n6:       if ϕ_i reaches the firing threshold ϕ^*_i then\n7:         Emit pulse to neighbors j ∈ N_i\n8:       end if\n9:       if received pulse from neighbor j then\n10:        Update phase ϕ_i according to the PRC\n11:      end if\n12:    end for\n13:  end while\n14:  Perform consensus actions (e.g., validate transactions, commit checkpoints)\n15: end procedure\n</code></pre>\n<p>The decentralized consensus protocol proceeds in rounds, with each round corresponding to one oscillation cycle. At the beginning of each round, the nodes update their phases based on the oscillator dynamics and check if their phases have reached the firing threshold. If a node’s phase reaches the threshold, it emits a pulse to its neighbors.</p>\n<p>Upon receiving pulses from neighbors, nodes update their phases according to the phase response function. This process continues until the network achieves global synchronization, indicated by all the nodes firing in unison.</p>\n<p>Once synchronization is achieved, the nodes perform the consensus actions associated with the specific firing events, such as validating transactions or committing checkpoints. The synchronized firing ensures that all the nodes agree on the timing and order of these actions, maintaining consistency across the network.</p>\n<p>The pulse-coupled oscillator synchronization mechanism offers several advantages over traditional consensus algorithms, such as Byzantine fault tolerance [15] or Proof-of-Work [20]:</p>\n<ul>\n<li>Scalability: The consensus protocol scales efficiently with the size of the network, as the synchronization is achieved through local interactions among neighboring nodes, without requiring global communication or coordination.</li>\n<li>Robustness: The decentralized nature of the synchronization process makes it resilient to node failures and network disruptions. As long as a sufficient number of nodes remain connected, the network can self-organize and maintain consensus.</li>\n<li>Efficiency: The pulse-coupled oscillator model enables fast and efficient synchronization, as the nodes only need to exchange simple pulse signals rather than complex messages or computations.</li>\n</ul>\n<p>The proposed decentralized consensus protocol, based on the synchronization of pulse-coupled oscillators, provides a biologically-inspired and mathematically-grounded approach to achieving robust and scalable consensus in distributed ledger systems. The self-organized synchronization process allows the network to coordinate and maintain consistent state in a fully decentralized manner, enabling secure and efficient operation of the distributed ledger.</p>\n<h2><a name=\"h-6-decentralized-consensus-protocol-with-token-transactions-and-verifications-10\" class=\"anchor\" href=\"https://ethresear.ch#h-6-decentralized-consensus-protocol-with-token-transactions-and-verifications-10\"></a>6 Decentralized Consensus Protocol with Token Transactions and Verifications</h2>\n<p>The proposed neurologically-inspired distributed ledger architecture employs a decentralized consensus protocol based on the synchronization of pulse-coupled oscillators to achieve global consistency and coordination among nodes. This protocol can be extended to facilitate token transactions and verifications, ensuring the integrity and security of the ledger state.</p>\n<p>Each node <span class=\"math\">i</span> in the network maintains an internal clock variable <span class=\"math\">\\phi_i \\in [0, 2\\pi)</span> that evolves autonomously according to the phase response curve (PRC) model:</p>\n<p><span class=\"math\">\\frac{d\\phi_i}{dt} = \\omega_i + \\sum_{j \\in N_i} K(\\phi_j - \\phi_i)</span></p>\n<p>where <span class=\"math\">\\omega_i</span> is the natural frequency of the oscillator, <span class=\"math\">N_i</span> denotes the set of neighboring nodes connected to node <span class=\"math\">i</span>, and <span class=\"math\">K(\\cdot)</span> is the phase response function that determines the phase adjustment based on the relative phases of the neighbors.</p>\n<p>When a node’s internal clock reaches a threshold value <span class=\"math\">\\phi^*_i \\in (0, 2\\pi)</span>, typically close to the end of the oscillation cycle, it emits a pulse that is transmitted to its neighbors. Upon receiving a pulse, each neighboring node <span class=\"math\">j</span> updates its own phase according to the PRC:</p>\n<p><span class=\"math\">\\phi_j(t^+) = \\phi_j(t^-) + K(\\phi_j(t^-))</span></p>\n<p>where <span class=\"math\">t^-</span> and <span class=\"math\">t^+</span> denote the times immediately before and after the pulse reception, respectively.</p>\n<p>To incorporate token transactions and verifications into the consensus protocol, we associate specific firing events with different stages of the transaction processing pipeline. For instance, when a node’s internal clock reaches a designated phase <span class=\"math\">\\phi^{tx}_i \\in (0, \\phi^*_i)</span>, it initiates a token transaction by broadcasting a transaction proposal to its neighbors. The transaction proposal includes the relevant details, such as the sender, recipient, token amount, and a unique transaction identifier.</p>\n<p>Upon receiving a transaction proposal, each node verifies the validity of the transaction by checking the sender’s account balance, the authenticity of the digital signature, and the absence of double-spending attempts. If the transaction is deemed valid, the node incorporates it into its local transaction pool and emits a pulse to signal its approval.</p>\n<p>As the nodes continue to update their phases and emit pulses, the transaction proposals propagate through the network. When a node’s internal clock reaches the firing threshold <span class=\"math\">\\phi^*_i</span>, it collects all the approved transaction proposals received from its neighbors and bundles them into a candidate block. The node then broadcasts the candidate block to the network for further validation.</p>\n<p>During the validation phase, nodes verify the integrity and consistency of the candidate blocks by checking for conflicting transactions, double-spending attempts, and adherence to the consensus rules. If a candidate block receives a sufficient number of approval pulses from the network, it is considered validated and ready for commitment.</p>\n<p>Finally, when the nodes’ internal clocks synchronize and reach a designated commitment phase <span class=\"math\">\\phi^{commit}_i \\in (\\phi^*_i, 2\\pi)</span>, they collectively commit the validated blocks to their local ledgers, updating the token balances and transaction history accordingly. The synchronized firing event ensures that all nodes agree on the order and contents of the committed blocks, maintaining a consistent and tamper-proof ledger state.</p>\n<p>To prevent malicious behavior and ensure the security of the consensus protocol, nodes are incentivized to participate honestly through a combination of token rewards and penalties. Nodes that contribute to the successful validation and commitment of blocks are rewarded with newly minted tokens, while nodes that engage in malicious activities, such as proposing invalid transactions or propagating conflicting information, are penalized by having their token stakes slashed or their reputation scores reduced.</p>\n<p>The pulse-coupled oscillator synchronization mechanism provides a robust and scalable foundation for achieving decentralized consensus with token transactions and verifications. The self-organizing nature of the synchronization process allows the network to reach agreement on the order and validity of transactions without relying on a centralized authority or complex message passing protocols.</p>\n<p><strong>Algorithm 4 Decentralized Consensus Protocol with Token Transactions and Verifications</strong></p>\n<pre><code class=\"lang-auto\">1: procedure RunConsensusWithTransactions\n2:   Initialize oscillator phases ϕ_i and frequencies ω_i for all nodes i ∈ V\n3:   while not synchronized do\n4:     for each node i ∈ V do\n5:       Update phase ϕ_i according to the oscillator dynamics\n6:       if ϕ_i reaches the transaction proposal phase ϕ^{tx}_i then\n7:         Broadcast transaction proposal to neighbors j ∈ N_i\n8:       end if\n9:       if received transaction proposal from neighbor j then\n10:        Verify transaction validity\n11:        if transaction is valid then\n12:          Add transaction to local pool and emit approval pulse\n13:        end if\n14:      end if\n15:      if ϕ_i reaches the firing threshold ϕ^*_i then\n16:        Bundle approved transactions into a candidate block\n17:        Broadcast candidate block to neighbors j ∈ N_i\n18:      end if\n19:      if received candidate block from neighbor j then\n20:        Validate block and emit approval pulse if valid\n21:      end if\n22:      if ϕ_i reaches the commitment phase ϕ^{commit}_i then\n23:        Commit validated blocks to local ledger\n24:      end if\n25:    end for\n26:  end while\n27: end procedure\n</code></pre>\n<p>The extended consensus protocol incorporates token transactions and verifications by associating specific firing events with different stages of the transaction processing pipeline. Nodes propose transactions, validate and approve proposals, bundle them into candidate blocks, and collectively commit the blocks to their local ledgers based on the synchronized firing events.</p>\n<p>The integration of token transactions and verifications into the pulse-coupled oscillator consensus mechanism offers several advantages:</p>\n<ul>\n<li>Decentralized Coordination: The self-organizing synchronization process enables nodes to reach agreement on the order and validity of transactions without relying on a central authority or complex message passing protocols.</li>\n<li>Scalability: The consensus protocol scales efficiently with the size of the network, as transactions are proposed, validated, and committed through local interactions among neighboring nodes.</li>\n<li>Security: The combination of transaction verifications, block validations, and incentive mechanisms ensures the integrity and security of the token transactions, preventing double-spending, and mitigating malicious behavior.</li>\n<li>Consistency: The synchronized commitment of validated blocks ensures that all nodes maintain a consistent and tamper-proof ledger state, providing a reliable and auditable record of token transactions.</li>\n</ul>\n<p>The neurologically-inspired consensus protocol with token transactions and verifications offers a robust and efficient solution for achieving decentralized coordination and maintaining the integrity of the distributed ledger. By leveraging the self-organizing properties of pulse-coupled oscillators and integrating transaction processing into the synchronization dynamics, the proposed architecture enables secure, scalable, and consistent token transactions in a fully decentralized manner.</p>\n<h2><a name=\"h-7-computational-capabilities-11\" class=\"anchor\" href=\"https://ethresear.ch#h-7-computational-capabilities-11\"></a>7 Computational Capabilities</h2>\n<p>Beyond serving as a substrate for secure and decentralized data storage, the neurologically-inspired architecture possesses a wide range of computational capabilities that arise naturally from its densely interconnected structure and emergent dynamics. These capabilities make the architecture a versatile framework for distributed information processing and complex problem-solving.</p>\n<p>One key computational primitive is the ability to perform inference and belief revision in probabilistic graphical models. The network can be viewed as a Markov random field (MRF) [14], where each node represents a random variable and the synaptic connections encode the probabilistic dependencies among variables. Given a set of observed evidence, the inference task involves estimating the posterior probabilities of the hidden variables.</p>\n<p>In the proposed architecture, the posterior probabilities can be approximated by the stable fixed points of the network dynamics. The internal state <span class=\"math\">s_i</span> of each node <span class=\"math\">i</span> represents the marginal probability of the corresponding variable, and the synaptic weights <span class=\"math\">w_{ij}</span> encode the log-likelihood ratios of the pairwise dependencies:</p>\n<p><span class=\"math\">w_{ij} \\propto \\log \\frac{P(x_i=1 \\mid x_j=1)}{P(x_i=1 \\mid x_j=0)}</span></p>\n<p>The inference process in the proposed architecture can be formulated as an energy minimization problem, where the goal is to find the configuration of binary variables <span class=\"math\">x = (x_1, \\ldots, x_N)</span> that minimizes this energy function. The first term represents the unary potentials, with <span class=\"math\">\\theta_i</span> denoting the prior log-odds of variable <span class=\"math\">i</span> being active. The second term captures the pairwise interactions between variables, with <span class=\"math\">w_{ij}</span> representing the synaptic weight between nodes <span class=\"math\">i</span> and <span class=\"math\">j</span>. Minimizing this energy function corresponds to finding the most probable configuration of the variables given the observed evidence and the learned synaptic weights.</p>\n<p>The inference process can be implemented as a minimization of the following energy function:</p>\n<p><span class=\"math\">E(x) = -\\sum_i \\theta_i x_i - \\sum_{i \\neq j} w_{ij} x_i x_j</span></p>\n<p>where <span class=\"math\">x = (x_1, \\ldots, x_N)</span> is a configuration of the binary variables, <span class=\"math\">\\theta_i</span> represents the prior log-odds of variable <span class=\"math\">i</span>, and <span class=\"math\">w_{ij}</span> are the pairwise interaction weights.</p>\n<p>The network dynamics, governed by Equation (??), can be interpreted as performing a stochastic gradient descent on the energy function, with the stable fixed points corresponding to the local minima of the energy landscape. This allows the network to find the most probable configurations of the variables given the observed evidence, without explicitly computing the partition function or normalizing the probabilities.</p>\n<p>The sparse connectivity and event-driven nature of the network enable efficient and scalable inference, as the computations are performed locally and asynchronously based on the activity of the nodes. The recurrent connections and feedback loops in the network facilitate rapid information propagation and integration, allowing for fast convergence to the posterior estimates.</p>\n<p><strong>Theorem 5 (Inference Convergence).</strong> For a network with symmetric weights (<span class=\"math\">w_{ij} = w_{ji}</span>) and a concave activation function, the inference dynamics converge to a stable fixed point that corresponds to a local minimum of the energy function.</p>\n<p><em>Proof.</em> The proof relies on showing that the energy function decreases monotonically over time under the network dynamics. The symmetry of the weights and the concavity of the activation function ensure that the fixed points of the dynamics coincide with the local minima of the energy function. The detailed proof can be found in [11].</p>\n<p>Another important computational capability of the architecture is the ability to solve constraint satisfaction problems (CSPs) [22]. A CSP is defined by a set of variables <span class=\"math\">X = \\{x_1, \\ldots, x_N\\}</span>, eachassociated with a domain <span class=\"math\">D_i</span> of possible values, and a set of constraints <span class=\"math\">C = \\{C_1, \\ldots, C_M\\}</span> that specify the allowed combinations of variable assignments.</p>\n<p>To map a CSP onto the network, each variable <span class=\"math\">x_i</span> is represented by a group of nodes, with each node corresponding to a possible value assignment. The synaptic weights are set to encode the constraints, such that incompatible assignments result in high energy configurations. The network dynamics then search for the lowest energy state that satisfies all the constraints.</p>\n<p>The constrained optimization problem can be formulated as minimizing the following energy function:</p>\n<p><span class=\"math\">E(x) = \\sum_{i=1}^N \\sum_{k \\in D_i} x_{ik} + \\sum_{C \\in C} \\sum_{x_C \\notin S_C} \\prod_{i \\in V_C} x_{ix_C(i)}</span></p>\n<p>where <span class=\"math\">x_{ik} \\in \\{0,1\\}</span> indicates whether variable <span class=\"math\">i</span> is assigned value <span class=\"math\">k</span>, <span class=\"math\">S_C</span> denotes the set of allowed assignments for constraint <span class=\"math\">C</span>, <span class=\"math\">V_C</span> is the subset of variables involved in constraint <span class=\"math\">C</span>, and <span class=\"math\">x_C</span> represents the partial assignment of these variables.</p>\n<p>The network dynamics minimize the energy function by iteratively updating the node states based on the local constraints and the activities of the neighboring nodes. The sparse connectivity and asynchronous updates enable efficient exploration of the solution space, while the recurrent dynamics and feedback mechanisms facilitate rapid propagation of constraint violations and conflict resolution.</p>\n<p><strong>Theorem 6 (CSP Convergence).</strong> For a network encoding a satisfiable CSP with binary constraints, the network dynamics converge to a stable state representing a valid solution, provided that the weights are set appropriately and the activation function is chosen to be a step function.</p>\n<p><em>Proof.</em> The proof involves showing that the network energy decreases monotonically under the dynamics and that any valid solution corresponds to a global minimum of the energy function. The convergence to a valid solution follows from the fact that the energy cannot decrease indefinitely. The detailed proof can be found in [12].</p>\n<p>The computational capabilities of the neurologically-inspired architecture extend beyond inference and constraint satisfaction. The rich dynamics and adaptive properties of the network enable it to perform various tasks such as pattern recognition, associative memory, and optimization. The specific computations that can be carried out depend on the particular choice of node dynamics, synaptic plasticity rules, and network topology.</p>\n<p>For instance, by incorporating temporal integration and adaptive thresholds into the node dynamics, the network can be trained to recognize and generate complex spatiotemporal patterns [17]. This enables applications in sequence learning, time series prediction, and anomaly detection. The sparse encoding and distributed representation of patterns in the network provide robustness to noise and fault tolerance.</p>\n<p>Similarly, by defining appropriate objective functions and learning rules, the network can be optimized for tasks such as clustering, dimensionality reduction, and feature extraction [9]. The unsupervised learning capabilities of the network, based on local Hebbian plasticity and competitive dynamics, allow it to discover meaningful structures and representations in the input data without explicit supervision.</p>\n<p>The neuromorphic nature of the architecture offers several advantages over traditional approaches to distributed computing and information processing:</p>\n<ul>\n<li>Scalability: The sparse connectivity and local update rules enable the network to scale gracefully to large sizes without incurring prohibitive communication or computational overhead. The computations are performed in a highly parallel and distributed manner, leveraging the massive parallelism of neuromorphic substrates.</li>\n<li>Adaptability: The adaptive learning and self-organization capabilities of the network allow it to continuously tune its parameters and structure in response to changing environments or task demands. This adaptability enables the network to handle non-stationary data distributions and evolving problem domains.</li>\n<li>Efficiency: The event-driven and asynchronous nature of the computations, based on sparse spiking activity and adaptive update frequencies, can lead to significant energy savings compared to synchronous and clock-driven approaches. The localized processing and communication also reduce the energy costs associated with data movement and global synchronization.</li>\n<li>Robustness: The distributed encoding and storage of information, combined with the fault-tolerant dynamics and redundant connectivity, make the network resilient to node failures and data corruption. The network can gracefully degrade and maintain its functionality even in the presence of localized damage or adversarial attacks.</li>\n</ul>\n<p><strong>Example 2 (Distributed Pattern Recognition).</strong> Consider a network tasked with recognizing handwritten digits from the MNIST dataset [16]. Each input image is encoded as a binary vector <span class=\"math\">x \\in \\{0,1\\}^d</span>, where <span class=\"math\">d</span> is the number of pixels. The network consists of <span class=\"math\">N</span> nodes, each representing a learned prototype or a feature detector. The synaptic weights are trained using a competitive learning rule, such as the winner-takes-all (WTA) Hebbian rule:</p>\n<p><span class=\"math\">\\Delta w_{ij} = \n\\begin{cases}\n\\eta(x_j - w_{ij}) &amp; \\text{if } i = \\arg\\max_k w_k \\cdot x \\\\\n0 &amp; \\text{otherwise}\n\\end{cases}</span></p>\n<p>where <span class=\"math\">\\eta</span> is the learning rate and <span class=\"math\">w_i</span> is the weight vector of node <span class=\"math\">i</span>.</p>\n<p>During inference, an input image is presented to the network, and the nodes compete to match their prototypes with the input pattern. The node with the highest activation (i.e., the closest match) is selected as the winner, and its associated class label is assigned to the input. The sparse encoding and WTA dynamics enable fast and efficient recognition, while the distributed representation provides robustness to noise and occlusions.</p>\n<p>The example above showcases how the neurologically-inspired architecture can be applied to solve real-world problems in a distributed and efficient manner. The competitive learning and winner-takes-all dynamics enable the network to learn discriminative features and prototypes from the input data, while the sparse encoding and event-driven processing allow for fast and energy-efficient inference.</p>\n<p>The computational capabilities of the proposed architecture are not limited to the examples discussed here but encompass a wide range of information processing tasks across various domains. The versatility and adaptability of the neuromorphic approach make it a promising framework for developing intelligent and scalable distributed systems that can handle the complexity and dynamics of real-world data and applications.</p>\n<h2><a name=\"h-8-failure-modes-attack-vectors-and-defense-mechanisms-12\" class=\"anchor\" href=\"https://ethresear.ch#h-8-failure-modes-attack-vectors-and-defense-mechanisms-12\"></a>8 Failure Modes, Attack Vectors, and Defense Mechanisms</h2>\n<p>To ensure the practical viability and security of the proposed neurologically-inspired distributed ledger architecture, it is crucial to consider potential failure modes, attack vectors, and corresponding defense mechanisms. This section explores some of the key challenges and strategies for maintaining the resilience and integrity of the system.</p>\n<h3><a name=\"h-81-failure-modes-13\" class=\"anchor\" href=\"https://ethresear.ch#h-81-failure-modes-13\"></a>8.1 Failure Modes</h3>\n<h4><a name=\"h-811-node-failures-14\" class=\"anchor\" href=\"https://ethresear.ch#h-811-node-failures-14\"></a>8.1.1 Node Failures</h4>\n<p>One potential failure mode is the malfunction or unavailability of individual nodes in the network. Given the distributed nature of the architecture, the system should be designed to tolerate a certain level of node failures without compromising the overall functionality and performance. This can be achieved through redundancy, fault-tolerant consensus protocols, and self-healing mechanisms that enable the network to detect and recover from node failures.</p>\n<h4><a name=\"h-812-network-partitioning-15\" class=\"anchor\" href=\"https://ethresear.ch#h-812-network-partitioning-15\"></a>8.1.2 Network Partitioning</h4>\n<p>Another failure mode is network partitioning, where the network is split into isolated subgroups due to communication disruptions or connectivity issues. This can lead to inconsistencies and divergence in the ledger state across different partitions. To mitigate this risk, the architecture should incorporate partition-tolerant consensus algorithms, such as the pulse-coupled oscillator synchronization, which can maintain consistency and eventual synchronization even in the presence of network partitions.</p>\n<h4><a name=\"h-813-consensus-failures-16\" class=\"anchor\" href=\"https://ethresear.ch#h-813-consensus-failures-16\"></a>8.1.3 Consensus Failures</h4>\n<p>Consensus failures can occur when the nodes in the network fail to reach agreement on the ledger state or the validity of transactions. This can happen due to malicious behavior, network delays, or other factors that disrupt the consensus process. To prevent consensus failures, the architecture should employ robust and fault-tolerant consensus mechanisms, such as Byzantine fault tolerance (BFT) protocols, which can tolerate a certain number of malicious or faulty nodes while still reaching consensus.</p>\n<h3><a name=\"h-82-attack-vectors-17\" class=\"anchor\" href=\"https://ethresear.ch#h-82-attack-vectors-17\"></a>8.2 Attack Vectors</h3>\n<h4><a name=\"h-821-sybil-attacks-18\" class=\"anchor\" href=\"https://ethresear.ch#h-821-sybil-attacks-18\"></a>8.2.1 Sybil Attacks</h4>\n<p>Sybil attacks involve an attacker creating multiple fake identities or nodes to gain disproportionate influence over the network. This can be used to manipulate the consensus process, censor transactions, or perform double-spending attacks. To defend against Sybil attacks, the architecture can employ identity verification mechanisms, such as proof-of-work (PoW) or proof-of-stake (PoS), which make it computationally expensive or economically infeasible for an attacker to create and maintain a large number of fake identities.</p>\n<h4><a name=\"h-822-eclipse-attacks-19\" class=\"anchor\" href=\"https://ethresear.ch#h-822-eclipse-attacks-19\"></a>8.2.2 Eclipse Attacks</h4>\n<p>Eclipse attacks aim to isolate a targeted node or a group of nodes from the rest of the network by controlling their peer connections. This allows the attacker to filter or manipulate the information received by the targeted nodes, potentially leading to consensus failures or double-spending attacks. Defense mechanisms against eclipse attacks include diverse peer selection strategies, regular reshuffling of peer connections, and multi-path data propagation to ensure that nodes receive information from multiple sources.</p>\n<h4><a name=\"h-823-51-attacks-20\" class=\"anchor\" href=\"https://ethresear.ch#h-823-51-attacks-20\"></a>8.2.3 51% Attacks</h4>\n<p>In a 51% attack, an attacker gains control over a majority of the network’s computing power or stake, enabling them to dictate the consensus process and perform malicious activities such as double-spending or transaction censorship. To mitigate the risk of 51% attacks, the architecture can employ a combination of consensus mechanisms, such as PoW and PoS, that make it economically costly for an attacker to acquire a majority stake. Additionally, implementing a multi-layered security approach, with different consensus algorithms operating at different scales and levels of the network, can further enhance the resilience against 51% attacks.</p>\n<h3><a name=\"h-83-defense-mechanisms-21\" class=\"anchor\" href=\"https://ethresear.ch#h-83-defense-mechanisms-21\"></a>8.3 Defense Mechanisms</h3>\n<h4><a name=\"h-831-cryptographic-primitives-22\" class=\"anchor\" href=\"https://ethresear.ch#h-831-cryptographic-primitives-22\"></a>8.3.1 Cryptographic Primitives</h4>\n<p>The architecture should leverage secure cryptographic primitives, such as hash functions, digital signatures, and encryption algorithms, to ensure the integrity, authenticity, and confidentiality of data and transactions. These primitives form the foundation of the security mechanisms employed in the distributed ledger, preventing unauthorized modifications, forged transactions, and privacy breaches.</p>\n<h4><a name=\"h-832-secure-consensus-protocols-23\" class=\"anchor\" href=\"https://ethresear.ch#h-832-secure-consensus-protocols-23\"></a>8.3.2 Secure Consensus Protocols</h4>\n<p>Employing secure and fault-tolerant consensus protocols is crucial for maintaining the integrity and consistency of the ledger state. The proposed pulse-coupled oscillator synchronization mechanism provides a decentralized and resilient approach to achieving consensus. However, it can be further enhanced with additional security measures, such as verifiable random functions (VRFs) for leader selection, threshold signatures for collective signing, and multi-party computation (MPC) for secure computation and privacy preservation.</p>\n<h4><a name=\"h-833-incentive-mechanisms-24\" class=\"anchor\" href=\"https://ethresear.ch#h-833-incentive-mechanisms-24\"></a>8.3.3 Incentive Mechanisms</h4>\n<p>Designing appropriate incentive mechanisms is essential to encourage honest participation and discourage malicious behavior in the network. This can include block rewards, transaction fees, and stake-based incentives that align the economic interests of the nodes with the overall security and performance of the system. Penalty mechanisms, such as slashing or reputation scores, can be implemented to punish and deter malicious actors.</p>\n<h4><a name=\"h-834-monitoring-and-detection-25\" class=\"anchor\" href=\"https://ethresear.ch#h-834-monitoring-and-detection-25\"></a>8.3.4 Monitoring and Detection</h4>\n<p>Implementing robust monitoring and detection systems is crucial for identifying and responding to potential failures, attacks, and anomalies in the network. This can involve distributed intrusion detection systems (DIDS), machine learning-based anomaly detection, and real-time network analysis tools. By continuously monitoring the network activity and employing advanced detection techniques, the architecture can proactively identify and mitigate threats before they cause significant damage.</p>\n<h4><a name=\"h-835-formal-verification-and-testing-26\" class=\"anchor\" href=\"https://ethresear.ch#h-835-formal-verification-and-testing-26\"></a>8.3.5 Formal Verification and Testing</h4>\n<p>Rigorous formal verification and testing methodologies should be applied to validate the correctness, security, and performance of the proposed architecture. Formal verification techniques, such as model checking and theorem proving, can be used to mathematically prove the properties and guarantees of the consensus protocols, smart contracts, and cryptographic primitives. Comprehensive testing, including unit tests, integration tests, and stress tests, should be conducted to ensure the robustness and reliability of the implementation.</p>\n<p>By carefully considering and addressing these potential failure modes, attack vectors, and defense mechanisms, the practical viability and security of the neurologically-inspired distributed ledger architecture can be significantly strengthened. The combination of fault-tolerant design, secure consensus protocols, cryptographic primitives, incentive mechanisms, monitoring and detection systems, and formal verification and testing approaches provides a multi-layered defense strategy to ensure the resilience and integrity of the system in the face of various challenges and threats.</p>\n<h2><a name=\"h-9-comparison-with-state-of-the-art-blockchain-platforms-27\" class=\"anchor\" href=\"https://ethresear.ch#h-9-comparison-with-state-of-the-art-blockchain-platforms-27\"></a>9 Comparison with State-of-the-Art Blockchain Platforms</h2>\n<p>To highlight the advantages of the proposed neurologically-inspired distributed ledger architecture, it is informative to compare its performance and features with existing state-of-the-art blockchain platforms. This section provides a comparative analysis of the proposed architecture with two prominent blockchain platforms: Solana and Holochain.</p>\n<h3><a name=\"h-91-solana-28\" class=\"anchor\" href=\"https://ethresear.ch#h-91-solana-28\"></a>9.1 Solana</h3>\n<p>Solana is a high-performance blockchain platform that aims to achieve scalability without compromising security or decentralization. It employs a novel consensus mechanism called Proof-of-History (PoH) along with a Proof-of-Stake (PoS) protocol to enable fast and efficient transaction processing.</p>\n<p>Key features of Solana include:</p>\n<ul>\n<li>High throughput: Solana claims to support up to 65,000 transactions per second (TPS), making it one of the fastest blockchain platforms currently available.</li>\n<li>Low latency: Solana achieves sub-second transaction confirmation times, providing near-instant finality for users.</li>\n<li>Scalability: Solana’s architecture is designed to scale horizontally across a large number of nodes, enabling it to handle increasing transaction loads as the network grows.</li>\n<li>Developer-friendly: Solana supports smart contracts and provides a suite of tools and frameworks for developers to build decentralized applications (dApps) on its platform.</li>\n</ul>\n<p>However, Solana’s approach to scalability relies heavily on its PoH consensus mechanism, which requires a high degree of time synchronization across nodes. This can potentially introduce vulnerabilities and challenges in maintaining long-term security and decentralization.</p>\n<h3><a name=\"h-92-holochain-29\" class=\"anchor\" href=\"https://ethresear.ch#h-92-holochain-29\"></a>9.2 Holochain</h3>\n<p>Holochain is a framework for building decentralized applications that aims to provide scalability, adaptability, and agent-centric control. Unlike traditional blockchain platforms, Holochain does not rely on a global consensus mechanism. Instead, it allows each agent to maintain their own local chain and participate in distributed hash tables (DHTs) for data storage and retrieval.</p>\n<p>Key features of Holochain include:</p>\n<ul>\n<li>Agent-centric: Holochain puts the control and ownership of data in the hands of individual agents, allowing them to manage their own identities, keys, and data.</li>\n<li>Scalability: Holochain’s architecture enables linear scalability, as the capacity of the network grows with the number of participating agents.</li>\n<li>Adaptability: Holochain allows for flexible and adaptable dApp designs, as each app can define its own governance rules, validation logic, and data structures.</li>\n<li>Efficiency: Holochain’s DHT-based storage and retrieval mechanism is more efficient compared to traditional blockchain’s global replication of data.</li>\n</ul>\n<p>However, Holochain’s approach to decentralization and scalability comes with its own challenges. The lack of a global consensus mechanism can make it difficult to ensure consistency and integrity across the network, especially in the presence of malicious agents.</p>\n<h3><a name=\"h-93-comparison-table-30\" class=\"anchor\" href=\"https://ethresear.ch#h-93-comparison-table-30\"></a>9.3 Comparison Table</h3>\n<p>The following table summarizes the key features and performance metrics of the neurologically-inspired distributed ledger architecture in comparison to Solana and Holochain:</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Neurologically-Inspired</th>\n<th>Solana</th>\n<th>Holochain</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Consensus Mechanism</td>\n<td>Pulse-Coupled Oscillators</td>\n<td>PoH + PoS</td>\n<td>DHT-based</td>\n</tr>\n<tr>\n<td>Scalability</td>\n<td>High</td>\n<td>High</td>\n<td>Linear</td>\n</tr>\n<tr>\n<td>Transaction Throughput (TPS)</td>\n<td>100,000+</td>\n<td>65,000</td>\n<td>Application-dependent</td>\n</tr>\n<tr>\n<td>Transaction Confirmation Time</td>\n<td>Sub-second</td>\n<td>Sub-second</td>\n<td>Near-instant</td>\n</tr>\n<tr>\n<td>Smart Contract Support</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes (Zomes)</td>\n</tr>\n<tr>\n<td>Decentralization</td>\n<td>High</td>\n<td>Moderate</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Fault Tolerance</td>\n<td>High</td>\n<td>Moderate</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td>Adaptability</td>\n<td>High</td>\n<td>Moderate</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Energy Efficiency</td>\n<td>High</td>\n<td>Moderate</td>\n<td>High</td>\n</tr>\n</tbody>\n</table>\n</div><p>From the comparison table, it is evident that the neurologically-inspired distributed ledger architecture offers several advantages over Solana and Holochain:</p>\n<ul>\n<li>Scalability: The proposed architecture achieves high scalability through its neuromorphic design and pulse-coupled oscillator consensus mechanism, enabling efficient parallel processing and fast convergence.</li>\n<li>Transaction Throughput: With a potential transaction throughput of over 100,000 TPS, the neurologically-inspired architecture surpasses the performance of both Solana and Holochain.</li>\n<li>Decentralization: The architecture maintains a high degree of decentralization through its distributed ledger design and self-organizing consensus mechanism, ensuring resilience against centralized control.</li>\n<li>Fault Tolerance: The neuromorphic nature of the architecture, combined with robust consensus protocols and error correction mechanisms, provides a high level of fault tolerance against node failures and network disruptions.</li>\n<li>Adaptability: The architecture’s adaptability stems from its ability to dynamically reconfigure its synaptic connections and learning mechanisms based on the changing network conditions and computational requirements.</li>\n<li>Energy Efficiency: The event-driven and asynchronous processing model of the neurologically-inspired architecture enables energy-efficient computation, as resources are utilized only when necessary.</li>\n</ul>\n<p>While Solana and Holochain offer their own unique advantages, such as Solana’s high throughput and Holochain’s agent-centric approach, the neurologically-inspired distributed ledger architecture stands out in terms of its scalability, decentralization, fault tolerance, and adaptability. By leveraging the computational principles of biological neural networks, the proposed architecture addresses the limitations of traditional blockchain platforms and offers a promising solution for building efficient, robust, and intelligent decentralized systems.</p>\n<p>It is important to note that the performance metrics and features presented in the comparison table are based on the theoretical potential and design principles of the neurologically-inspired architecture. Further empirical evaluations and benchmarking studies are necessary to validate these claims and assess the practical performance of the architecture in real-world scenarios. Nonetheless, the comparative analysis highlights the potential advantages and unique characteristics of the proposed approach in relation to state-of-the-art blockchain platforms.</p>\n<h2><a name=\"h-10-conclusion-31\" class=\"anchor\" href=\"https://ethresear.ch#h-10-conclusion-31\"></a>10 Conclusion</h2>\n<p>In this paper, we have presented a novel distributed ledger architecture inspired by the information processing principles of biological neural networks. The proposed architecture aims to address the scalability, efficiency, and robustness challenges faced by traditional blockchain-based systems by leveraging key neurological mechanisms such as adaptive synaptic plasticity, associative memory, and self-organized consensus.</p>\n<p>The neuromorphic design, based on densely interconnected nodes communicating through synaptic connections, enables efficient and parallel processing of large-scale data and complex computations. The distributed associative memory scheme provides a scalable and content-addressable storage mechanism for securely encoding and retrieving data items. The decentralized consensus protocol, based on the synchronization of pulse-coupled oscillators, allows for fast and robust coordination among nodes without relying on global communication or centralized control.</p>\n<p>The computational capabilities of the architecture, arising from its rich dynamics and adaptive learning properties, make it a versatile framework for solving a wide range of information processing tasks, including inference, constraint satisfaction, pattern recognition, and optimization. The sparse encoding, event-driven processing, and fault-tolerant dynamics offer significant advantages in terms of scalability, efficiency, and robustness compared to traditional distributed computing approaches.</p>\n<p>The proposed architecture opens up new possibilities for building intelligent and sustainable distributed systems that can handle the growing complexity and scale of modern data and applications. The neurologically-inspired design principles can be applied across various domains, such as decentralized finance, supply chain management, Internet of Things, and edge computing, to develop secure, efficient, and adaptable solutions.</p>\n<p>However, realizing the full potential of this architecture requires further research and development efforts. Some key challenges and future directions include:</p>\n<ul>\n<li>Developing efficient and scalable hardware implementations of neuromorphic substrates that can support the massive parallelism and connectivity required by the architecture.</li>\n<li>Designing novel learning algorithms and synaptic plasticity rules that can effectively capture the complex patterns and dependencies in real-world data while being computationally tractable and biologically plausible.</li>\n<li>Integrating advanced cryptographic primitives and privacy-preserving mechanisms into the architecture to ensure the security and confidentiality of data and computations.</li>\n<li>Conducting extensive empirical evaluations and benchmarking studies to assess the performance and robustness of the architecture on real-world datasets and application scenarios.</li>\n<li>Establishing formal theoretical foundations for analyzing the convergence, stability, and computational complexity of the proposed algorithms and protocols.</li>\n</ul>\n<p>In conclusion, the neurologically-inspired distributed ledger architecture presents a promising approach to designing scalable, efficient, and robust decentralized systems that can tackle the challenges of the rapidly evolving data-driven world. By bridging the gap between biological and artificial information processing, this architecture paves the way for the development of a new generation of intelligent and sustainable distributed systems.</p>\n<h2><a name=\"references-32\" class=\"anchor\" href=\"https://ethresear.ch#references-32\"></a>References</h2>\n<p>[1] Androulaki, E., Barger, A., Bortnikov, V., Cachin, C., Christidis, K., De Caro, A., … &amp; Yellick, J. (2018, April). Hyperledger fabric: a distributed operating system for permissioned blockchains. In Proceedings of the thirteenth EuroSys conference (pp. 1-15).</p>\n<p>[2] Baza, M., Nabil, M., Lasla, N., Fidan, K., Mahmoud, M., &amp; Abdallah, M. (2019). Blockchain-based firmware update scheme tailored for autonomous vehicles. In 2019 IEEE Wireless Communications and Networking Conference (WCNC) (pp. 1-7). IEEE.</p>\n<p>[3] Bell, A. J., &amp; Sejnowski, T. J. (1997). The “independent components” of natural scenes are edge filters. Vision research, 37(23), 3327-3338.</p>\n<p>[4] Bi, G. Q., &amp; Poo, M. M. (1998). Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type. Journal of neuroscience, 18(24), 10464-10472.</p>\n<p>[5] Bruck, J. (1990). On the convergence properties of the Hopfield model. Proceedings of the IEEE, 78(10), 1579-1585.</p>\n<p>[6] Calzavara, S., Lande, S., &amp; Oswald, D. (2020). Private Smart Contracts in Proof-of-Stake Blockchains. In 2020 IEEE European Symposium on Security and Privacy (EuroS&amp;P) (pp. 362-378). IEEE.</p>\n<p>[7] Davies, M., Srinivasa, N., Lin, T. H., Chinya, G., Cao, Y., Choday, S. H., … &amp; Wang, H. (2018). Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro, 38(1), 82-99.</p>\n<p>[8] Drachman, D. A. (2005). Do we have brain to spare?. Neurology, 64(12), 2004-2005.</p>\n<p>[9] Hinton, G. E., &amp; Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. science, 313(5786), 504-507.</p>\n<p>[10] Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 2554-2558.</p>\n<p>[11] Hopfield, J. J. (1984). Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the national academy of sciences, 81(10), 3088-3092.</p>\n<p>[12] Hopfield, J. J., &amp; Tank, D. W. (1985). “Neural” computation of decisions in optimization problems. Biological cybernetics, 52(3), 141-152.</p>\n<p>[13] Indyk, P., &amp; Motwani, R. (1998, May). Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing (pp. 604-613).</p>\n<p>[14] Koller, D., &amp; Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press.</p>\n<p>[15] Lamport, L., Shostak, R., &amp; Pease, M. (1982). The Byzantine generals problem. ACM Transactions on Programming Languages and Systems (TOPLAS), 4(3), 382-401.</p>\n<p>[16] LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324.</p>\n<p>[17] Maass, W., Natschläger, T., &amp; Markram, H. (2002). Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural computation, 14(11), 2531-2560.</p>\n<p>[18] Merolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy, A. S., Sawada, J., Akopyan, F., … &amp; Modha, D. S. (2014). A million spiking-neuron integrated circuit with a scalable communication network and interface. Science, 345(6197), 668-673.</p>\n<p>[19] Mirollo, R. E., &amp; Strogatz, S. H. (1990). Synchronization of pulse-coupled biological oscillators. SIAM Journal on Applied Mathematics, 50(6), 1645-1662.</p>\n<p>[20] Nakamoto, S. (2008). Bitcoin: A peer-to-peer electronic cash system. Decentralized Business Review, 21260.</p>\n<p>[21] Newman, M. E. (2001). The structure of scientific collaboration networks. Proceedings of the national academy of sciences, 98(2), 404-409.</p>\n<p>[22] Russell, S., &amp; Norvig, P. (2016). Artificial intelligence: a modern approach. Pearson Education Limited.</p>\n<p>[23] Strogatz, S. H. (2000). From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators. Physica D: Nonlinear Phenomena, 143(1-4), 1-20.</p>\n<p>[24] Vujičić, D., Jagodić, D., &amp; Ranđić, S. (2018, March). Blockchain technology, bitcoin, and Ethereum: A brief overview. In 2018 17th international symposium infoteh-jahorina (infoteh) (pp. 1-6). IEEE.</p>\n<p>[25] Wood, G. (2014). Ethereum: A secure decentralised generalised transaction ledger. Ethereum project yellow paper, 151(2014), 1-32.</p>\n<p>[26] Xiao, Y., Zhang, N., Lou, W., &amp; Hou, Y. T. (2020). A survey of distributed consensus protocols for blockchain networks. IEEE Communications Surveys &amp; Tutorials, 22(2), 1432-1465.</p>\n<p>*<strong>Paper written with the help of Claude 3 Opus. Thoughts and concepts conveyed are my own.</strong></p>\n            <p><small>2 posts - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/unbounded-scaling-of-a-fully-decentralized-network-without-global-coordination-may-the-4th-be-with-you/19456\">Read full topic</a></p>","link":"https://ethresear.ch/t/unbounded-scaling-of-a-fully-decentralized-network-without-global-coordination-may-the-4th-be-with-you/19456","pubDate":"Sat, 04 May 2024 16:20:24 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19456"},"source":{"@url":"https://ethresear.ch/t/unbounded-scaling-of-a-fully-decentralized-network-without-global-coordination-may-the-4th-be-with-you/19456.rss","#text":"Unbounded Scaling of a fully Decentralized Network without Global Coordination - - May the 4th Be With You"},"filter":false},{"title":"CEX/DEX arbitrage, transaction fees, block times, and LP profits","dc:creator":"atiselsts","category":"Economics","description":"<p>It’s widely recognized that CEX/DEX arbitrage trades create a large part of DEX volume, perhaps even the majority of that volume. The <a href=\"https://a16zcrypto.com/posts/article/lvr-quantifying-the-cost-of-providing-liquidity-to-automated-market-makers/\" rel=\"noopener nofollow ugc\">Loss Versus Rebalancing</a> (LVR) model stands out as a key tool for quantifying and modeling this arbitrage volume from a theoretical perspective. However, the research focusing on LVR  so far has mostly ignored transaction cost as a parameter in CEX/DEX arbitrage.</p>\n<p>This post aims to extend the LVR model to blockchains such as Ethereum’s mainnet, where CEX/DEX arbitrage transactions are expected to have a significant fixed cost term. It conceptualizes LVR as a quantity that is distributed between three primary actors: (1) the LPs of the AMM, (2) the searcher-builder-proposer (SBP) as an aggregate entity, and (3) ETH holders, due to the block basefee that is burned by each transaction. Some implications are:</p>\n<ul>\n<li>As the block time is decreased, an increased share of the nominal LVR is spent on transaction fees.</li>\n<li>Liquidity provider (LP) losses from arbitrage trades do not have the same magnitude as the profits of the arbitrager (searcher-builder-proposer), and as such, are not accurately predicted by a model that approximates them with the square root of the block time.</li>\n<li>Changes in Ethereum’s block time (either increase or decrease) are expected to affect the profitability of AMM LPs, but in many situations, other factors are more important, including the transaction fees.</li>\n</ul>\n<p><em>A more explanatory and less formal version of this post is available on <a href=\"https://atise.medium.com/anatomy-of-cex-dex-arbitrage-481936c83831\" rel=\"noopener nofollow ugc\">Medium</a>.</em></p>\n<h1><a name=\"background-1\" class=\"anchor\" href=\"https://ethresear.ch#background-1\"></a>Background</h1>\n<p>According to (<a href=\"https://arxiv.org/abs/2208.06046\" rel=\"noopener nofollow ugc\">Millionis 2022</a>, <a href=\"https://arxiv.org/abs/2305.14604\" rel=\"noopener nofollow ugc\">Millionis 2023</a>) the expected instantaneous LVR is :</p>\n<p><span class=\"math\">\n\\overline{\\mathrm{LVR}} \\triangleq \\lim _{T \\rightarrow 0} \\frac{\\mathrm{E}\\left[\\mathrm{LVR}_T\\right]}{T}=\\frac{\\sigma^2 P}{2} \\times y^{* \\prime}(P) </span> .</p>\n<p>This quantity depends only on the volatility, price, and marginal liquidity of the pool. By integrating the expected instantaneous LVR over time, we can obtain the expected LVR for a time period <span class=\"math\">t</span>. Once again, it is not dependent on external factors such as swap fees, block times, transaction costs, etc., and can serve as a nominal baseline metric for any further investigations in this area.</p>\n<p>In (Millionis 2023) the authors push their LVR model further, and consider a situation when the AMM has a trading fee <span class=\"math\">γ ≥ 0</span>, and that arbitrageurs arrive to trade on the AMM at discrete times according to the arrivals of a Poisson process with rate <span class=\"math\">λ &gt; 0</span>. They extend the asymptotic analysis of arbitrage profit in a fast block regime (<span class=\"math\">\\lambda \\rightarrow \\infty</span>). They establish a key result that <span class=\"math\">\\overline{ARB} = Θ( \\sqrt{λ^{-1}} )</span>, where <span class=\"math\">\\overline{ARB}</span> is the expected arbitrage profits over time. The authors do not explicitly discuss the case when block times are not Poisson distributed, however, intuitively, one can expect the approximation to remain reasonably accurate when the blocks are uniformly distributed. To formalize this idea: <span class=\"math\">\\overline{ARB} = Θ( \\sqrt{BT} )</span>, where <span class=\"math\">BT</span> is the average block time.</p>\n<p>One key question is whether we see this formula play out in real-world data? A group of research papers generally confirm the <span class=\"math\">\\sqrt{BT}</span> model:</p>\n<ul>\n<li>(<a href=\"https://arxiv.org/abs/2308.04159\" rel=\"noopener nofollow ugc\">McMenamin 2023</a>) draws an analogy between the model and the time value of options, which “typically grows proportionally to the square root of time to expiration.”</li>\n<li>(<a href=\"https://arxiv.org/abs/2403.09494v2\" rel=\"noopener nofollow ugc\">Adams 2024</a>) describe shorter block times as a reason for  increased fee returns for liquidity providers, stressing the difference between Optimism and Arbitrum, in favor of later due to shorter blocks.</li>\n<li>(<a href=\"https://arxiv.org/abs/2404.05803\" rel=\"noopener nofollow ugc\">Fritsch 2024</a>) empirically study the arbitrage profits predicted by the LVR model, and conclude that “our empirical findings come close to [the <span class=\"math\">\\sqrt{BT}</span> model] for most pairs and block times larger than 1s”, and attempt to explain any deviations as a result of: (1) uniform block times (i.e. not Poisson-distributed), and (2) price action that does not match the Geometric Brownian motion (GBM) model. Their work is a step towards verifying the <span class=\"math\">\\sqrt{BT}</span> model – however, it must be stressed that they measure arbitrage profits, not the LP losses.</li>\n</ul>\n<p>In contrast:</p>\n<ul>\n<li>(<a href=\"https://arxiv.org/abs/2312.13749\" rel=\"noopener nofollow ugc\">Dahi 2023</a>) investigate AMM on the XRP ledger and find only a tiny impact on the LPs: 0.35% in relative terms, if block time in simulations is reduced from 12 seconds to 4 seconds. This is much smaller than predicted by the <span class=\"math\">\\sqrt{BT}</span> model.</li>\n<li>The Uniswap Foundation’s <a href=\"https://medium.com/@atise/liquidity-provider-strategies-for-uniswap-v3-loss-versus-rebalancing-lvr-ee0ffdf1f937\" rel=\"noopener nofollow ugc\">LP strategies series</a> include <a href=\"https://github.com/atiselsts/uniswap-lp-articles-code/blob/500d54032e00db88147db97124acab66a82d3979/plot_article_3.py\" rel=\"noopener nofollow ugc\">simulation results</a> that show a limited dependence on the block time, overshadowed by other factors.</li>\n</ul>\n<p>How do we reconcile these differing results and bridge the gap between theory and practice?</p>\n<p>The LVR model treats the arbitrage problem as a two-player, zero-sum game, where <span class=\"math\">\\overline{ARB} = - \\overline{LP} </span> (where the latter term refers to the expected LP profits). However, this assumption is not valid in the post EIP-1559 world, where transactions cannot be free. Each arbitrage trade not only divides profits among the searcher, builder, and proposer (“SBP” further in this article) but also burns some ETH, contingent on the blockspace demand at the time of the arbitrage. To borrow a term from physics, the basefee introduces a friction in the process. This friction eliminates a significant portion of potential trades, and reduces LP income.</p>\n<h1><a name=\"analyzing-the-single-trade-lvr-2\" class=\"anchor\" href=\"https://ethresear.ch#analyzing-the-single-trade-lvr-2\"></a>Analyzing the single-trade LVR</h1>\n<p>Let’s look at the details of how LVR arises in real-world arbitrage trades. The difference between the DEX and CEX quoted prices (<span class=\"math\">P_{DEX}</span> and <span class=\"math\">P_{CEX}</span>) triggers the arbitrage trade. However, the trade is not going to happen if:</p>\n<ul>\n<li>The CEX price is in the non-arbitrage region created by AMM’s swap fees.</li>\n<li>The CEX price is in the friction region, created by the chain’s basefees and other factors (CEX fees, other  operational costs for arbitrager, risk aversion, etc.).</li>\n</ul>\n<p><img src=\"https://ethresear.ch/uploads/default/original/3X/6/7/672739182dd8b8981b4f531f74a221f1d289dd42.png\" alt=\"No-trade and friction regions\" data-base62-sha1=\"eIxdi9yS6EkUXdjFAecp1xhaT4e\" width=\"498\" height=\"195\"><br>\n<em><strong>Figure 1:</strong> Non-arbitrage and friction regions of an AMM pool with 0.05% swap fee.</em></p>\n<p>Moreover, the nominal single-trade LVR is “distributed” between three entities:</p>\n<ul>\n<li>Liquidity providers.</li>\n<li>The searcher, block builder, and block proposer (SBP) as a collective entity.</li>\n<li>Holders of ETH, due to the ETH burned in the transaction.</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/9/5/953c0c9cfa203ff4a017fba5b4f37c5d4c17b2e3.png\" data-download-href=\"https://ethresear.ch/uploads/default/953c0c9cfa203ff4a017fba5b4f37c5d4c17b2e3\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/9/5/953c0c9cfa203ff4a017fba5b4f37c5d4c17b2e3_2_345x86.png\" alt=\"image\" data-base62-sha1=\"libNtLgylsSTNZVtY5aRKipuh9h\" width=\"345\" height=\"86\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/9/5/953c0c9cfa203ff4a017fba5b4f37c5d4c17b2e3_2_345x86.png, https://ethresear.ch/uploads/default/optimized/3X/9/5/953c0c9cfa203ff4a017fba5b4f37c5d4c17b2e3_2_517x129.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/9/5/953c0c9cfa203ff4a017fba5b4f37c5d4c17b2e3_2_690x172.png 2x\" data-dominant-color=\"9BB9AC\"></a></div><br>\n<em><strong>Figure 2:</strong> The nominal LVR is distributed between three entities: ETH holders (due to the burned basefees), LPs, and the searcher/builder/proposer as a collective entity.</em><p></p>\n<p><strong>LPs</strong> receive the swap fee, while the <strong>SBP</strong> collects the arbitrage profits, which are subsequently divided among these three actors. It’s no surprise that integrated searchers-builders dominate the arbitrage market (<a href=\"https://arxiv.org/abs/2401.01622\" rel=\"noopener nofollow ugc\">Heimbach 2024</a>), as for them it is simpler to divide up the profits. <strong>ETH</strong> <strong>holders</strong> do not directly receive compensation, but burning the basefee creates a deflationary pressure on ETH as an asset.</p>\n<p>By comparing the nominal single-trade LVR with the LP fees from that trade, we can assess the fairness of the trade to the LP. In a scenario where price evolution is smooth without any jumps, the LP fee nearly recoups the LVR, and the LP loss is minimal. However, if the DEX-to-CEX price difference fluctuates due to block time granularity or actual price discontinuities on the CEX, the LP fee becomes smaller than the LVR, resulting in some loss for the LP against the theoretical rebalancing strategy. One example is shown in the figure below:</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/2/7/27985324648027915d4f1d0cbb40dcb010494d61.png\" data-download-href=\"https://ethresear.ch/uploads/default/27985324648027915d4f1d0cbb40dcb010494d61\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/2/7/27985324648027915d4f1d0cbb40dcb010494d61_2_690x240.png\" alt=\"image\" data-base62-sha1=\"5EgYutFyIfKo5NbBgWhwQp9n46B\" width=\"690\" height=\"240\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/2/7/27985324648027915d4f1d0cbb40dcb010494d61_2_690x240.png, https://ethresear.ch/uploads/default/original/3X/2/7/27985324648027915d4f1d0cbb40dcb010494d61.png 1.5x, https://ethresear.ch/uploads/default/original/3X/2/7/27985324648027915d4f1d0cbb40dcb010494d61.png 2x\" data-dominant-color=\"C7DBDD\"></a></div><p></p>\n<p><em><strong>Figure 3:</strong> Distribution of the LVR between the three actors. Relative scale. The LVR created by 0.1 % price changes has a much more equitable distribution than the LVR created by 1.0% price change at once.</em></p>\n<h1><a name=\"computing-the-nominal-lvr-3\" class=\"anchor\" href=\"https://ethresear.ch#computing-the-nominal-lvr-3\"></a>Computing the nominal LVR</h1>\n<p>Let’s assume that we are a given sequence of CEX prices at <span class=\"math\">BT</span> intervals and a constant product AMM (i.e. AMM that follows the equation <span class=\"math\">xy=k</span>). In order to compute the empirical approximation of the nominal LVR defined as above, we can use the following algorithm:</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/a/7/a7817e6afc2ee78f955045da5d6d1e370b8bc64e.png\" data-download-href=\"https://ethresear.ch/uploads/default/a7817e6afc2ee78f955045da5d6d1e370b8bc64e\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/a/7/a7817e6afc2ee78f955045da5d6d1e370b8bc64e_2_552x500.png\" alt=\"image\" data-base62-sha1=\"nTPaXB8hoi52GMTeK1IJWGTGqJw\" width=\"552\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/a/7/a7817e6afc2ee78f955045da5d6d1e370b8bc64e_2_552x500.png, https://ethresear.ch/uploads/default/optimized/3X/a/7/a7817e6afc2ee78f955045da5d6d1e370b8bc64e_2_828x750.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/a/7/a7817e6afc2ee78f955045da5d6d1e370b8bc64e_2_1104x1000.png 2x\" data-dominant-color=\"FBFBFB\"></a></div><p></p>\n<p>The results of the algorithm have been verified to match both the nominal LVR and the arbitrage trade probability as defined in (Millionis 2023) – see <a href=\"https://atise.medium.com/anatomy-of-cex-dex-arbitrage-481936c83831\" rel=\"noopener nofollow ugc\">“Replicating the theoretical results”</a> section in the Medium post.</p>\n<h1><a name=\"simulation-studies-4\" class=\"anchor\" href=\"https://ethresear.ch#simulation-studies-4\"></a>Simulation studies</h1>\n<p>Let’s model the in-range liquidity of the <a href=\"https://app.uniswap.org/explore/pools/ethereum/0x88e6A0c2dDD26FEEb64F039a2c41296FcB3f5640\" rel=\"noopener nofollow ugc\">Uniswap v3 ETH/USDC 0.05% pool</a>. As of April 2024, it has approximately $1 billion worth of virtual assets, corresponding to approximately $150 millions of real assets. As a result, the liquidity concentration factor is between 6 and 7. It’s essential to have deep liquidity if we want arbitrage swaps to happen even on relatively small price changes.</p>\n<p>The graphs below show the DEX performance metrics using random GBM simulations. The simulations assume 50% yearly volatility, which corresponds to approximately 2.6% daily volatility and 0.03% per-block volatility for 12-second blocks. This is the approximate volatility of ETH in the recent years.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/5/2/52ed204954c80c12fe60d115b8530dc19e33c5d9.png\" data-download-href=\"https://ethresear.ch/uploads/default/52ed204954c80c12fe60d115b8530dc19e33c5d9\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/5/2/52ed204954c80c12fe60d115b8530dc19e33c5d9_2_517x369.png\" alt=\"image\" data-base62-sha1=\"bPBbXFRxaXGCM8wZvpk3YoS2Gmd\" width=\"517\" height=\"369\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/5/2/52ed204954c80c12fe60d115b8530dc19e33c5d9_2_517x369.png, https://ethresear.ch/uploads/default/original/3X/5/2/52ed204954c80c12fe60d115b8530dc19e33c5d9.png 1.5x, https://ethresear.ch/uploads/default/original/3X/5/2/52ed204954c80c12fe60d115b8530dc19e33c5d9.png 2x\" data-dominant-color=\"F2F2F1\"></a></div><br>\n<em><strong>Figure 4:</strong> Simulated DEX metrics with zero basefee.</em><p></p>\n<p>Due to a coincidence, the LVR turns out to be approximately $1 per second or $3600 per hour. The LP losses in turn are in the range from $350 to $900 per hour ($3 to $8 million per year). To be clear, this theoretical model does not include any LP fees coming from noise / uninformed traders, which are expected to produce zero LVR, and consequently compensate for the LP losses from the arbitrage trades.</p>\n<p>The results show that when the basefee is zero, the LP losses indeed can be more-or-less accurately modeled by the function <span class=\"math\">\\sqrt{BT}</span>. However, after introducing EIP-1559 basefees to the model, this isn’t true anymore.</p>\n<p>The subsequent two graphs show what happens with when each transaction is assumed to cost a fixed amount of $. A simple Uniswap v3 swap might consume around 150’000 gas, which corresponds to $10 cost per swap in USD terms – assuming the reasonable 22 gwei basefee cost and $3000 ETH/USDC price. In times of high usage or high volatility, the cost of a transaction can easily increase several times.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/5/a/5afc2619927bd3babb3015f3cb3ed9e9b00cd2b8.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/5afc2619927bd3babb3015f3cb3ed9e9b00cd2b8\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/5/a/5afc2619927bd3babb3015f3cb3ed9e9b00cd2b8_2_517x369.jpeg\" alt=\"image\" data-base62-sha1=\"cYTcjQHdt79uojqCj33VWL8Dpos\" width=\"517\" height=\"369\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/5/a/5afc2619927bd3babb3015f3cb3ed9e9b00cd2b8_2_517x369.jpeg, https://ethresear.ch/uploads/default/original/3X/5/a/5afc2619927bd3babb3015f3cb3ed9e9b00cd2b8.jpeg 1.5x, https://ethresear.ch/uploads/default/original/3X/5/a/5afc2619927bd3babb3015f3cb3ed9e9b00cd2b8.jpeg 2x\" data-dominant-color=\"F1F0F0\"></a></div><br>\n<em><strong>Figure 5:</strong> Simulated DEX metrics with $10 per swap spent on the basefee.</em><p></p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/4/2/42c2d88113ebb2d718f19a54de820c151ca37b90.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/42c2d88113ebb2d718f19a54de820c151ca37b90\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/4/2/42c2d88113ebb2d718f19a54de820c151ca37b90_2_517x369.jpeg\" alt=\"image\" data-base62-sha1=\"9wAY1idJtClIQWeCWwS5L8yDVv2\" width=\"517\" height=\"369\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/4/2/42c2d88113ebb2d718f19a54de820c151ca37b90_2_517x369.jpeg, https://ethresear.ch/uploads/default/original/3X/4/2/42c2d88113ebb2d718f19a54de820c151ca37b90.jpeg 1.5x, https://ethresear.ch/uploads/default/original/3X/4/2/42c2d88113ebb2d718f19a54de820c151ca37b90.jpeg 2x\" data-dominant-color=\"F1F0F0\"></a></div><br>\n<em><strong>Figure 6:</strong> Simulated DEX metrics with $30 per swap spent on the basefee.</em><p></p>\n<p>For a summary of the LP losses, see the figure below. Even adding a constant offset from the <em>x</em> axis to the <span class=\"math\">\\sqrt{BT}</span>. model does not lead to a good fit (the brown line). More frequent transactions also burn more ETH, thus canceling out most of the positive impact on the LP fees.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/b/5/b53b8658e7ad04dc746ae61a681bd639a0c3d72e.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/b53b8658e7ad04dc746ae61a681bd639a0c3d72e\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/b/5/b53b8658e7ad04dc746ae61a681bd639a0c3d72e_2_517x370.jpeg\" alt=\"image\" data-base62-sha1=\"pRfX707wr0qsMtNX8RgvFb5nH6m\" width=\"517\" height=\"370\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/b/5/b53b8658e7ad04dc746ae61a681bd639a0c3d72e_2_517x370.jpeg, https://ethresear.ch/uploads/default/original/3X/b/5/b53b8658e7ad04dc746ae61a681bd639a0c3d72e.jpeg 1.5x, https://ethresear.ch/uploads/default/original/3X/b/5/b53b8658e7ad04dc746ae61a681bd639a0c3d72e.jpeg 2x\" data-dominant-color=\"F0ECEC\"></a></div><br>\n<em><strong>Figure 7:</strong> The <span class=\"math\">\\sqrt{BT}</span> model shows a good-but-not-perfect fit if the basefee is zero, but much worse otherwise.</em><p></p>\n<h3><a name=\"simulations-with-longer-block-times-5\" class=\"anchor\" href=\"https://ethresear.ch#simulations-with-longer-block-times-5\"></a>Simulations with longer block times</h3>\n<p>Following Table 1 in (Millionis 2023), I also include results with 120 second and 600 second blocktimes.</p>\n<p>Uniformly distributed blocks, $10 swap basefees:</p>\n<pre><code class=\"lang-auto\">swap fee:                          1bp   5bp  10bp  30bp 100bp\nblock time   600 sec, arb prob %: 92.7  85.2  76.4  50.5  21.7 \nblock time   120 sec, arb prob %: 83.7  68.4  53.5  27.4   9.9 \nblock time    12 sec, arb prob %: 53.1  29.9  19.1   7.7   2.0 \nblock time     2 sec, arb prob %: 20.8   9.4   5.5   1.8   0.1 \nswap fee:                          1bp   5bp  10bp  30bp 100bp\nblock time   600 sec, LP loss  %: 96.3  83.5  71.3  44.8  19.6 \nblock time   120 sec, LP loss  %: 92.1  69.1  52.5  26.9  10.0 \nblock time    12 sec, LP loss  %: 80.0  44.2  28.3  11.6   3.8 \nblock time     2 sec, LP loss  %: 69.6  31.3  18.6   7.1   2.2 \n</code></pre>\n<p>Uniformly distributed blocks, $30 swap basefees:</p>\n<pre><code class=\"lang-auto\">swap fee:                          1bp   5bp  10bp  30bp 100bp\nblock time   600 sec, arb prob %: 88.8  81.5  72.8  48.1  20.8 \nblock time   120 sec, arb prob %: 75.4  61.0  47.8  24.5   8.8 \nblock time    12 sec, arb prob %: 36.2  21.3  14.0   5.8   1.5 \nblock time     2 sec, arb prob %: 10.7   5.5   3.3   1.1   0.1 \nswap fee:                          1bp   5bp  10bp  30bp 100bp\nblock time   600 sec, LP loss  %: 96.3  83.6  71.4  45.0  19.8 \nblock time   120 sec, LP loss  %: 92.3  69.8  53.3  27.6  10.3 \nblock time    12 sec, LP loss  %: 82.5  48.5  32.0  13.5   4.5 \nblock time     2 sec, LP loss  %: 76.5  39.5  24.6   9.8   3.2 \n</code></pre>\n<p>The results show that:</p>\n<ul>\n<li>The approximation from (Millionis 2023) that arbitrage probability is approximately equal to LP loss does not hold, in general.</li>\n<li>In most cases, the swap fee is a more dominant factor for the LPs than either the block time or basefee.</li>\n<li>Increasing the block time does increase LP losses, but by a much smaller factor than predicted by the <span class=\"math\">\\sqrt{BT}</span> model, especially in pools with low swap fees (1 bps to 10 bps).</li>\n</ul>\n<h1><a name=\"discussion-6\" class=\"anchor\" href=\"https://ethresear.ch#discussion-6\"></a>Discussion</h1>\n<h3><a name=\"generalizing-the-results-7\" class=\"anchor\" href=\"https://ethresear.ch#generalizing-the-results-7\"></a>Generalizing the results</h3>\n<ol>\n<li>\n<p><strong>Generalizing to other pools.</strong> The simulation results above are specific to the USDC/ETH 0.05% pool, which is typically the most liquid volatile pool on Uniswap v3. (There are stable pools with deeper liquidity). For pools with <strong>lower liquidity,</strong> the importance of the friction created by the basefee would be increased. For instance, let’s say that the ETH/USDT 0.05% pool has 1/3 of the liquidity. Then the results from $10 basefee on that pool would match those of the USDC/ETH pools with $30 basefee. For pools with <strong>higher liquidity</strong> (such as USDC/USDT) the friction would be decreased, proportional to the rise in liquidity.</p>\n</li>\n<li>\n<p><strong>Generalizing to other chains.</strong> The model assumes that shorter blocks simply divide the available blockspace differently, rather than add more blockspace. The simulation results would not generalize if the block time decrease was accompanied with a proportional decrease in basefees.</p>\n</li>\n</ol>\n<h3><a name=\"implications-for-blockchain-design-8\" class=\"anchor\" href=\"https://ethresear.ch#implications-for-blockchain-design-8\"></a>Implications for blockchain design</h3>\n<p>The results clearly show that LP losses are not accurately predicted by the <span class=\"math\">\\sqrt{BT}</span> model, unless the basefee is close to zero. While it’s true that shorter blocks benefit LPs, the effect is limited and frequently less significant than other factors (basefee, liquidity depth, swap fee %, and potentially others). More compelling arguments for fast block times may come from other perspectives, like the perspective of traders, or that of block builders, but these are out of the scope for this article.</p>\n<p>Potentially, we can view the optimal block time choice problem from two viewpoints:</p>\n<ul>\n<li>From <strong>L1 perspective</strong>, reducing the basefee could provide a quicker win, compared with changing the block time. However, keeping the design decentralized and credibly-neutral is arguably a higher priority.</li>\n<li>From <strong>L2 / appchain perspective,</strong> especially for chains that focus on DeFi or on trading in particular, it makes sense to excessively optimize for LP profits, including block time and basefee reduction.</li>\n<li>\n<ul>\n<li>The <span class=\"math\">\\sqrt{BT}</span> model implies that decreasing block times is especially important if the block time is already close to zero, since the <span class=\"math\">\\sqrt{~}</span> function rapidly grows in the near-zero region.</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>L2 fees may already be low enough to minimize the friction caused by the basefee to negligible levels. If this isn’t the case, then paradoxically, exempting CEX/DEX arbitrage swaps from the EIP-1559 basefee would benefit DEX users (both LPs and retail traders).</li>\n</ul>\n</li>\n</ul>\n<h3><a name=\"shorter-blocks-less-mev-9\" class=\"anchor\" href=\"https://ethresear.ch#shorter-blocks-less-mev-9\"></a>Shorter blocks = less MEV?</h3>\n<p>To be clear, this article makes no claims about MEV in general, just about CEX/DEX arbitrage in particular. Regarding the latter, clearly, there’s a connection between CEX/DEX arbitrage, transaction fees, and block times. However, it is perhaps confusing because this connection goes <strong>both ways</strong>:</p>\n<ul>\n<li>Shorter blocks increase the number of arbitrage transactions and the proportion of gas that they consume.</li>\n<li>On the other hand, shorter blocks decrease the expected value of LP losses.</li>\n</ul>\n<p>The same conflicting results apply to changes in basefees. As a result, there’s potential for a confusion, because MEV is increased in one sense, and decreased in another sense.</p>\n<h1><a name=\"conclusion-10\" class=\"anchor\" href=\"https://ethresear.ch#conclusion-10\"></a>Conclusion</h1>\n<p>This work extends the existing LVR and LVR-with-fees models by adding another component: transaction cost. To summarize the main results:</p>\n<ul>\n<li>CEX/DEX arbitrage transactions are not frictionless due to the EIP-1559 basefee and other factors.</li>\n<li>As a result, the nominal LVR in real-world AMMs is divided among three entities: LPs, ETH stakers, and the SBP as a collective entity.</li>\n<li>More gradual price changes result in more equitable LVR distribution among these three entities.</li>\n<li>On chains with significant transaction costs, LP losses under the LVR assumptions are not accurately predicted by the <span class=\"math\">\\sqrt{BT}</span> model.</li>\n<li>The LP losses are determined by several factors, including basefees, swap fees, and block times, the relative importance of which varies.</li>\n</ul>\n<p><em>Source code of the simulations <a href=\"https://github.com/atiselsts/cex-dex-arbitrage-anatomy\" rel=\"noopener nofollow ugc\"><em>is available here</em></a></em>.</p>\n            <p><small>4 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/cex-dex-arbitrage-transaction-fees-block-times-and-lp-profits/19444\">Read full topic</a></p>","link":"https://ethresear.ch/t/cex-dex-arbitrage-transaction-fees-block-times-and-lp-profits/19444","pubDate":"Thu, 02 May 2024 16:37:01 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19444"},"source":{"@url":"https://ethresear.ch/t/cex-dex-arbitrage-transaction-fees-block-times-and-lp-profits/19444.rss","#text":"CEX/DEX arbitrage, transaction fees, block times, and LP profits"},"filter":false},{"title":"Integrity proofs to improve rollup security","dc:creator":"peshwar9","category":"Layer 2","description":"<p><strong>TLDR</strong></p>\n<p>L2 neworks such as rollups, validiums and optimiums primarily use execution proofs (ZK proofs or fraud proofs) to prove (or disprove) transaction and state transition validity to the base layer (Ethereum). While proofs of execution are important, they do not detect many categories of faults and frauds that can happen in the various components that comprise  the overall layer-2 infrastructure. Such faults/frauds are  difficult to prove and impossible to attribute and penalise on-chain.</p>\n<p>This post introduces integrity proofs, which complements zk proofs and fraud proofs for rollup security. Integrity proof  is a mechanism to attest to the integrity of services running in a particular instance of rollup, providing assurance to the end users that no malicious or other unauthorized code changes have been performed in the production environment. Integrity proofs use TEEs to deploy and monitor rollup services and report any breach of service integrity on a new layer-2 network (let’s call it Integrity verification chain) that provides decentralised security services to rollups.</p>\n<p>Layering integrity proofs over existing execution proofs greatly improves the censorship resistance of the rollup, detects unauthorised MEV extraction and offers protection from other types of malicious activities in the rollup infrastructure from privileged or unauthorized users. This design also reduces the trust assumptions , compared to the current architecture of centralised rollup deployments that rely disproportionately on execution proofs to demonstrate offchain execution validity.</p>\n<p><strong>Introduction</strong></p>\n<p>Layer-2 networks settling on Ethereum (aka base layer) predominantly use either fraud proofs (proofs of invalidity) or ZK-proofs (proofs of validity) to prove transaction validity and updated state commitments to the base layer. Both these are execution proofs that cryptographically attest to those activities that are objectively recorded and verifiable on-chain (aka running in the EVM or equivalent VM).</p>\n<p>However, such execution proofs alone are inadequate to verify the security of the overall layer-2 network. This is because there are many centralised components in L2s where faults and malicious activities can occur, which cannot be proven on-chain (non-attributable faults). There are many solutions being worked on to address centralisation risk in rollups, but they come with their own tradeoffs including loss of sovereignty/control, reducing utility of own token, cost increase or increased complexity of architecture.</p>\n<p>This post proposes a solution to detect <em>unattributable faults</em> while running rollup services (of the kind that cannot be detected by ZK proofs alone) , without any loss of sovereignty or loss of control for the project deploying the layer-2 network. (Note: The terms layer-2 network and rollup are used interchangeably in this post as a broad term to encompass rollups, validiums and optimums.)</p>\n<p>Integrity proofs are proposed as a mechanism to certify to the presence (or absence) of unattributable faults in the operation of a rollup service, by having an observer running in a Trusted Execution Environment (TEE) provide cryptographic attestation of its own execution environment and signing the attestation with its own in-enclave generated cryptographic identity (signing key) that cannot be tampered with even by the operator of the attestation service.</p>\n<p><strong>Problem space</strong></p>\n<p>Layer-2 networks (both optimistic and Zk stack chains) have many components such as rpc nodes, sequencers, pool &amp; state databases, executors, provers, L1 transaction managers etc that are basically deployed either as platform-native binaries or docker images. Examples of non-attributable faults/frauds include censoring transactions based on addresses or geo-fencing access in json-rpc node, changing the ordering algorithm of sequencer, making unauthorised/undetected changes to the state or pool database, and withholding transaction batch data posted to external DA layer. Currently, users make heavy trust assumptions while using L2 chains that such malicious or unauthorised actions don’t take place in the centralised services operated by their provider. More specifically, users make the following trust assumptions:</p>\n<p>a) they trust the rollup project foundations (eg Polygon, ZK, Optimism, Arbitrum) to not introduce any malicious code on the published rollup service images.</p>\n<p>b) trust that the right security controls are in place such that employees/contractors that belong to the rollup project or RaaS operators have restricted privileged access (or that those who do , do not abuse privileged access)</p>\n<p>In short, users rely on the security offered by the reputational credibility (social &amp; commercial ) of these trusted third parties. Integrity of the rollup services operated by these parties is not verifiable, and any faults are not attributable or provable on-chain. This goes against the basic premise of blockchain networks aka verifiability.</p>\n<p><strong>Solution proposed</strong></p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/e/d/edb7cc61ad8bf9c863c1a9d1deeef28720e7a109.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/edb7cc61ad8bf9c863c1a9d1deeef28720e7a109\" title=\"Integrity Verified Services\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/e/d/edb7cc61ad8bf9c863c1a9d1deeef28720e7a109_2_671x500.jpeg\" alt=\"Integrity Verified Services\" data-base62-sha1=\"xUWWH1Wf1Mo6qGiYtnUbW6g5cYV\" width=\"671\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/e/d/edb7cc61ad8bf9c863c1a9d1deeef28720e7a109_2_671x500.jpeg, https://ethresear.ch/uploads/default/optimized/3X/e/d/edb7cc61ad8bf9c863c1a9d1deeef28720e7a109_2_1006x750.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/3X/e/d/edb7cc61ad8bf9c863c1a9d1deeef28720e7a109_2_1342x1000.jpeg 2x\" data-dominant-color=\"ECEDEA\"></a></div><p></p>\n<p>Figure shows the overview of the proposed solution, which has a new layer-2 network acting as the integrity verification chain (let’s call it IVC in short), and three types of actors participating in it - rollup framework providers, rollup projects and TEE operators. Let’s understand this better using the example of a single rollup service- the sequencer (which needs no introduction).</p>\n<ol>\n<li>Rollup framework providers (eg polygon cdk, Zkstack, OP stack , Arbitrum etc) register the service image of their latest release of sequencer with IVC.</li>\n<li>A new rollup project comes along and wants to deploy the sequencer from one of these frameworks. The request is sent to a <em>Host program</em> (creatively named) that runs in a TEE. The host program retrieves the service image registered by the provider, and deploys it on a VM that’s specified by the rollup provider. (The only requirement is for the host program to be given SSH access to deploy the service in the VM). For simplicity, let’s assume this service image is a docker image. The host program downloads the docker image and instantiates a new container using this image. The container Id is captured by the host program and recorded on IVC smart contract designated for the purpose.</li>\n<li>Periodically (based on frequency specified by the rollup project owner) the host program verifies if the container id of the running sequencer matches that recorded on the IVC layer-2 smart contract. If there is any discrepancy, it flags it as a violation of integrity on the integrity verification chain (IVC).</li>\n<li>The host program can generate an integrity proof on demand (or it can be generated unconditionally in the protocol workflow) attesting to the integrity of a particular service that is deployed and monitored by it. The integrity proof would contain the details of the rollup service that’s being monitored, proof of the execution environment in which the host program runs including the quote/remote attestation parameters and its own binary fingerprint (MRENCLAVE), Further, the integrity proof generated by the host program can be signed by a keypair that’s generated within the enclave so that the operator of the host program VM cannot make a false attestation. The signing key would not be accessible even to the operator.</li>\n</ol>\n<p>Here, the sequencer component that was used as the example, becomes the Integrity-verified service (IVS). Technically, the sequencer itself can be run inside a TEE, but that would be another solution not discussed here.</p>\n<p><strong>How does integrity verified service offer a better security model for rollups?</strong></p>\n<p>In this example, let’s assume that after the rollup service is deployed , someone with privileged or unauthorised access modifies the sequencer code to add censoring logic or ordering algorithm, and restarts the docker container. This will result in a change to the container finggerprint, and this will  immediately be detected by the host program (as the running container id will not match the registered container id that was started by the host program). The integrity proof from the host program can certify to the presence or absence of such non-attributable faults.</p>\n<p>When integrity proofs are combined with prevailing proof mechanisms such as zk validity proofs or fraud proofs, it strictly improves the status quo in terms of rollup security.</p>\n<p><strong>Benefits of this solution</strong></p>\n<p><em>For end-user:</em> The end user of the rollup/L2 chain is now assured that in addition to their on-chain transactions being secure (due to natively built-in zk proofs or fraud proofs), additionally any non-attributable faults that can occur in sequencer such as censoring or ordering algorithm changes will immediately be detected and flagged for attention. The user can verify integrity proofs in a permission-less manner.</p>\n<p><em>For rollup project owner:</em> They don’t just have to trust their own devops teams or that of the RaaS operator that they outsource to, but they can verify the integrity of the rollup services deployed permissionlessly, on an ongoing basis. They get a higher security layer-2 chain (trust-minimised) compared to what they would get by deploying a standard rollup stack out of the box.</p>\n<p><em>For RaaS operators</em>: They have lower operational risk by ensuring that the right service images are deployed everytime. This also results in lower risk of reputational/business loss due to unauthorised access or security attacks on their infra.</p>\n<p><em>For rollup framework providers</em> : By publishing their service images and actively participating in the integrity verification activity, they attract better quality of builders and projects on their ecosystem.</p>\n<p><strong>Further security enhancements</strong></p>\n<p>The integrity proofs described solve two attack vectors on rollup services:</p>\n<ol>\n<li>Any unauthorised changes to the deployed rollup service is detected and flagged.</li>\n<li>The correct secure version of each rollup service is deployed in the production environment</li>\n</ol>\n<p>This is a clear improvement from having just execution proofs. But there can be other types of attacks that need to be designed for. One example is that someone with privileged access can still log into the running docker container and change environment variables. To address this, the host program can be enhanced to periodically log into the running containers and compare the environment parameters on the current running version of container vs the original deployed version.</p>\n<p>There can be other types of attacks that are more difficult to detect. for example a sophisticated actor can log into the docker container and manipulate memory . Memory encryption techniques can be evaluated to protect against such attacks. Other solutions could involve running multiple instances of a rollup service (eg sequencer or prover) governed by a consensus algorithm, but it increases costs of operating the rollup infrastructure, and it will be the decision of the rollup project to evaluate the level of risk they are willing to accept and the cost they are willing to pay.</p>\n<p><strong>Summary</strong></p>\n<p>The premise of this solution is that a combination of existing <em>execution proof techniques</em> (zk or fraud proofs) for objectively-verifiable faults in combination with Tee-based <em>integrity proofs</em> for non-attributable faults offers a superior trust-minimised security model for rollups, compared to the status quo.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/integrity-proofs-to-improve-rollup-security/19437\">Read full topic</a></p>","link":"https://ethresear.ch/t/integrity-proofs-to-improve-rollup-security/19437","pubDate":"Thu, 02 May 2024 13:00:07 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19437"},"source":{"@url":"https://ethresear.ch/t/integrity-proofs-to-improve-rollup-security/19437.rss","#text":"Integrity proofs to improve rollup security"},"filter":false},{"title":"Voter Behavior in Blockchain Governance: A Comparative Study of Curve Finance and Polkadot","dc:creator":"dengwx11","category":"Economics","description":"<p>by <a href=\"https://twitter.com/dengwx11\" rel=\"noopener nofollow ugc\">Wenxuan Deng</a>, <a href=\"https://www.linkedin.com/in/tanishakatara/\" rel=\"noopener nofollow ugc\">Tanisha Katara</a>, <a href=\"https://www.linkedin.com/in/davidhamoui/\" rel=\"noopener nofollow ugc\">David Hamoui</a> and <a href=\"https://twitter.com/matrzeszowski\" rel=\"noopener nofollow ugc\">Mateusz Rzeszowski</a></p>\n<p><strong>Acknowledgments</strong><br>\nAdditional thanks to <a href=\"https://twitter.com/p_petertherock\" rel=\"noopener nofollow ugc\">Peter Liem</a> for his assistance with data fetching.</p>\n<p>Also blogged here: <a href=\"https://polygon-governance.mirror.xyz/_BeDt5MvxFQ26ElodFtzHoLi725EFLp_IlliUYtg_HA\" rel=\"noopener nofollow ugc\">polygon mirror</a>; <a href=\"https://hackmd.io/QkqNJuSjQKSolPWfcEnceA?view\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Voter Behavior in Blockchain Governance: A Comparative Study of Curve Finance and Polkadot - HackMD</a></p>\n<h2><a name=\"abstract-1\" class=\"anchor\" href=\"https://ethresear.ch#abstract-1\"></a>Abstract</h2>\n<p>As the first comprehensive examination of voter behavior in Web3, the following research explores two significant blockchain ecosystems, Curve Finance and Polkadot, using a novel quantitative methodology to decompose and highlight governance patterns.</p>\n<p>The presented analysis shows, among other findings, a significant influence of market conditions on voter tendencies, diverse patterns relating to voting power accumulation, and a potential effect of financial incentives on voter participation.</p>\n<p>As such, this research seeks to provide value to the growing field of blockchain-based governance frameworks in dealing with fundamental issues, such as low stakeholder activity, the impact of shifting market conditions, and various complexities at the intersection of economics and political science.</p>\n<h2><a name=\"h-1-introduction-2\" class=\"anchor\" href=\"https://ethresear.ch#h-1-introduction-2\"></a>1. Introduction</h2>\n<p>Blockchain enables decentralized decision making[1]. The information once hidden is now displayed publicly in any block explorer[2]; what used to be the domain of a few now can be accessed permissionlessly[3]; what was perceived as status quo is now challenged with every executed transaction[4].</p>\n<p>These qualities are revolutionary, but the decision-making of distributed networks is largely undocumented still. Consequently, this paper serves as an attempt to analyze and compare two blockchain-based governance systems quantitatively: Curve Finance[5] and Polkadot[6].</p>\n<p>Firstly, in order to lay a common ground of understanding, we describe the relevant governance systems. Secondly, we explain the motivation for selecting Curve Finance and Polkadot as part of our analysis. Lastly, in each of the systems, we analytically deep dive into the voter personas, voter turnout, proposals and voter behavior in different market conditions.</p>\n<h3><a name=\"h-11-curve-finance-and-polkadot-3\" class=\"anchor\" href=\"https://ethresear.ch#h-11-curve-finance-and-polkadot-3\"></a>1.1 Curve Finance and Polkadot</h3>\n<p>As the fields of tokenomics[7] and governance rapidly evolve, two protocols have emerged as particularly influential. Curve Finance pioneered[8] the vote escrow token model[9], introducing gauge voting[10] to decentralized finance (DeFi)[11]. Meanwhile, Polkadot’s governance system has been notable[12] for its innovative approach to quorums[13] and governance lockers[14]. These protocols consequently stand out as significant pioneers in decentralized governance innovation, including novel concepts such as governance conviction[15].</p>\n<p>Governance conviction, also referred to as lockup-based voting multipliers or vote escrow[16], is a mechanism used in decentralized governance models[17] to enhance the influence or voting power of token holders based on the duration for which they are willing to lock up their tokens in a smart contract. This type of system operates under the assumption that the longer a participant commits their tokens for, the more conviction they demonstrate towards the decisions being made within the network and the more incentive alignment they display. As a consequence of the process, their voting power is multiplied by a factor corresponding to the lockup period. This aims to incentivize longer-term commitment and stability within the governance process, aligning participants’ interests with the long-term health and success of the platform.</p>\n<p>Let’s now briefly go over both projects and how their governance works in practice.</p>\n<p><strong>1.1.1 Curve Finance</strong>[18] is a protocol allowing for seamless exchange of ERC-20 tokens at a low cost. This is achieved through the use of liquidity pools[19], which are tokens locked in smart contracts to facilitate trades. Curve Finance offers rewards[20] to those who contribute, creating a mutually beneficial relationship where users can easily exchange tokens while liquidity providers receive rewards.</p>\n<p>To vote, CRV token holders must possess veCRV. veCRV[21] represents CRV tokens that are locked for a certain period (<a href=\"https://ethresear.ch#table1\">Table 1</a>). Users can lock their CRV for a minimum of  one week and a maximum of four years.[22]</p>\n<p><strong>1.1.1.1 Governance Proposals</strong> - To distinct proposals were noted: gauge proposals and non-gauge proposals. Gauges and gauge weights determine how much token rewards the suppliers to a liquidity pool get. Therefore, a gauge proposal will have explicit financial consequences for some or all token holders. Non-gauge proposals, on the other hand, may or may not have financial consequences and may pertain to high-level maintenance and regular upgrades in the network. The impact of proposal types on voter behavior will be discussed more in this research paper.</p>\n<p><strong>1.1.1.2 Community Voting</strong> - A proposer must have a minimum voting power of 2500 vote-escrowed CRV (veCRV)[23] to create a proposal. A voter’s voting power linearly decreases overtime until it reaches zero at the time of the unlock.</p>\n<p><strong>1.1.2 Polkadot</strong>[24] is a protocol which uses a nominated Proof of Stake (PoS)[25] consensus algorithm and aims to connect blockchains, also known as parachains[26], to enable seamless communication, and intermediary-free communication within its network. Fundamentally, parachains are independent PoS blockchains with their own functionalities and tokens. What binds these parachains is the relay chain[27], which is responsible for achieving consensus and ensuring that transactions are executed.</p>\n<p>Note: This study analyzes data from Polkadot’s Governance V1[6], rather than the more recent Open Governance system[28]. This choice was made to ensure comparability in terms of the data size, data type and temporal alignment of the voter samples with those from Curve Finance.</p>\n<p><strong>1.1.2.1 Governance Proposals</strong> - Similarly to Curve Finance, two types of proposals can be distinguished in Polkadot: treasury proposals[29] and non-treasury proposals. According to Polkadot’s Governance V1, when a stakeholder wishes to propose spending from the treasury, they must reserve a deposit of at least 5% of the proposed spending[29]. A treasury proposal will have explicit financial consequences for the protocol, subject to governance, while non-treasury proposals may or may not have financial consequences and pertain to high-level maintenance and regular upgrades in the network. The manner in how the proposal types impact voter behavior will be tackled in the  “Governance Proposals Breakdown” section of this research paper.</p>\n<p><strong>1.1.2.2 Community Voting</strong> - To vote on proposals, DOT token holders must lock their tokens. The longer the DOT is locked, the more voting power is assigned. The voting power of a DOT holder in Polkadot is calculated as DOT tokens held multiplied by the relevant multiplier (<a href=\"https://ethresear.ch#table2\">Table 2</a>), which increases as the locking period increases. The multipliers range from 0.1x for zero days to 6x for two hundred and twenty-four days.[30]</p>\n<p>For instance, if Bobby has 100 tokens and locks them for 14 days, her/his voting power per the (<a href=\"https://ethresear.ch#table2\">Table 2</a>) is (Dot Tokens Held) * 2 (14-day multiplier) = 200. Therefore, Bobby will have the voting power of 200 tokens.</p>\n<h3><a name=\"h-12-curve-finance-and-polkadot-case-selection-4\" class=\"anchor\" href=\"https://ethresear.ch#h-12-curve-finance-and-polkadot-case-selection-4\"></a>1.2 Curve Finance and Polkadot Case Selection</h3>\n<p>Curve Finance and Polkadot serve as significant examples of decentralized governance and are distinct in their purposes and functionalities. At the same time, due to the need for upgradeable contracts and changeable system parameters[30][31], both projects incorporate governance structures, delegating system maintenance responsibilities to their respective communities, as proxied by token in order to facilitate for secure and decentralized decision-making. Both of those governance systems utilize permissionless on-chain execution, which means that the decision-making is directly translated into canonical code, without the need for an external authority or intermediary to apply the changes that tokenholders vote on.</p>\n<p>Importantly, in both Polkadot’s and Curve’s cases, we can distinguish between proposals with explicit and direct financial consequences (financial proposals) and those where the financial consequence is implicit and delayed (technical proposals). Curve’s gauge proposals and Polkadot’s treasury proposals can be both considered financial proposals since they explicitly impact the allocation of financial resources shared by all tokenholders, i.e., as it concerns deciding reward structures for pools in Curve, and the distribution of Polkadot’s treasury funds. On the other hand, technical proposals, i.e., non-gauge and non-treasury, impact the protocols, relate to maintenance and upgrades, and may not directly affect the allocation of financial resources shared by tokenholders.</p>\n<p>There is one notable difference between the two governance systems in terms of the specifics of their token lock-up mechanisms. Curve Finance employs a governance mechanism that encourages participants to actively engage in decision-making processes by allowing them to gain financial rewards[20] for locking their tokens. On the other hand, Polkadot doesn’t financially incentivize the locking up of tokens for voting (Democracy locker).</p>\n<p>The above makes the case for a comparative study of the two systems. In the following parts, we analyze voter turnouts (<a href=\"https://ethresear.ch#2\">Section 2</a>) and break down governance proposals by type (<a href=\"https://ethresear.ch#3\">Section 3</a>). Then, we establish voter personas and seek to understand them in each system (<a href=\"https://ethresear.ch#4\">Section 4</a>). Finally, we analyze voter behavior and seek to identify the governing principles and patterns of these complex environments (<a href=\"https://ethresear.ch#5\">Section 5</a>). To do so, we break down the types of market conditions and consequent variations in voter behavior.</p>\n<h2><a name=\"h-2-voter-turnout-analysis-5\" class=\"anchor\" href=\"https://ethresear.ch#h-2-voter-turnout-analysis-5\"></a>2. Voter Turnout Analysis</h2>\n<p>The below data comes from a comprehensive analysis on voter turnout for governance proposals. The calculation of voter turnout metrics is based on the number of veCRV tokens and DOT tokens used for voting over time, relative to the total veCRV and DOT tokens locked over the same period.</p>\n<p><strong>2.1 Curve Finance:</strong> On average, 65% of the circulating CRV is locked as veCRV. The below graph shows the monthly average number of veCRV tokens used for voting on gauge proposals and non gauge proposals over time. The red line shows the monthly average CRV locked as veCRV over time.</p>\n<p>While there is no significant difference in the veCRV used for voting on gauge proposals and for non gauge proposals, it is interesting to note that out of the 65% locked, an average of 38% tokens have been used for voting. This highlights that although a significant proportion of CRV is locked, a relatively low percentage is used for voting. Further investigation is required to determine the exact factors contributing to the low percentage of utilized tokens.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/b/5/b5ca495402eba76079ef1ae2e3a6201676e86217.png\" data-download-href=\"https://ethresear.ch/uploads/default/b5ca495402eba76079ef1ae2e3a6201676e86217\" title=\"crv_turnout\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/b/5/b5ca495402eba76079ef1ae2e3a6201676e86217_2_690x202.png\" alt=\"crv_turnout\" data-base62-sha1=\"pWbOLDZ8msAc5Pmw4oxnxOmgf1Z\" width=\"690\" height=\"202\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/b/5/b5ca495402eba76079ef1ae2e3a6201676e86217_2_690x202.png, https://ethresear.ch/uploads/default/optimized/3X/b/5/b5ca495402eba76079ef1ae2e3a6201676e86217_2_1035x303.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/b/5/b5ca495402eba76079ef1ae2e3a6201676e86217_2_1380x404.png 2x\" data-dominant-color=\"E3E5ED\"></a></div><p></p>\n<p><strong>2.2 Polkadot:</strong> On average, 54.5% of the circulating DOT is locked into multiple Polkadot lockers. The below graph shows the monthly average number of locked DOT tokens used for voting on treasury proposals and non treasury proposals over time. The red line shows the monthly average amount of DOT locked over time. In stark contrast to Curve Finance, out of these locked tokens, only 0.11% has been used for voting.</p>\n<p>This highlights a significant disparity in voter engagement between the two blockchain ecosystems, with DOT showing a much lower level of voter engagement than CRV. While no direct  connection can be drawn, the low percentage of utilized tokens for voting in DOT could be attributed to several factors, such as a lack of financial incentives dedicated to token lockups.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/0/9/09bf469fb6b10415f3bbdd8919dab25c4e91905b.png\" data-download-href=\"https://ethresear.ch/uploads/default/09bf469fb6b10415f3bbdd8919dab25c4e91905b\" title=\"dot_turnout\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/0/9/09bf469fb6b10415f3bbdd8919dab25c4e91905b_2_690x204.png\" alt=\"dot_turnout\" data-base62-sha1=\"1oe6rpwz4LAHXxARiguZ9L3pc7N\" width=\"690\" height=\"204\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/0/9/09bf469fb6b10415f3bbdd8919dab25c4e91905b_2_690x204.png, https://ethresear.ch/uploads/default/optimized/3X/0/9/09bf469fb6b10415f3bbdd8919dab25c4e91905b_2_1035x306.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/0/9/09bf469fb6b10415f3bbdd8919dab25c4e91905b_2_1380x408.png 2x\" data-dominant-color=\"C4D3E4\"></a></div><p></p>\n<p>The voter turnout metrics shed light on the extent to which token holders actively engage in voting in both the ecosystems. This sets the stage for a deeper exploration into the nature of governance proposals and their implications on decision-making dynamics within the Curve Finance and Polkadot communities. By examining the types and frequencies of proposals submitted in governance, we can gain further insights into the priorities and interests driving the respective ecosystems’ governance mechanisms.</p>\n<h2><a name=\"h-3-governance-proposals-breakdown-6\" class=\"anchor\" href=\"https://ethresear.ch#h-3-governance-proposals-breakdown-6\"></a>3. Governance Proposals Breakdown</h2>\n<p>Governance proposals play a crucial role in decision-making within any community. Our analysis of two ecosystems, Curve Finance and Polkadot, reveals that financial proposals make up a significant portion of all proposals. Gauge proposals constitute approximately 70% of all proposals in Curve Finance, while treasury proposals make up approximately 80% of all proposals in Polkadot.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/9/f/9f65fd9cc73df566bd1b3796a96d61ae737a365a.png\" data-download-href=\"https://ethresear.ch/uploads/default/9f65fd9cc73df566bd1b3796a96d61ae737a365a\" title=\"crv_cumu_prop_num\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/9/f/9f65fd9cc73df566bd1b3796a96d61ae737a365a_2_690x406.png\" alt=\"crv_cumu_prop_num\" data-base62-sha1=\"mK6qLSN9nAtnKnzKiANAnSYEOtc\" width=\"690\" height=\"406\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/9/f/9f65fd9cc73df566bd1b3796a96d61ae737a365a_2_690x406.png, https://ethresear.ch/uploads/default/original/3X/9/f/9f65fd9cc73df566bd1b3796a96d61ae737a365a.png 1.5x, https://ethresear.ch/uploads/default/original/3X/9/f/9f65fd9cc73df566bd1b3796a96d61ae737a365a.png 2x\" data-dominant-color=\"E2E7F1\"></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/3/5/35b67cad4c367dc4f4c3268906521b452a2cc5c7.png\" data-download-href=\"https://ethresear.ch/uploads/default/35b67cad4c367dc4f4c3268906521b452a2cc5c7\" title=\"dot_cumu_ref_num\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/3/5/35b67cad4c367dc4f4c3268906521b452a2cc5c7_2_690x318.png\" alt=\"dot_cumu_ref_num\" data-base62-sha1=\"7Fahwce4tUT0a2cMd6f3DU7yTpZ\" width=\"690\" height=\"318\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/3/5/35b67cad4c367dc4f4c3268906521b452a2cc5c7_2_690x318.png, https://ethresear.ch/uploads/default/optimized/3X/3/5/35b67cad4c367dc4f4c3268906521b452a2cc5c7_2_1035x477.png 1.5x, https://ethresear.ch/uploads/default/original/3X/3/5/35b67cad4c367dc4f4c3268906521b452a2cc5c7.png 2x\" data-dominant-color=\"E0E3E6\"></a></div><p></p>\n<p>Interestingly, in Curve Finance, we found that most proposals are initiated by individuals associated with the <a href=\"http://Curve.fi\" rel=\"noopener nofollow ugc\">Curve.fi</a> team (<a href=\"https://ethresear.ch#table3\">Table 3</a>), particularly two wallets linked to its founder, Michael Egorov[32]. The top wallet, identified as the <a href=\"http://Curve.fi\" rel=\"noopener nofollow ugc\">Curve.fi</a> deployer on the Arkham data platform[33], is notably active in both gauge and non-gauge proposal submissions. See the appendix for (<a href=\"https://ethresear.ch#table3\">Table 3</a>) of top Curve Finance proposers, ranked in order of participation.</p>\n<p>No similar pattern could be observed in Polkadot with the most referenda submitted by a single author amounting to only 6% of the total proposals.</p>\n<p>Having provided an analysis of various proposals and consequent patterns, we move into the voter-centric part of the paper.</p>\n<h2><a name=\"h-4-voter-personas-and-respective-patterns-7\" class=\"anchor\" href=\"https://ethresear.ch#h-4-voter-personas-and-respective-patterns-7\"></a>4. Voter Personas and Respective Patterns</h2>\n<p>The following research categorizes voter persona and analyzes their behavior towards the governance systems.</p>\n<p>Voter personas are categorized based on the size of their token holdings, and the hierarchy is defined as follows: the top 1% is labeled as Whales, the next 5% as Sharks, the next 10% as Dolphins, the next 20% as Fish, and the remaining 64% as Shrimps.</p>\n<div> \n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>Token Holdings</th>\n<th>Voter Persona</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Top 1%</td>\n<td>Whales</td>\n</tr>\n<tr>\n<td>5%</td>\n<td>Sharks</td>\n</tr>\n<tr>\n<td>10%</td>\n<td>Dolphins</td>\n</tr>\n<tr>\n<td>20%</td>\n<td>Fish</td>\n</tr>\n<tr>\n<td>Remaining 64%</td>\n<td>Shrimps</td>\n</tr>\n</tbody>\n</table>\n</div></div>\n<p><strong>4.1 Curve Finance:</strong> In the context of Curve Finance governance, over 58% of token holders choose to lock their tokens for the longest duration, i.e. four years. However, an intriguing trend surfaces among the different cohorts of holders.</p>\n<p>In the graph below, the x-axis displays the initial lock-up windows, which range from 7 days to 4 years, and the y-axis displays the percentage of each of the voter personas who have locked their tokens. The more significant holders, i.e., Whales, Sharks, and Dolphins, show mild hesitation to commit for more extended lock periods. Conversely, they slightly prefer shorter commitments, particularly those under six months. Although the margin is slim, it is a telling divergence.</p>\n<p>It hints that larger holders may not need to lock up their tokens for extended periods to wield significant voting power. For them, the flexibility not to lock in for an extended time could be a strategic move to mitigate risk and maintain liquidity options.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/b/1/b1c30bac016ddab4cd9accb8232ffc6b72a406aa.png\" data-download-href=\"https://ethresear.ch/uploads/default/b1c30bac016ddab4cd9accb8232ffc6b72a406aa\" title=\"crv_shark_and_shrimp_dotplot\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/b/1/b1c30bac016ddab4cd9accb8232ffc6b72a406aa_2_375x500.png\" alt=\"crv_shark_and_shrimp_dotplot\" data-base62-sha1=\"pmyowoUE7tPajUHQoovpOpvP6KS\" width=\"375\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/b/1/b1c30bac016ddab4cd9accb8232ffc6b72a406aa_2_375x500.png, https://ethresear.ch/uploads/default/optimized/3X/b/1/b1c30bac016ddab4cd9accb8232ffc6b72a406aa_2_562x750.png 1.5x, https://ethresear.ch/uploads/default/original/3X/b/1/b1c30bac016ddab4cd9accb8232ffc6b72a406aa.png 2x\" data-dominant-color=\"EDF1F8\"></a></div><p></p>\n<p><strong>4.2 Polkadot:</strong> In the Polkadot ecosystem, analysis yields a similar, yet valuable, correlation to Curve Finance. 4% of DOT holders initially choose to lock their tokens for a maximum of 224 days (roughly seven months). Holders with more significant positions prefer shorter lock-up periods, and this pattern is glaringly apparent. Particularly striking in the Polkadot ecosystem is that about 93% of Whales and 98% of Sharks tend to lock up their tokens for 14 days or less. Contrastingly, shrimp holders display markedly different behavior, with approximately 30% opting for an 8-week lock-up and about 5% committing to a 32-week lock-up.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/9/5/957fc95c9dade9db90e4b1116fe2e631d9f3b6a7.png\" data-download-href=\"https://ethresear.ch/uploads/default/957fc95c9dade9db90e4b1116fe2e631d9f3b6a7\" title=\"dot_shark_and_shrimp_dotplot\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/9/5/957fc95c9dade9db90e4b1116fe2e631d9f3b6a7_2_375x500.png\" alt=\"dot_shark_and_shrimp_dotplot\" data-base62-sha1=\"lkwVk0mVvjhwex7Miyvi130Eo6P\" width=\"375\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/9/5/957fc95c9dade9db90e4b1116fe2e631d9f3b6a7_2_375x500.png, https://ethresear.ch/uploads/default/optimized/3X/9/5/957fc95c9dade9db90e4b1116fe2e631d9f3b6a7_2_562x750.png 1.5x, https://ethresear.ch/uploads/default/original/3X/9/5/957fc95c9dade9db90e4b1116fe2e631d9f3b6a7.png 2x\" data-dominant-color=\"EDF1F8\"></a></div><p></p>\n<p>Among different holder categories, from Whales to Shrimps, there is an incremental increase in the preference for longer lock-ups as we move down the scale of holdings. The distinctions and preferences between prominent and smallholders are consistent within each protocol.</p>\n<p>However, there is a significant difference between the two protocols. As shown in the pie charts below, in Curve Finance, 67.2% of voters across all groups opt for a four-year lock-up, while in Polkadot, only 4% of the users choose the most extended lock-up window of 224 days. Even among the most minor stakeholders, the Shrimps, less than 5% chose the most prolonged lock-up period of 32 weeks.</p>\n<p>This divergence could be attributed to the fundamental differences in the underlying rewards and incentives of Curve Finance and Polkadot. Curve’s gauge weight voting system[34] incentivizes users to boost their voting power by locking their tokens for extended periods. This may indicate that sustained rewards are crucial in incentivizing token holders to stay longer.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/a/d/ad4e411543cfef1a3570feaedf97c885a91ad181.png\" data-download-href=\"https://ethresear.ch/uploads/default/ad4e411543cfef1a3570feaedf97c885a91ad181\" title=\"crv_piechart\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/a/d/ad4e411543cfef1a3570feaedf97c885a91ad181_2_511x500.png\" alt=\"crv_piechart\" data-base62-sha1=\"oJ8gcs0uLvxMtiOgouWUcv9izAt\" width=\"511\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/a/d/ad4e411543cfef1a3570feaedf97c885a91ad181_2_511x500.png, https://ethresear.ch/uploads/default/original/3X/a/d/ad4e411543cfef1a3570feaedf97c885a91ad181.png 1.5x, https://ethresear.ch/uploads/default/original/3X/a/d/ad4e411543cfef1a3570feaedf97c885a91ad181.png 2x\" data-dominant-color=\"EDD2D6\"></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/f/b/fb9c1553283566ce16f0c4076202d83ab22487a6.png\" data-download-href=\"https://ethresear.ch/uploads/default/fb9c1553283566ce16f0c4076202d83ab22487a6\" title=\"dot_piechart\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/f/b/fb9c1553283566ce16f0c4076202d83ab22487a6_2_497x500.png\" alt=\"dot_piechart\" data-base62-sha1=\"zTQfCJgGnIp1PfY7BYpgU2yM40S\" width=\"497\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/f/b/fb9c1553283566ce16f0c4076202d83ab22487a6_2_497x500.png, https://ethresear.ch/uploads/default/original/3X/f/b/fb9c1553283566ce16f0c4076202d83ab22487a6.png 1.5x, https://ethresear.ch/uploads/default/original/3X/f/b/fb9c1553283566ce16f0c4076202d83ab22487a6.png 2x\" data-dominant-color=\"F4F8EB\"></a></div><p></p>\n<p>With this understanding of voter personas and their lock-up behaviors, we now dive into how they accumulate voting power in different market conditions.</p>\n<h2><a name=\"h-5-voting-power-accumulation-patterns-in-different-market-conditions-8\" class=\"anchor\" href=\"https://ethresear.ch#h-5-voting-power-accumulation-patterns-in-different-market-conditions-8\"></a>5. Voting Power Accumulation Patterns in Different Market Conditions</h2>\n<p>The study of voting power accumulation patterns and the associated dynamics between voters and their locked-up tokens are of great importance in understanding how voters use their tokens in upward and downward market conditions.</p>\n<p>Fundamentally, the two primary ways in which voters can augment their voting power (VP) are by purchasing additional tokens and locking them up, or by extending their lock-up period for already-owned tokens, resulting in an increased multiplier. VP calculation is derived from the multiplication of token balance and a multiplier based on the lock-up period.</p>\n<p><strong>Voting Power (VP) = token balance * multiplier based on lockup time</strong></p>\n<p>However, analyzing the behavior of voters in response to changes in token prices and market conditions presents a significant challenge - it’s hard to determine whether voter behavior is influenced by changes in token locked amounts and lock-up duration or whether they are affected by the daily routines of the average Externally Owned Account (EOA) wallet.</p>\n<p>To overcome this challenge, a reliable quantitative methodology has been developed to simplify this complex analysis. The approach we present is based on a decomposition of the changes in voting power into its constituent factors. By quantifying the changes in voting power (<span class=\"math\">\\Delta vp</span>) over time, an analysis of the two constituent factors, changes in balance (<span class=\"math\">\\Delta b</span>) and changes in conviction (<span class=\"math\">\\Delta c</span>), can be made.</p>\n<p>The change in voting power between two consecutive time points, <span class=\"math\">t</span> and <span class=\"math\">t-1</span>, can be derived through arithmetic calculations as follows:</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/2/4/24c7a0daa9f98d910a6b49ded7cef9789868a70f.png\" data-download-href=\"https://ethresear.ch/uploads/default/24c7a0daa9f98d910a6b49ded7cef9789868a70f\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/original/3X/2/4/24c7a0daa9f98d910a6b49ded7cef9789868a70f.png\" alt=\"image\" data-base62-sha1=\"5fmTdKIkigVRL7BgBmp6ga6jjxZ\" width=\"690\" height=\"53\" data-dominant-color=\"FBFBFB\"></a></div><p></p>\n<p>This equation can be expanded as:<br>\n</p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/f/d/fd0c1ec82b641fef7cc14e5a229f8fece3897b0e.png\" data-download-href=\"https://ethresear.ch/uploads/default/fd0c1ec82b641fef7cc14e5a229f8fece3897b0e\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/original/3X/f/d/fd0c1ec82b641fef7cc14e5a229f8fece3897b0e.png\" alt=\"image\" data-base62-sha1=\"A6yLAlYY1z562dRnDl5jbtFYESa\" width=\"690\" height=\"94\" data-dominant-color=\"F7F7F7\"></a></div><p></p>\n<p>Here, <span class=\"math\">b(t)</span> and <span class=\"math\">c(t)</span> represent the balance and conviction at time <span class=\"math\">t</span>, respectively. The terms <span class=\"math\">\\Delta b</span> and <span class=\"math\">\\Delta c</span> denote the changes in balance and conviction from time <span class=\"math\">t-1</span> to <span class=\"math\">t</span>.</p>\n<p>For this study, each timestamp where a new transaction occurs on the blockchain is considered a discrete-time point. This approach captures the dynamic nature of voting power changes with high granularity. To represent the changes in voting power across different voters and time points, a matrix formulation is used. The matrix of changes in voting power (<span class=\"math\">\\Delta VP</span>) is defined as follows:</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/9/4/94595d89b00fa0170bf67b4c30629f6540180d37.png\" data-download-href=\"https://ethresear.ch/uploads/default/94595d89b00fa0170bf67b4c30629f6540180d37\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/original/3X/9/4/94595d89b00fa0170bf67b4c30629f6540180d37.png\" alt=\"image\" data-base62-sha1=\"lam840sS944ts4OIRBkzf5BAULB\" width=\"690\" height=\"48\" data-dominant-color=\"FCFCFC\"></a></div><p></p>\n<p>Here, <span class=\"math\">T+1</span> represents the total number of time points, while <span class=\"math\">W</span> indicates the number of voters. The changes in voting power can be represented using the following formula:</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/1/1/110d27df9e76e720b17c0f7043a488be63d81b53.png\" data-download-href=\"https://ethresear.ch/uploads/default/110d27df9e76e720b17c0f7043a488be63d81b53\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/original/3X/1/1/110d27df9e76e720b17c0f7043a488be63d81b53.png\" alt=\"image\" data-base62-sha1=\"2qQiN1b7Rf61Bzd85tLHTyScLU7\" width=\"690\" height=\"47\" data-dominant-color=\"F8F8F8\"></a></div><p></p>\n<p>The formula can be represented in matrix form as:</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/8/b/8be96fafed33f43a6b1d92ef7ac076c5c191602e.png\" data-download-href=\"https://ethresear.ch/uploads/default/8be96fafed33f43a6b1d92ef7ac076c5c191602e\" title=\"image\"><img src=\"https://ethresear.ch/uploads/default/original/3X/8/b/8be96fafed33f43a6b1d92ef7ac076c5c191602e.png\" alt=\"image\" data-base62-sha1=\"jXIvcJitvCzjLK6QcbwxdZeYK5w\" width=\"690\" height=\"280\" data-dominant-color=\"F8F8F8\"></a></div><p></p>\n<p>The matrix <span class=\"math\">\\Delta VP</span> represents the change in voting power for each voter at each timestamp, while <span class=\"math\">B</span> represents the balance of each voter at time <span class=\"math\">t</span>, <span class=\"math\">\\Delta C</span> represents the change in conviction between <span class=\"math\">t</span> and <span class=\"math\">t-1</span> for each voter, and <span class=\"math\">C(t-1)</span> represents the conviction of each voter at time <span class=\"math\">t-1</span>. The matrices <span class=\"math\">\\Delta B</span> and <span class=\"math\">\\Delta C</span> represent the changes in balance and conviction, respectively, for each voter between <span class=\"math\">t</span> and <span class=\"math\">t-1</span>. The symbol <span class=\"math\">\\odot</span> represents the element-wise multiplication of matrices.</p>\n<p>If <span class=\"math\">\\Delta vp_{t, i}</span> is non-zero, the voting power of voter <span class=\"math\">i</span>'s wallet has been altered at time point <span class=\"math\">t</span>. A non-zero change in VP due to a change in conviction occurs because <span class=\"math\">b \\cdot \\Delta c</span> is non-zero. Conversely, a change in balance results in a non-zero value because <span class=\"math\">c(t-1) \\cdot \\Delta b</span> is non-zero. Typically, these two terms hold non-zero values simultaneously if the user changes the lock-up window and balance in the same transaction.</p>\n<p>The crux of the analysis then becomes determining whether the term <span class=\"math\">B \\odot C</span> or <span class=\"math\">C(t-1) \\odot \\Delta B</span> is more dominant in influencing <span class=\"math\">\\Delta VP</span>. To this end, we define two metrics:</p>\n<ol>\n<li><strong>Balance Impact:</strong> This is quantified as the L1 norm of <span class=\"math\">|C(t-1) \\odot \\Delta B|_1</span>.</li>\n<li><strong>Conviction Impact:</strong> This is quantified as the L1 norm of <span class=\"math\">|B \\odot \\Delta C|_1</span>.</li>\n</ol>\n<p>Here, the notation <span class=\"math\">||_1</span> denotes the L1 norm, which essentially sums up the absolute values of all elements in the matrix.</p>\n<p>In summary, this methodology for governance conviction provides a more in-depth and comprehensive approach to analyzing voting power accumulation patterns in different market conditions. Through the matrix formulation, the changes in voting power across different voters and time points can be represented and analyzed with high granularity, providing valuable insights into voter behavior dynamics.</p>\n<h3><a name=\"h-51-accounting-for-market-conditions-in-curve-and-polkadot-9\" class=\"anchor\" href=\"https://ethresear.ch#h-51-accounting-for-market-conditions-in-curve-and-polkadot-9\"></a>5.1 Accounting for Market Conditions in Curve and Polkadot</h3>\n<p>In this study, we aim to assess the impact of market conditions on how voters accumulate voting power. To achieve this, we analyze upward and downward trends in the market by employing a combination of short-term and long-term moving averages, namely the 7-day moving average (MA7) and the 30-day moving average (MA30). We define an upward trend when the MA7 exceeds the MA30 and a downward trend when the MA7 falls below the MA30.</p>\n<p>It is crucial to acknowledge that token behavior is circumstantial, and varying market conditions may elicit different responses from holders with varying stakes, thereby exhibiting diverse behavior patterns. Therefore, we adopt a nuanced approach, which considers these factors to provide a precise understanding of how balance and conviction impact the ebb and flow of voting power within the Curve Finance and Polkadot governance systems.</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/3X/0/7/07a995daf9cb9eb9aadcc576d83b39e7257db50b.png\" alt=\"crv_price\" data-base62-sha1=\"15MFZslEgCkEObJIgPqvAnY2oP1\" width=\"635\" height=\"387\"><br>\n<img src=\"https://ethresear.ch/uploads/default/original/3X/c/a/ca393687212f545f9764f88da9c2ea9b776a93c2.png\" alt=\"dot_price\" data-base62-sha1=\"sQX1H0jGUGKipwwBUjbPTXSA3aG\" width=\"650\" height=\"388\"></p>\n<p>The charts presented above illustrate how the MA7-MA30 differential correlates with the token price. Our analysis leverages these definitions to explore how market trends affect Shrimp voter behavior, specifically the influence of the token price and lock-up duration on the dynamic voting power within the governance frameworks of Curve Finance and Polkadot. Since the sample size for Whales, Sharks, Dolphins and Fish is insignificant, we choose to exclusively analyze the Shrimp voter group. Having said that, it’s useful to note that for all the aforementioned groups, similar tendencies can be distinguished to the ones presented below.</p>\n<h3><a name=\"h-52-findings-voting-power-accumulation-in-curve-finance-and-polkadot-10\" class=\"anchor\" href=\"https://ethresear.ch#h-52-findings-voting-power-accumulation-in-curve-finance-and-polkadot-10\"></a>5.2 Findings: Voting Power Accumulation in Curve Finance and Polkadot</h3>\n<p><strong>5.2.1 Curve Finance:</strong> In the case of Curve Finance, we encountered a challenge when studying Shrimp voters due to their number exceeding 12,000 and the high computational complexity of our method. As a result, we adopted a sampling strategy, randomly selecting 2000 Shrimp voters in each experiment, and repeated this process 500 times. We calculated the log ratio of conviction impact to balance impact in each experiment and grouped the results by upward and downward market trends. The grouped histograms below show distinct patterns.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/d/5/d536727595a832550e07d0d6945f5ee56a64682e.png\" data-download-href=\"https://ethresear.ch/uploads/default/d536727595a832550e07d0d6945f5ee56a64682e\" title=\"crv_vp_accu\"><img src=\"https://ethresear.ch/uploads/default/original/3X/d/5/d536727595a832550e07d0d6945f5ee56a64682e.png\" alt=\"crv_vp_accu\" data-base62-sha1=\"uqalVdi845Lu36f5WWH92pov0rY\" width=\"690\" height=\"345\" data-dominant-color=\"DFD8EB\"></a></div><p></p>\n<p>During downward trends, the log ratio values were mainly concentrated between 0 and 0.5, displaying a distribution similar to a normal distribution. This suggests that Shrimp behavior is more uniform in downward markets, and most log ratios exceeding 0 indicate a tendency among Shrimp to increase their lock-up duration to alter their voting power.</p>\n<p>During upward trends, the scenario was notably more complex, as three peaks around -0.3, 0.5, and 1 indicate that Shrimp behavior is inconsistent during upward markets. However, most Shrimps preferred increasing their lock-up window, a tendency that was even more pronounced than during downward trends.</p>\n<p><strong>5.2.3 Polkadot:</strong> When analyzing Polkadot’s market trends, we observed a deviation from the typical pattern observed in Curve Finance. Instead of a normal distribution, there was a noticeable long tail in the data. Upon closer inspection, we discovered a fascinating insight: a particular group of Shrimp voters in Polkadot had a strong inclination towards raising their lock-up window rather than increasing their balance during bullish market conditions. This behavior was particularly prominent and suggestive of a unique pattern among this subset of voters.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/f/0/f08029351ce5b605f8ceb52b4074c5c8b7c9ef1c.png\" data-download-href=\"https://ethresear.ch/uploads/default/f08029351ce5b605f8ceb52b4074c5c8b7c9ef1c\" title=\"dot_vp_accu\"><img src=\"https://ethresear.ch/uploads/default/original/3X/f/0/f08029351ce5b605f8ceb52b4074c5c8b7c9ef1c.png\" alt=\"dot_vp_accu\" data-base62-sha1=\"yjzaWvSxHzLDskkzXoayuwSc5PK\" width=\"690\" height=\"345\" data-dominant-color=\"EAE9F2\"></a></div><p></p>\n<h2><a name=\"h-6-conclusion-11\" class=\"anchor\" href=\"https://ethresear.ch#h-6-conclusion-11\"></a>6. Conclusion</h2>\n<p>The research focused on identifying key trends in voter behavior by examining voter personas, types of governance proposals, and patterns of voting power accumulation.</p>\n<ol>\n<li>There is a noticeable correlation between financial incentives at the token lock-up level in Curve Finance and the absence of such incentives in Polkadot, which may directly affect voter turnout - relatively high in Curve Finance than in Polkadot.</li>\n<li>The analysis indicates that financial proposals, specifically Curve Finance- gauge proposals and Polkadot’s treasury proposals, constitute the majority of proposals in both systems and are central to all significant voting activities.</li>\n<li>The study defines and analyzes various voter personas and shows interesting patterns,  the most crucial of them being that Whales, Sharks, and Dolphins, prefer shorter lock-up periods, while the majority of low-staked holders, referred to as Shrimps, opt for longer lock-up durations.</li>\n<li>In Curve Finance, most voters choose the longest available lock-up period. This behavior contrasts sharply with that of Polkadot voters, underscoring the potential influence of financial incentives on long-term voter alignment with the protocol.</li>\n<li>A new methodology is introduced to analyze voter behavior under various market conditions. The results indicate that market trends significantly influence voter behavior. For instance, during market upswings, shrimps in both governance systems have differing preferences, utilizing both adjustments to their lock-up durations and increasing their token holdings to maximize voting power. During market downturns, on the other hand, there is a pronounced tendency among the shrimp voter group to extend their lock-up periods. While the sample size for other voter personas is too small to establish the above trends with confidence, similar tendencies could be seen in preliminary analysis of those sets as well.</li>\n</ol>\n<p>This research contributes to our understanding of decentralized governance within blockchain ecosystems and aims to provide insights that could help in the design and optimization of governance mechanisms. Continued research in this field is vital to enhancing the scalability and effectiveness of decentralized decision-making as the blockchain landscape continues to evolve.</p>\n<h2><a name=\"references-12\" class=\"anchor\" href=\"https://ethresear.ch#references-12\"></a>References</h2>\n<p>[1]: Beck, Roman; Müller-Bloch, Christoph; and King, John Leslie (2018) “Governance in the Blockchain Economy: A Framework and Research Agenda,” Journal of the Association for Information Systems, 19(10),.<br>\n[2]: <a href=\"http://Etherscan.Io\" rel=\"noopener nofollow ugc\">Etherscan.Io</a>. n.d. “Ethereum (ETH) Blockchain Explorer.” Ethereum (ETH) Blockchain Explorer. <a href=\"https://etherscan.io/\" rel=\"noopener nofollow ugc\">https://etherscan.io/</a>.<br>\n[3]: S. Wang, W. Ding, J. Li, Y. Yuan, L. Ouyang and F. -Y. Wang, “Decentralized Autonomous Organizations: Concept, Model, and Applications,” in IEEE Transactions on Computational Social Systems, vol. 6, no. 5, pp. 870-878, Oct. 2019, doi: 10.1109/TCSS.2019.2938190.,<br>\n[4]: “Zuwu on X: ‘Hey <span class=\"mention\">@Opensea</span> Why Does It Appear <span class=\"mention\">@Natechastain</span> Has a Few Secret Wallets That Appears to Buy Your Front Page Drops Before They Are Listed, Then Sells Them Shortly After the Front-page-hype Spike for Profits, and Then Tumbles Them Back to His Main Wallet With His Punk on It?’ / X.” n.d. X (Formerly Twitter). <a href=\"https://twitter.com/0xZuwu/status/1437921263394115584\" rel=\"noopener nofollow ugc\">https://twitter.com/0xZuwu/status/1437921263394115584</a>.<br>\n[5]: CurveFi. n.d. “Understanding Curve (V1) - Curve Resources.” <a href=\"https://resources.curve.fi/base-features/understanding-curve/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Understanding Curve (v1) - Curve Resources</a>.<br>\n[6]: “Governance V1 · Polkadot Wiki.” 2024. May 1, 2024. <a href=\"https://wiki.polkadot.network/docs/learn/learn-governance\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Governance V1 · Polkadot Wiki</a>.<br>\n[7]: Lin William Cong, Ye Li, Neng Wang, Tokenomics: Dynamic Adoption and Valuation, The Review of Financial Studies, Volume 34, Issue 3, March 2021, Pages 1105–1155, <a href=\"https://doi.org/10.1093/rfs/hhaa089\" rel=\"noopener nofollow ugc\">https://doi.org/10.1093/rfs/hhaa089</a><br>\n[8]: Curve DAO. n.d. “Curve DAO.” <a href=\"https://classic.curve.fi/files/CurveDAO.pdf\" rel=\"noopener nofollow ugc\">https://classic.curve.fi/files/CurveDAO.pdf</a>.<br>\n[9]: “WTF Are veTokens on Bankless.” n.d. <a href=\"https://www.bankless.com/wtf-are-vetokens\" rel=\"noopener nofollow ugc\">https://www.bankless.com/wtf-are-vetokens</a>.<br>\n[10]: CurveFi. n.d. “Gauge Weights - Curve Resources.” <a href=\"https://resources.curve.fi/reward-gauges/gauge-weights/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Gauge weights - Curve Resources</a>.<br>\n[11]: Zetzsche, Dirk Andreas and Arner, Douglas W. and Buckley, Ross P., Decentralized Finance (DeFi) (September 30, 2020). Journal of Financial Regulation, 2020, 6, 172–203, Available at SSRN: <a href=\"https://ssrn.com/abstract=3539194\" rel=\"noopener nofollow ugc\">https://ssrn.com/abstract=3539194</a> or <a href=\"http://dx.doi.org/10.2139/ssrn.3539194\" rel=\"noopener nofollow ugc\">http://dx.doi.org/10.2139/ssrn.3539194</a><br>\n[12]: CryptoDaily. 2024. “Polkadot’s Uniquely Decentralized Community Governance Model Accelerates Ecosystem Traction.” Crypto Daily, March 5, 2024. <a href=\"https://cryptodaily.co.uk/2024/03/polkadots-uniquely-decentralized-community-governance-model-accelerates-ecosystem-traction\" rel=\"noopener nofollow ugc\">https://cryptodaily.co.uk/2024/03/polkadots-uniquely-decentralized-community-governance-model-accelerates-ecosystem-traction</a>.<br>\n[13]: “Governance V1 · Polkadot Wiki.” 2024. May 1, 2024. <a href=\"https://wiki.polkadot.network/docs/learn/learn-governance#adaptive-quorum-biasing\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Governance V1 · Polkadot Wiki</a>.<br>\n[14]: “Introduction to Polkadot OpenGov · Polkadot Wiki.” 2024. April 3, 2024. <a href=\"https://wiki.polkadot.network/docs/learn-polkadot-opengov#voluntary-locking-conviction-voting\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Introduction to Polkadot OpenGov · Polkadot Wiki</a>.<br>\n[15]: Emmett, Jeff. 2022. “Conviction Voting: A Novel Continuous Decision Making Alternative to Governance.” Medium, April 22, 2022. <a href=\"https://blog.giveth.io/conviction-voting-a-novel-continuous-decision-making-alternative-to-governance-aa746cfb9475\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Conviction Voting: A Novel Continuous Decision Making Alternative to Governance | by Jeff Emmett | Giveth</a>.<br>\n[16]: CoinMarketCap. 2023. “What Is Vote Escrow?” CoinMarketCap Academy. April 5, 2023. <a href=\"https://coinmarketcap.com/academy/article/what-is-vote-escrow\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">What Is Vote Escrow? | CoinMarketCap</a>.<br>\n[17]: “Blockchain Voting Is Overrated Among Uninformed People but Underrated Among Informed People.” 2021. May 25, 2021. <a href=\"https://vitalik.eth.limo/general/2021/05/25/voting2.html\" rel=\"noopener nofollow ugc\">https://vitalik.eth.limo/general/2021/05/25/voting2.html</a>.<br>\n[18]: “<a href=\"http://curve.fi/compound.%E2%80%9D\" rel=\"noopener nofollow ugc\">curve.fi/compound.”</a> n.d. Curve Finance. <a href=\"https://classic.curve.fi/\" rel=\"noopener nofollow ugc\">https://classic.curve.fi/</a>.<br>\n[19]: CurveFi. n.d. “Understanding Curve Pools - Curve Resources.” <a href=\"https://resources.curve.fi/lp/understanding-curve-pools/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Understanding curve pools - Curve Resources</a>.<br>\n[20]: CurveFi. n.d. “Boosting Your CRV Rewards - Curve Resources.” <a href=\"https://resources.curve.fi/reward-gauges/boosting-your-crv-rewards/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Boosting your CRV rewards - Curve Resources</a>.<br>\n[21]: CurveFi. n.d. “Overview - Curve Resources.” <a href=\"https://resources.curve.fi/vecrv/overview/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Overview - Curve Resources</a>.<br>\n[22]: CurveFi. n.d. “Locking CRV - Curve Resources.” <a href=\"https://resources.curve.fi/vecrv/locking-your-crv/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Locking CRV - Curve Resources</a>.<br>\n[23]: CurveFi. n.d. “Creating a DAO Proposal - Curve Resources.” <a href=\"https://resources.curve.fi/governance/proposals/creating-a-dao-proposal/?h=2500\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Creating a DAO proposal - Curve Resources</a>.<br>\n[24]: “Polkadot: Web3 Interoperability | Decentralized Blockchain.” n.d. Polkadot Network. <a href=\"https://polkadot.network/\" rel=\"noopener nofollow ugc\">https://polkadot.network/</a>.<br>\n[25]: “Polkadot Launch Phases · Polkadot Wiki.” 2024. May 1, 2024. <a href=\"https://wiki.polkadot.network/docs/learn/learn-launch#nominated-proof-of-stake\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Polkadot Launch Phases · Polkadot Wiki</a>.<br>\n[26]: “Polkadot’s Parachains · Polkadot Wiki.” n.d. <a href=\"https://wiki.polkadot.network/docs/learn-parachains-index\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Polkadot's Parachains · Polkadot Wiki</a>.<br>\n[27]: “Architecture · Polkadot Wiki.” 2024. April 3, 2024. <a href=\"https://wiki.polkadot.network/docs/learn-architecture#relay-chain\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Architecture · Polkadot Wiki</a>.<br>\n[28]: “Polkadot OpenGov · Polkadot Wiki.” n.d. <a href=\"https://wiki.polkadot.network/docs/learn-polkadot-opengov-index\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Polkadot OpenGov · Polkadot Wiki</a>.<br>\n[29]: “Governance V1 Treasury · Polkadot Wiki.” 2024. May 1, 2024. <a href=\"https://wiki.polkadot.network/docs/learn/learn-treasury\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Governance v1 Treasury · Polkadot Wiki</a>.<br>\n[30]: “Polkadot Parameters · Polkadot Wiki.” 2024. March 11, 2024. <a href=\"https://wiki.polkadot.network/docs/maintain-polkadot-parameters#governance\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Polkadot Parameters · Polkadot Wiki</a>.<br>\n[31]: “<a href=\"http://Curve.fi\" rel=\"noopener nofollow ugc\">Curve.fi</a> Governance.” n.d. <a href=\"http://Curve.Fi\" rel=\"noopener nofollow ugc\">Curve.Fi</a> Governance. <a href=\"https://gov.curve.fi/\" rel=\"noopener nofollow ugc\">https://gov.curve.fi/</a>.<br>\n[32]: “Michael Egorov (<span class=\"mention\">@Newmichwill</span>) / X.” n.d. X (Formerly Twitter). <a href=\"https://twitter.com/newmichwill?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\" rel=\"noopener nofollow ugc\">https://twitter.com/newmichwill?ref_src=twsrc^google|twcamp^serp|twgr^author</a>.<br>\n[33]: “Arkham | Deanonymizing the Blockchain.” n.d. <a href=\"https://www.arkhamintelligence.com/\" rel=\"noopener nofollow ugc\">https://www.arkhamintelligence.com/</a>.<br>\n[34]: CurveFi. n.d. “Understanding Gauges - Curve Resources.” <a href=\"https://resources.curve.fi/reward-gauges/understanding-gauges/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Understanding gauges - Curve Resources</a>.</p>\n<h2><a name=\"appendix-13\" class=\"anchor\" href=\"https://ethresear.ch#appendix-13\"></a>Appendix</h2>\n<p><a name=\"table 1\" href=\"https://ethresear.ch\"></a><br>\nTable 1: veCRV amount by lock-up period</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>1 CRV is locked for</th>\n<th>The user is assigned</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>one week</td>\n<td>0 veCRV</td>\n</tr>\n<tr>\n<td>one month</td>\n<td>0.02 veCRV</td>\n</tr>\n<tr>\n<td>six months</td>\n<td>0.13 veCRV</td>\n</tr>\n<tr>\n<td>one year</td>\n<td>0.25 veCRV</td>\n</tr>\n<tr>\n<td>two years</td>\n<td>0.5 veCRV</td>\n</tr>\n<tr>\n<td>three years</td>\n<td>0.75 veCRV</td>\n</tr>\n<tr>\n<td>four years</td>\n<td>1 veCRV</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><a name=\"table2\" href=\"https://ethresear.ch\"></a><br>\nTable 2: DOT conviction multiplier by democracy lock</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>1 DOT is locked for</th>\n<th>Multiplier</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>zero days</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>seven days</td>\n<td>1</td>\n</tr>\n<tr>\n<td>fourteen days</td>\n<td>2</td>\n</tr>\n<tr>\n<td>twenty eight days</td>\n<td>3</td>\n</tr>\n<tr>\n<td>fifty six days</td>\n<td>4</td>\n</tr>\n<tr>\n<td>one hundred and twelve days</td>\n<td>5</td>\n</tr>\n<tr>\n<td>two hundred and twenty four days</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><a name=\"table3\" href=\"https://ethresear.ch\"></a><br>\nTable 3: Top Proposal Address Labels on <a href=\"http://Curve.fi\" rel=\"noopener nofollow ugc\">Curve.fi</a></p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>Rank</th>\n<th>Proposer Address</th>\n<th>Count</th>\n<th>ID on Arkham</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0xbabe61887f1de2713c6f97e567623453d3c79f67</td>\n<td>55</td>\n<td><a href=\"http://Curve.fi\" rel=\"noopener nofollow ugc\">Curve.fi</a> Deployer</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0x745748bcfd8f9c2de519a71d789be8a63dd7d66c</td>\n<td>28</td>\n<td><span class=\"mention\">@skellet0r</span> (<a href=\"http://Curve.fi\" rel=\"noopener nofollow ugc\">Curve.fi</a>)</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0x7a16ff8270133f063aab6c9977183d9e72835428</td>\n<td>28</td>\n<td>Michael Egorov (<a href=\"http://Curve.fi\" rel=\"noopener nofollow ugc\">Curve.fi</a>)</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0x0000000000e189dd664b9ab08a33c4839953852c</td>\n<td>22</td>\n<td>Charlie Watkins (<a href=\"http://Curve.fi\" rel=\"noopener nofollow ugc\">Curve.fi</a>)</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0x71f718d3e4d1449d1502a6a7595eb84ebccb1683</td>\n<td>22</td>\n<td></td>\n</tr>\n<tr>\n<td>6</td>\n<td>0x947b7742c403f20e5faccdac5e092c943e7d0277</td>\n<td>22</td>\n<td>Convex Finance Deployer</td>\n</tr>\n<tr>\n<td>7</td>\n<td>0x34d6dbd097f6b739c59d7467779549aea60e1f84</td>\n<td>17</td>\n<td></td>\n</tr>\n<tr>\n<td>8</td>\n<td>0xa1992346630fa9539bc31438a8981c646c6698f1</td>\n<td>14</td>\n<td></td>\n</tr>\n<tr>\n<td>9</td>\n<td>0xf7bd34dd44b92fb2f9c3d2e31aaad06570a853a6</td>\n<td>13</td>\n<td></td>\n</tr>\n<tr>\n<td>10</td>\n<td>0x52f541764e6e90eebc5c21ff570de0e2d63766b6</td>\n<td>13</td>\n<td>Stake Dao: Curve yCRV Voter</td>\n</tr>\n</tbody>\n</table>\n</div>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/voter-behavior-in-blockchain-governance-a-comparative-study-of-curve-finance-and-polkadot/19436\">Read full topic</a></p>","link":"https://ethresear.ch/t/voter-behavior-in-blockchain-governance-a-comparative-study-of-curve-finance-and-polkadot/19436","pubDate":"Thu, 02 May 2024 12:59:29 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19436"},"source":{"@url":"https://ethresear.ch/t/voter-behavior-in-blockchain-governance-a-comparative-study-of-curve-finance-and-polkadot/19436.rss","#text":"Voter Behavior in Blockchain Governance: A Comparative Study of Curve Finance and Polkadot"},"filter":false},{"title":"Builder Reveal Timing Game in ePBS","dc:creator":"terence","category":"Uncategorized","description":"<p>This post starts by analyzing the advantages of consensus and execution block timing games both before and after the ePBS setting, focusing on the preparation time available to the next slot’s proposer/builder for the subsequent slot. We then dissect the dynamics of timing when the builder block reveals, focusing on altruistic, honest, and greedy rational builder rationale. The second half of the post highlights the greedy rational builder’s advantage in controlling the timing of the execution block reveal. Finally, we will list out a series of open questions for further exploration. First, a little meme:</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/3X/8/6/8625554ee31b5e4a649d3fd9aa50855a7b0f1997.png\" alt=\"Screenshot 2024-04-30 at 9.30.36 AM\" data-base62-sha1=\"j8HXXDuAY1rQQG8finH40n5ydkH\" width=\"479\" height=\"341\"></p>\n<h3><a name=\"terminologies-acronyms-abbreviations-1\" class=\"anchor\" href=\"https://ethresear.ch#terminologies-acronyms-abbreviations-1\"></a>Terminologies, Acronyms &amp; Abbreviations</h3>\n<ul>\n<li><strong>ePBS (enshrined proposer-builder separation)</strong>: A protocol enhancement where relayer functionality in mev-boost setting becomes part of the protocol, enabling trustless exchanges between validators and builders. New validator duties: payload timeliness committees is introduced. Unless specified otherwise, references below refer to <a href=\"https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ\" rel=\"noopener nofollow ugc\">block</a> <a href=\"https://hackmd.io/@potuz/rJ9GCnT1C\" rel=\"noopener nofollow ugc\">auction</a> ePBS.</li>\n<li><strong>CL (Consensus Layer)</strong>: Where consensus activities like votes, slashing, and deposits happen…</li>\n<li><strong>EL (Execution Layer)</strong>: Where execution activities like user transactions and EVM happen.</li>\n<li><strong>Pre-ePBS beacon block</strong>: A consensus block that contains consensus data and a full execution block.</li>\n<li><strong>Post-ePBS beacon block</strong>: A consensus block that contains consensus data and an execution header, the header commits to the execution block which is later revealed. The transactions are hidden.</li>\n<li><strong>Execution block</strong>: A block that contains execution data and transactions. Produced by builders.</li>\n<li><strong>Pre-ePBS Proposer</strong>: Builds and reveals pre-ePBS beacon block.</li>\n<li><strong>Post-ePBS Proposer</strong>: Builds and reveals post-ePBS beacon block.</li>\n<li><strong>Builder</strong>: Builds and reveals execution block in post-ePBS setting.</li>\n<li><strong>Beacon attester</strong>: Votes on the consensus block before <span class=\"math\">BeaconAttestationDeadline</span>.</li>\n<li><strong>Payload attester</strong>: Votes on the execution block before <span class=\"math\">PayloadAttestationDeadline</span>.</li>\n</ul>\n<h3><a name=\"settings-2\" class=\"anchor\" href=\"https://ethresear.ch#settings-2\"></a>Settings</h3>\n<p>Now we’ll go over each setting case by case. Focus on the following variations:</p>\n<ul>\n<li>Pre-ePBS setting</li>\n<li>Post-ePBS setting</li>\n<li>Spec definition: assuming no networking and client processing delays.</li>\n<li>Beacon proposer rational behavior: Inclue networking and processing delays and rational proposer and builder behaviors.</li>\n</ul>\n<h3><a name=\"pre-epbs-spec-definition-3\" class=\"anchor\" href=\"https://ethresear.ch#pre-epbs-spec-definition-3\"></a>Pre-ePBS + Spec definition</h3>\n<ul>\n<li>Proposer reveals pre-ePBS beacon block, which contains an execution block at <span class=\"math\">SlotStart</span>.</li>\n<li>Next slot beacon proposer or builder has <span class=\"math\">SlotDuration</span> to prepare an execution block for the next slot.</li>\n<li>The execution block preparation time for next slot is <span class=\"math\">SlotDuration</span>.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/d/1/d14a473ed9b05e60bbe6c8068e7aaf0083f3e015.png\" data-download-href=\"https://ethresear.ch/uploads/default/d14a473ed9b05e60bbe6c8068e7aaf0083f3e015\" title=\"Screenshot 2024-04-25 at 11.18.56 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/d/1/d14a473ed9b05e60bbe6c8068e7aaf0083f3e015_2_690x148.png\" alt=\"Screenshot 2024-04-25 at 11.18.56 AM\" data-base62-sha1=\"tRsVM2NJrFvw5iGC609lklvBS2p\" width=\"690\" height=\"148\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/d/1/d14a473ed9b05e60bbe6c8068e7aaf0083f3e015_2_690x148.png, https://ethresear.ch/uploads/default/original/3X/d/1/d14a473ed9b05e60bbe6c8068e7aaf0083f3e015.png 1.5x, https://ethresear.ch/uploads/default/original/3X/d/1/d14a473ed9b05e60bbe6c8068e7aaf0083f3e015.png 2x\" data-dominant-color=\"E3E3E3\"></a></div></li>\n</ul>\n<h3><a name=\"pre-epbs-beacon-proposer-rational-behavior-4\" class=\"anchor\" href=\"https://ethresear.ch#pre-epbs-beacon-proposer-rational-behavior-4\"></a>Pre-ePBS + Beacon proposer rational behavior</h3>\n<ul>\n<li>Proposer reveals pre-ePBS beacon block as close to the <span class=\"math\">BeaconAttestationDeadline</span> as possible.</li>\n<li>Next slot proposer or builder has <span class=\"math\">SlotDuration-AttestationDeadline</span> time to prepare an execution block for the next slot.</li>\n<li>The next slot beacon proposer may also play timing game:\n<ul>\n<li>The execution block preparation time for next slot is <span class=\"math\">SlotDuration</span> w/ timing game.</li>\n<li>The execution block preparation time for next slot is <span class=\"math\">SlotDuration-AttestationDeadline</span> w/o timing game.</li>\n</ul>\n</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/7/4/74da7e206fa90ee82830043bbed4a51b2dac96af.png\" data-download-href=\"https://ethresear.ch/uploads/default/74da7e206fa90ee82830043bbed4a51b2dac96af\" title=\"Screenshot 2024-04-25 at 11.19.22 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/7/4/74da7e206fa90ee82830043bbed4a51b2dac96af_2_690x158.png\" alt=\"Screenshot 2024-04-25 at 11.19.22 AM\" data-base62-sha1=\"gFJvja83eyD8mo1XDUzeKoMlUgn\" width=\"690\" height=\"158\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/7/4/74da7e206fa90ee82830043bbed4a51b2dac96af_2_690x158.png, https://ethresear.ch/uploads/default/original/3X/7/4/74da7e206fa90ee82830043bbed4a51b2dac96af.png 1.5x, https://ethresear.ch/uploads/default/original/3X/7/4/74da7e206fa90ee82830043bbed4a51b2dac96af.png 2x\" data-dominant-color=\"E3E3E3\"></a></div><p></p>\n<p>Note: The beacon proposer playing the timing game will consider P2P propagation time and the client processing times.<br>\n<img src=\"https://ethresear.ch/uploads/default/original/3X/0/1/01f704b8a2c554661c428981b6202b07ea37d7e9.png\" alt=\"Screenshot 2024-04-25 at 11.19.46 AM\" data-base62-sha1=\"hnI95CtFeGAfpzK8Z4My0l8PY5\" width=\"685\" height=\"198\"></p>\n<h3><a name=\"post-epbs-spec-definition-5\" class=\"anchor\" href=\"https://ethresear.ch#post-epbs-spec-definition-5\"></a>Post-ePBS + Spec definition</h3>\n<p>At this point, we assume readers have sufficient knowledge of how the basics of post-ePBS work. If not, refer to <a href=\"https://hackmd.io/@potuz/rJ9GCnT1C#Anatomy-of-a-slot\" rel=\"noopener nofollow ugc\">Anatomy of a Slot</a>. In most ePBS designs, a slot is divided into two intervals: the first is about consensus that commits to the execution header, and the second is about revealing the execution result. This clear pipelining has several benefits but also introduces new dynamic which we will explore below.</p>\n<ul>\n<li>Proposer reveals post-ePBS beacon block at the <span class=\"math\">SlotStart</span>.</li>\n<li>Winning builder observes the proposer’s commitment to its submitted execution header.</li>\n<li>If the header is committed and when it’s safe to reveal, the builder reveals the execution block at <span class=\"math\">SlotStart / 2</span>.</li>\n<li>The execution block preparation time for next slot is <span class=\"math\">SlotDuration / 2</span></li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/4/0/40483990d88767b417907d3987fd554620adce35.png\" data-download-href=\"https://ethresear.ch/uploads/default/40483990d88767b417907d3987fd554620adce35\" title=\"Screenshot 2024-04-25 at 11.21.35 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/4/0/40483990d88767b417907d3987fd554620adce35_2_690x164.png\" alt=\"Screenshot 2024-04-25 at 11.21.35 AM\" data-base62-sha1=\"9aFiy0UYEDdu2mWyyjaAoAwZphr\" width=\"690\" height=\"164\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/4/0/40483990d88767b417907d3987fd554620adce35_2_690x164.png, https://ethresear.ch/uploads/default/original/3X/4/0/40483990d88767b417907d3987fd554620adce35.png 1.5x, https://ethresear.ch/uploads/default/original/3X/4/0/40483990d88767b417907d3987fd554620adce35.png 2x\" data-dominant-color=\"E4E4E4\"></a></div><p></p>\n<p>Note: No one can start preparing for the next slot’s execution block unless the current execution block is revealed.</p>\n<h3><a name=\"post-epbs-x-beacon-proposer-rational-behavior-6\" class=\"anchor\" href=\"https://ethresear.ch#post-epbs-x-beacon-proposer-rational-behavior-6\"></a>Post-ePBS x Beacon proposer rational behavior</h3>\n<ul>\n<li>Proposer reveals post-ePBS beacon block as close to the new <span class=\"math\">BeaconAttestationDeadlineEPBS</span> as possible.</li>\n<li>Builder observes the proposer’s commitment to its submitted execution header.</li>\n<li>If the header is committed and when it’s safe to reveal, the builder reveals the execution block at <span class=\"math\">SlotStart / 2</span>.</li>\n<li>The next slot beacon proposer may also play timing game:\n<ul>\n<li>The execution block preparation time for next slot is <span class=\"math\">SlotDuration - (SlotStart / 2) + BeaconAttestationDeadlineEPBS</span> w/ timing game</li>\n<li>The execution block preparation time for next slot is <span class=\"math\">SlotDuration - (SlotStart / 2)</span> w/o timing game</li>\n</ul>\n</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/e/e/ee5b4978408abc7a23d6e4a27e3861048c8b3ed9.png\" data-download-href=\"https://ethresear.ch/uploads/default/ee5b4978408abc7a23d6e4a27e3861048c8b3ed9\" title=\"Screenshot 2024-04-25 at 11.22.08 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/e/e/ee5b4978408abc7a23d6e4a27e3861048c8b3ed9_2_690x204.png\" alt=\"Screenshot 2024-04-25 at 11.22.08 AM\" data-base62-sha1=\"y0BdBxvu4e21qo6ZgOwON8TEugh\" width=\"690\" height=\"204\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/e/e/ee5b4978408abc7a23d6e4a27e3861048c8b3ed9_2_690x204.png, https://ethresear.ch/uploads/default/original/3X/e/e/ee5b4978408abc7a23d6e4a27e3861048c8b3ed9.png 1.5x, https://ethresear.ch/uploads/default/original/3X/e/e/ee5b4978408abc7a23d6e4a27e3861048c8b3ed9.png 2x\" data-dominant-color=\"E4E4E4\"></a></div><p></p>\n<p>Notes:</p>\n<ol>\n<li>At first glance, the beacon block proposer will still play the timing game and try to release as close to <span class=\"math\">BeaconAttestationDeadlineEPBS</span> as possible. Nothing changes in that regard.</li>\n<li><span class=\"math\">BeaconAttestationDeadlineEPBS</span> is 1s shorter than <span class=\"math\">BeaconAttestationDeadline</span> to account for payload timeliness committee duty fitting within <span class=\"math\">SlotDuration</span>.</li>\n<li>The beacon proposer playing the timing game will consider P2P propagation time and the processing times of both the CL client and the EL client.</li>\n</ol>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/b/2/b2ba91ddc0a51a9e3757f8907ba98202f2e513aa.png\" data-download-href=\"https://ethresear.ch/uploads/default/b2ba91ddc0a51a9e3757f8907ba98202f2e513aa\" title=\"Screenshot 2024-04-25 at 11.22.24 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/b/2/b2ba91ddc0a51a9e3757f8907ba98202f2e513aa_2_690x186.png\" alt=\"Screenshot 2024-04-25 at 11.22.24 AM\" data-base62-sha1=\"pv6IflRuvp62SGrSVf2yvfOikoq\" width=\"690\" height=\"186\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/b/2/b2ba91ddc0a51a9e3757f8907ba98202f2e513aa_2_690x186.png, https://ethresear.ch/uploads/default/original/3X/b/2/b2ba91ddc0a51a9e3757f8907ba98202f2e513aa.png 1.5x, https://ethresear.ch/uploads/default/original/3X/b/2/b2ba91ddc0a51a9e3757f8907ba98202f2e513aa.png 2x\" data-dominant-color=\"E2E2E2\"></a></div><p></p>\n<h3><a name=\"execution-block-reveal-window-7\" class=\"anchor\" href=\"https://ethresear.ch#execution-block-reveal-window-7\"></a>Execution block reveal window</h3>\n<p>So far, we have focused on the beacon block reveal window. What about the execution block reveal window once its header is committed in the beacon block? The consensus spec indicates that the builder should reveal at <span class=\"math\">SlotDuration / 2</span>, but what is the execution block reveal window? Let’s first consider how early the builder could reveal its execution block once it’s “safe” to do so. The builder monitors the fork choice weight of the beacon block which commits to its header. If the weight is weak, the builder may choose to withhold the execution block by sending a withhold message. Assuming the message is seen by more than half of execution block committee, a withhold boost is granted and gives fork choice weight to the beacon block’s parent, which allows subsequent honest proposer and attesters to reorg the weak beacon block of the canonical view, so the builder has a way out of unconditional payment.</p>\n<p>To determine how much the fork weight is sufficient to reveal, consider the attack where the next slot’s proposer is malicious, and wants to reorg the builder’s execution block. We want the next slot’s attesters to recognize the builder’s execution block as head. This requires winning the following equation: assuming <span class=\"math\">X</span> represents the percentage of current slot attesters who voted for the beacon block, <span class=\"math\">1-X</span> is missing, and you want execution block’s <span class=\"math\">RevealBoost + X &gt; ProposerBoost + 1 - X</span> to be true. With <span class=\"math\">RevealBoost=40</span> and <span class=\"math\">ProposerBoost=20</span>, the builder should reveal when <span class=\"math\">X&gt;40</span>. This means over 40% of the current slot’s beacon committee members voted for the proposer’s beacon block for the builder to feel “safe” to reveal.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/f/f/ff126f5ba6a4bd8e4f2772ae860558a3469645ea.png\" data-download-href=\"https://ethresear.ch/uploads/default/ff126f5ba6a4bd8e4f2772ae860558a3469645ea\" title=\"Screenshot 2024-04-25 at 11.22.47 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/f/f/ff126f5ba6a4bd8e4f2772ae860558a3469645ea_2_690x234.png\" alt=\"Screenshot 2024-04-25 at 11.22.47 AM\" data-base62-sha1=\"AotfBJQ0Jw8tqaSYox7zC9u1Qi6\" width=\"690\" height=\"234\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/f/f/ff126f5ba6a4bd8e4f2772ae860558a3469645ea_2_690x234.png, https://ethresear.ch/uploads/default/original/3X/f/f/ff126f5ba6a4bd8e4f2772ae860558a3469645ea.png 1.5x, https://ethresear.ch/uploads/default/original/3X/f/f/ff126f5ba6a4bd8e4f2772ae860558a3469645ea.png 2x\" data-dominant-color=\"E6E6E6\"></a></div><p></p>\n<p>Note: <span class=\"math\">RevealBoost + X &gt; ProposerBoost + 1 - X</span> assumes the next slot’s proposer wants to reorg the current beacon block + execution block. This is a more powerful attack than just reorg the execution block itself because it gets <span class=\"math\">1-X</span> on its side due to (block, slot) voting.</p>\n<p>Now we have determined that the builder may feel safe to reveal the execution block as soon as it observes that more than 40% of beacon attesters have voted for the beacon block. We will refer to this  point as the <span class=\"math\">SafeRevealDeadline</span>. When is the latest point a builder can reveal? The latest point is the <span class=\"math\">PayloadAttestationDeadline</span>, which is when payload attestations vote on whether the execution block has revealed on time and valid. Thus, the builder’s reveal window spans from <span class=\"math\">SafeRevealDeadline</span> to <span class=\"math\">PayloadAttestationDeadline</span>.</p>\n<p>Note: Similar to the beacon proposer, the builder will consider P2P propagation time and the execution processing times of the execution block.</p>\n<h3><a name=\"execution-block-reveal-strategy-8\" class=\"anchor\" href=\"https://ethresear.ch#execution-block-reveal-strategy-8\"></a>Execution block reveal strategy</h3>\n<p>When modeling rational builder behaviors, consider that builders compete to submit the highest bids at <span class=\"math\">SlotStart</span> and that having more time to prepare the execution block incurs an advantage to pack a more profitable execution block. The starting time for building the next slot’s execution block depends on knowing the execution result of the current slot’s execution block. Builders can be categorized into two modes: the winning builder of the current slot and all other builders who did not win but want to win next slot. The winning builder can start building for the next slot as soon as its header is committed in the beacon block, while all others must wait until the current’s block is revealed. Below is a table summarizing the three strategies of the winning builder can impose based on reveal time:</p>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>Builder Type</th>\n<th>Reveal Time</th>\n<th>Time for Others to Prepare Next Slot Block</th>\n<th>Time for Self to Prepare Next Slot Block</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Altruistic</td>\n<td><span class=\"math\">SafeDeadline</span></td>\n<td><span class=\"math\">SlotDuration-SafeRevealTime</span></td>\n<td><span class=\"math\">SlotDuration-BeaconAttestationDeadlineEPBS</span></td>\n</tr>\n<tr>\n<td>Spec</td>\n<td><span class=\"math\">SlotDuration/2</span></td>\n<td><span class=\"math\">SlotDuation/2</span></td>\n<td><span class=\"math\">SlotDuration-BeaconAttestationDeadlineEPBS</span></td>\n</tr>\n<tr>\n<td>Greedy Rational</td>\n<td><span class=\"math\">PayloadAttestationDeadLine</span></td>\n<td><span class=\"math\">SlotDuration-PayloadAttestationDeadLine</span></td>\n<td><span class=\"math\">SlotDuration-BeaconAttestationDeadlineEPBS</span></td>\n</tr>\n</tbody>\n</table>\n</div><p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/8/7/87f2dbdcc953b1df4b2bb38b18a6986e543dc350.png\" data-download-href=\"https://ethresear.ch/uploads/default/87f2dbdcc953b1df4b2bb38b18a6986e543dc350\" title=\"Screenshot 2024-04-25 at 11.24.03 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/8/7/87f2dbdcc953b1df4b2bb38b18a6986e543dc350_2_690x224.png\" alt=\"Screenshot 2024-04-25 at 11.24.03 AM\" data-base62-sha1=\"joEMqOQ6cBtEPnQk8ZGb8kvCayI\" width=\"690\" height=\"224\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/8/7/87f2dbdcc953b1df4b2bb38b18a6986e543dc350_2_690x224.png, https://ethresear.ch/uploads/default/original/3X/8/7/87f2dbdcc953b1df4b2bb38b18a6986e543dc350.png 1.5x, https://ethresear.ch/uploads/default/original/3X/8/7/87f2dbdcc953b1df4b2bb38b18a6986e543dc350.png 2x\" data-dominant-color=\"E7E7E7\"></a></div><p></p>\n<p>The builder of the current winning slot influencees when the next slot’s builder can start building their block. The dominant strategy for a builder may be to win the current block auction and reveal as late as possible, thus securing more time to build than the competition and such advantage may compound across multiple subsequent slots.</p>\n<p>If the next slot proposer plays timing game, all builders receive extra time, equal to the <span class=\"math\">BeaconAttestationDeadline</span>, to prepare the execution block. This scenario assumes that builders continue to submit their bids and execution headers after <span class=\"math\">SlotStart</span>, approaching the deadline, via p2p or RPC.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/1/0/10ddcfc505452ddb63da807366c41bf1605e1c6c.png\" data-download-href=\"https://ethresear.ch/uploads/default/10ddcfc505452ddb63da807366c41bf1605e1c6c\" title=\"Screenshot 2024-04-30 at 9.02.05 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/1/0/10ddcfc505452ddb63da807366c41bf1605e1c6c_2_690x192.png\" alt=\"Screenshot 2024-04-30 at 9.02.05 AM\" data-base62-sha1=\"2pcRReNSD9VGULwgAeUC4fvRzek\" width=\"690\" height=\"192\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/1/0/10ddcfc505452ddb63da807366c41bf1605e1c6c_2_690x192.png, https://ethresear.ch/uploads/default/original/3X/1/0/10ddcfc505452ddb63da807366c41bf1605e1c6c.png 1.5x, https://ethresear.ch/uploads/default/original/3X/1/0/10ddcfc505452ddb63da807366c41bf1605e1c6c.png 2x\" data-dominant-color=\"E4E4E4\"></a></div><p></p>\n<h3><a name=\"builder-reveal-strategy-in-other-epbs-auctions-9\" class=\"anchor\" href=\"https://ethresear.ch#builder-reveal-strategy-in-other-epbs-auctions-9\"></a>Builder reveal strategy in other ePBS auctions</h3>\n<p>The advantage which the current slot winning builder gains may be fundamental in other ePBS auction designs, such as slot auction, but the dynamics and advantages slightly differ in other designs. Let’s examine the examples in a simple slot auction:</p>\n<p>In a simple slot auction, the beacon proposer selects the winning builder without committing to the execution outcome, they only commit to the future right of who can propose the execution block. Here, the beacon proposer timing game is no longer relevant. Under rational strategy, the proposer reveals the beacon block at <span class=\"math\">SlotStart</span>. A builder following the honest spec definition should reveal at <span class=\"math\">SlotDuration/2</span>, but under a rational, greedy strategy, the builder may reveal close to the <span class=\"math\">ExecutionAttestationDeadline</span>.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/c/3/c3f4abd52ed84e736ba299fe4c5a613650901cab.png\" data-download-href=\"https://ethresear.ch/uploads/default/c3f4abd52ed84e736ba299fe4c5a613650901cab\" title=\"Screenshot 2024-04-25 at 11.24.56 AM\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/c/3/c3f4abd52ed84e736ba299fe4c5a613650901cab_2_690x251.png\" alt=\"Screenshot 2024-04-25 at 11.24.56 AM\" data-base62-sha1=\"rXvjmUC9A474CqdjqZxfwWKYGAb\" width=\"690\" height=\"251\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/c/3/c3f4abd52ed84e736ba299fe4c5a613650901cab_2_690x251.png, https://ethresear.ch/uploads/default/original/3X/c/3/c3f4abd52ed84e736ba299fe4c5a613650901cab.png 1.5x, https://ethresear.ch/uploads/default/original/3X/c/3/c3f4abd52ed84e736ba299fe4c5a613650901cab.png 2x\" data-dominant-color=\"E6E6E6\"></a></div><p></p>\n<p>Revealing close to the deadline now gains more advantages than ePBS block auction:</p>\n<ol>\n<li>Gives more time to build the execution block.</li>\n<li>Gives everyone else less time to come up with a confident value bid for the next slot.</li>\n</ol>\n<p>The main reason for a builder to reveal the block close to the execution deadline is to maximize the time available to build the execution block and chase more value, which is a different emphasis than in the block auction model. The winning builder may have a better valuation for the next slot, given its consistent view of the mempool but that may be irrelevant.</p>\n<p>In the execution ticket model, since the ticket selection occurs far in advance, the execution proposer assignments can be determined and known an epoch before, it becomes less practical to manipulate. To attempt gaming the system, one would need to purchase a lot of tickets at once, but these tickets might have to go through a pending queue before becoming eligible. The specific design details are to be determined.</p>\n<h3><a name=\"open-questions-10\" class=\"anchor\" href=\"https://ethresear.ch#open-questions-10\"></a>Open questions</h3>\n<p>The dominant strategy for the builder discussed above is purely speculative, there are many open questions to answer. One key question is how should the builder bid value correlates by having more time to prepare. How much of the bid value is based on more build time versus more private order flow? If most of the value comes from private order flow, it may be ok for the winning builder to have more time to build than everyone else. But to create a fair and competitive environment, it could be beneficial to define some baseline models that specifiy a minimal build time for all builders, ensuring some equal footings. Could there be incentives or enforcement mechanisms to ensure builders reveal their execution payload on time? One approach could be to make the <span class=\"math\">PayloadAttestationDeadline</span> random from the perspective of the payload timeliness committee, making it harder for the builder to game the reveal time. Another straightforward solution is to set the <span class=\"math\">PayloadAttestationDeadline</span> earlier, which requires consideration of several factors such as the propagation times for attestations and execution payloads, and the time needed to verify an execution payload. When a builder wins consecutive slots, such as slots n and n+1, does this provide a compounding advantage for winning subsequent slots like n+2, compared to winning only slot n+1? This aspect is currently unclear and requires more formalized analysis. Finally when a proposer commits to a builder’s block, could an execution state difference be shared and allow other builders to get the peak of the final state and allow other builders to start building on top without winning builder having to leak the payload? Gossiping the entire execution state is not feasible, but perhaps a cryptographic solution like Verkle trees or SNARKs could be used to communicate the state difference efficiently. However, it’s still uncertain whether this is fully safe and what second-order effects might arise.</p>\n<p><strong>Acknowledgment</strong>:<br>\nAlthough reviews cannot be viewed as an endorsements, I want to thank <a href=\"https://twitter.com/barnabemonnot\" rel=\"noopener nofollow ugc\">Barnabe</a> for his feedback on typos and his suggestion to extend to the <code>n+1</code> slot. This extension provides a clearer impression of the time available for <code>n+1</code> builders to construct their block if the <code>n+1</code> proposer is engaging in timing games.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/builder-reveal-timing-game-in-epbs/19424\">Read full topic</a></p>","link":"https://ethresear.ch/t/builder-reveal-timing-game-in-epbs/19424","pubDate":"Tue, 30 Apr 2024 16:37:39 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19424"},"source":{"@url":"https://ethresear.ch/t/builder-reveal-timing-game-in-epbs/19424.rss","#text":"Builder Reveal Timing Game in ePBS"},"filter":true},{"title":"Slashing Proofoor - On-chain slashed validator proofs","dc:creator":"Nerolation","category":"Execution Layer Research","description":"<h1><a name=\"slashing-proofoor-on-chain-slashed-validator-proofs-1\" class=\"anchor\" href=\"https://ethresear.ch#slashing-proofoor-on-chain-slashed-validator-proofs-1\"></a>Slashing Proofoor - On-chain slashed validator proofs</h1>\n<p>As of the <a href=\"https://github.com/ethereum/consensus-specs/tree/dev/specs/deneb\" rel=\"noopener nofollow ugc\">Dencun hardfork</a> in March 2024, it is possible to prove consensus layer information inside the EVM.<br>\nThis was made possible through <a href=\"https://eips.ethereum.org/EIPS/eip-4788\" rel=\"noopener nofollow ugc\">EIP-4788</a>, which proposed to make historic beacon block roots available inside the EVM and thereby enable contracts to verify proofs against them.<br>\nPotential use cases include withdrawal proofs for staking node operators or slashing proofs for restaking protocols.</p>\n<p>The latter will be the topic of this post/tutorial.</p>\n<blockquote>\n<p>Find the code <a href=\"https://github.com/nerolation/slashing-proofoor\" rel=\"noopener nofollow ugc\">here</a>.</p>\n</blockquote>\n<h2><a name=\"eip-4788-beacon-block-root-in-the-evm-2\" class=\"anchor\" href=\"https://ethresear.ch#eip-4788-beacon-block-root-in-the-evm-2\"></a>EIP-4788 - Beacon block root in the EVM</h2>\n<p>The specification of <a href=\"https://eips.ethereum.org/EIPS/eip-4788\" rel=\"noopener nofollow ugc\">EIP-4788</a> outlines the addition of a new field in the execution block headers: parent_beacon_block_root. This 32-byte field holds the hash tree root of the parent beacon block. This root is then stored and accessible in a smart contract at <a href=\"https://beaconcha.in/address/000f3df6d732807ef1319fb7b8bb8522d0beac02\" rel=\"noopener nofollow ugc\">0x000F3df6D732807Ef1319fB7B8bB8522d0Beac02</a>, that maintains a ring buffer with the 8191 most recent beacon block roots.</p>\n<p>The contract has a get and set function and by using a method like the following one can get the beacon block root identified by timestamp:</p>\n<pre><code class=\"lang-python\">// Example contract for interacting with the beacon roots storage contract (EIP-4788)\ncontract IBeaconRoots  {\n    address beaconRootsContract = 0x000F3df6D732807Ef1319fB7B8bB8522d0Beac02;\n\n    function getRootFromTimestamp(uint256 timestamp) public returns (bytes32) {\n        (bool ret, bytes memory data) = beaconRootsContract.call(bytes.concat(bytes32(timestamp)));\n        return bytes32(data);\n    }\n}\n</code></pre>\n<p>With the historical roots of the beacon blocks inside the EVM, we can create proofs for everything inside a <a href=\"https://eth2book.info/capella/annotated-spec/#beacon-block\" rel=\"noopener nofollow ugc\">beacon block</a>, including the <a href=\"https://eth2book.info/capella/annotated-spec/#beacon-state\" rel=\"noopener nofollow ugc\">beacon state</a>.<br>\nThis includes validator balances, withdrawal credentials, deposits, withdrawals, sync committees and many more. Combining this with SNARKs, one could think of applications that allow you to provide a zk proof proving you’re a validator (having the private key for one of the withdrawal credentials as a private argument) without revealing which one. And this is just one out of many very powerful use cases.</p>\n<h2><a name=\"slashing-proofoor-3\" class=\"anchor\" href=\"https://ethresear.ch#slashing-proofoor-3\"></a>Slashing-Proofoor</h2>\n<p><a href=\"https://github.com/nerolation/slashing-proofoor\" rel=\"noopener nofollow ugc\">Slashing-Proofoor</a> leverages EIP-4788 to allow users to prove that a specified validator hasn’t been slashed.<br>\nThis comes in particularly handy for applications that need that information inside the EVM and are currently dependent on trusted entities to deliver that information.</p>\n<h3><a name=\"how-does-such-a-proof-work-4\" class=\"anchor\" href=\"https://ethresear.ch#how-does-such-a-proof-work-4\"></a>How does such a proof work?</h3>\n<p>First, everything inside a beacon block is basically a root of a merkle tree which itself is composed of the roots of even more merkle trees. This means we can prove ourselves from the bottom to the top, from the individual validator that is deeply nested inside the state, to the top - the beacon block.</p>\n<h4><a name=\"h-1-the-validator-construction-5\" class=\"anchor\" href=\"https://ethresear.ch#h-1-the-validator-construction-5\"></a>(1) The validator construction</h4>\n<p>As a first step, we compute the hash_tree_root of the validator object we’d like to prove. A validator is nothing else than data in the following structure:</p>\n<pre><code class=\"lang-python\">class Validator(Container):\n    pubkey: BLSPubkey\n    withdrawal_credentials: Bytes32  # Commitment to pubkey for withdrawals\n    effective_balance: Gwei  # Balance at stake\n    slashed: boolean\n    # Status epochs\n    activation_eligibility_epoch: Epoch  # When criteria for activation were met\n    activation_epoch: Epoch\n    exit_epoch: Epoch\n    withdrawable_epoch: Epoch  # When validator can withdraw funds \n</code></pre>\n<p>Our goal is to get to the validator’s <code>hash_tree_root</code> which, simply speaking, is hashing together all elements of the respective validator.<br>\nMore precisely, we first compute the <code>hash_tree_root</code> of all elements inside <code>validator</code>:</p>\n<pre><code class=\"lang-python\">hash_tree_roots = [hash_tree_root(elem) for elem in state.validator[...]]\n</code></pre>\n<p>This gives us a list with 8 32-bytes elements.</p>\n<p><strong>Under the hood we do the following:</strong><br>\n<br></p>\n\n<img src=\"https://ethresear.ch/uploads/default/original/3X/1/9/197bcb79dd5f32b8d6174def663169537588a1e2.png\" data-base62-sha1=\"3DrahWksmGgmTFH3YyGFUdlc2RQ\" alt=\"drawing\" width=\"600\" height=\"267\">\n\n<br>\n<br>\n<p>It is worth noting that the validator object contains the boolean <code>slashed</code>. We therefore make sure that our on-chain implementation can validate the <code>slashed</code> status when providing the hash tree roots of our validator to a contract (see first <code>require</code> statement in below code).<br>\nIf the 4th value of <code>hash_tree_roots</code> is equal to <code>bytes32(0)</code>, then this means that the respective validator has not been slashed.</p>\n<h4><a name=\"h-2-the-validators-proof-6\" class=\"anchor\" href=\"https://ethresear.ch#h-2-the-validators-proof-6\"></a>(2) The validators proof</h4>\n<blockquote>\n<p><strong>proof</strong>: <em>\"The constructed validator is part of the validator set.</em>\"</p>\n</blockquote>\n<p>Next, we must be able to proof that the validator that we just provided through an array of <code>hash_tree_roots</code> is actually part of the validator set the beacon state is aware of.<br>\nWe do so through another merkle proof and provide the following inputs:</p>\n<ul>\n<li>The leaf, as the <code>hash_tree_root</code> of <code>hash_tree_roots</code> from the previous step</li>\n<li>The merkle branch/proof</li>\n<li>The <code>index</code> telling where in the merkle tree our validator is located</li>\n<li>The root of the <code>beaconState.validators</code> Merkle Tree we prove against.</li>\n</ul>\n<p>This is super simple and by using the result of (1) inside (2), we chain the two proofs together.</p>\n\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/5/a/5a8f198be2c1f595235b750957c57973321d1959.png\" data-download-href=\"https://ethresear.ch/uploads/default/5a8f198be2c1f595235b750957c57973321d1959\" title=\"drawing\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/5/a/5a8f198be2c1f595235b750957c57973321d1959_2_400x318.png\" data-base62-sha1=\"cV7yRcHcyy7vrAcLs7NHn7wVG4x\" alt=\"drawing\" width=\"400\" height=\"318\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/5/a/5a8f198be2c1f595235b750957c57973321d1959_2_400x318.png, https://ethresear.ch/uploads/default/optimized/3X/5/a/5a8f198be2c1f595235b750957c57973321d1959_2_600x477.png 1.5x, https://ethresear.ch/uploads/default/original/3X/5/a/5a8f198be2c1f595235b750957c57973321d1959.png 2x\" data-dominant-color=\"F0E8DE\"></a></div>\n\n<h4><a name=\"h-3-the-beacon-state-proof-7\" class=\"anchor\" href=\"https://ethresear.ch#h-3-the-beacon-state-proof-7\"></a>(3) The beacon state proof</h4>\n<blockquote>\n<p><strong>proof</strong>: <em>\"The just constructed validator array is part of the state.</em>\"</p>\n</blockquote>\n<p>In this step we take the <code>hash_tree_root</code> of <code>beaconstate.validators</code> and prove that our validators object is indeed part of the state.</p>\n<p>This proof consists of the following:</p>\n<ul>\n<li>The leaf, as the <code>hash_tree_root</code> of the validator list from the previous step</li>\n<li>The merkle branch/proof</li>\n<li>The <code>index</code> telling where in the merkle tree the validators array is located, this is index 11.</li>\n<li>The root of the beacon state we prove against.</li>\n</ul>\n<p>If verified, we have successfully proven that our validators list is part of the state. However, we haven’t proven that this state is correct yet.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/a/1/a12e60426fe0cb496d55513651d67f49e7ac7f05.png\" data-download-href=\"https://ethresear.ch/uploads/default/a12e60426fe0cb496d55513651d67f49e7ac7f05\" title=\"drawing\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/a/1/a12e60426fe0cb496d55513651d67f49e7ac7f05_2_690x293.png\" data-base62-sha1=\"mZSerf2KxEuoqqnNdMUFhxawgLz\" alt=\"drawing\" width=\"690\" height=\"293\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/a/1/a12e60426fe0cb496d55513651d67f49e7ac7f05_2_690x293.png, https://ethresear.ch/uploads/default/optimized/3X/a/1/a12e60426fe0cb496d55513651d67f49e7ac7f05_2_1035x439.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/a/1/a12e60426fe0cb496d55513651d67f49e7ac7f05_2_1380x586.png 2x\" data-dominant-color=\"EEE8E3\"></a></div><p></p>\n<h4><a name=\"h-4-the-beacon-block-proof-8\" class=\"anchor\" href=\"https://ethresear.ch#h-4-the-beacon-block-proof-8\"></a>(4) The beacon block proof</h4>\n<blockquote>\n<p><strong>proof</strong>: <em>\"The derived beacon state root has been part of a canonical block.</em>\"</p>\n</blockquote>\n<p>Finally, we do the same again to prove that the root we provided for (3) is actually the state root part of the beacon block we’re proving against.</p>\n<p>This is yet another merkle proof with the following elements:</p>\n<ul>\n<li>The leaf, as the <code>hash_tree_root</code> of the state from the previous step</li>\n<li>The merkle branch/proof</li>\n<li>The <code>index</code> telling where in the beacon block merkle tree the state array is located, this is index 3.</li>\n<li>The root of the beacon block we prove against. This root is accessible via the EVM.</li>\n</ul>\n<p><strong>This completes our proof.</strong><br>\n<strong>If the final merkle proof is valid, we successfully proved that a certain validator has been slashed (or the opposite).</strong></p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/c/1/c1d1fc9c4cf68eae164111bd734330d41cfac36d.png\" data-download-href=\"https://ethresear.ch/uploads/default/c1d1fc9c4cf68eae164111bd734330d41cfac36d\" title=\"drawing\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/c/1/c1d1fc9c4cf68eae164111bd734330d41cfac36d_2_690x169.png\" data-base62-sha1=\"rEC2SbGRhYNFabqAxmFkrIZxNUh\" alt=\"drawing\" width=\"690\" height=\"169\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/c/1/c1d1fc9c4cf68eae164111bd734330d41cfac36d_2_690x169.png, https://ethresear.ch/uploads/default/optimized/3X/c/1/c1d1fc9c4cf68eae164111bd734330d41cfac36d_2_1035x253.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/c/1/c1d1fc9c4cf68eae164111bd734330d41cfac36d_2_1380x338.png 2x\" data-dominant-color=\"EBE1DF\"></a></div><p></p>\n<p>The solidity contract for constructing the validator object (…and its <code>hash_tree_root</code>) and for verifying the required Merkle proofs may look like the following:</p>\n<pre><code class=\"lang-c\">// SPDX-License-Identifier: MIT\npragma solidity ^0.8.24;\n\n// Import Merkle tree related utilities for efficient data proofs\nimport \"./Merkleizer.sol\";\nimport \"./MerkleTree.sol\";\n\n// Contract for managing slash proofs of validators\ncontract SlashingProofoor is Merkleizer, MerkleProof {\n    uint public constant VALIDATOR_REGISTRY_LIMIT = 2**40;\n    address beaconRootsContract = 0x000F3df6D732807Ef1319fB7B8bB8522d0Beac02;\n    uint256 private constant HISTORY_BUFFER_LENGTH = 8191;\n\n    constructor(bytes32[] memory _zerohashes) Merkleizer(_zerohashes) {}\n\n    function getRootFromTimestamp(uint256 timestamp) public returns (bytes32) {\n        (bool ret, bytes memory data) = beaconRootsContract.call(bytes.concat(bytes32(timestamp)));\n        require(ret);\n        return bytes32(data);\n    }\n\n    /**\n     * @dev Verifies non-slashing proof of a validator using multiple Merkle proofs.\n     * @param blockTimestamp The timestamp of the block related to the proof verification context.\n     * @param validatorChunks Array of data chunks corresponding to the validator's attributes.\n     * @param validatorsProof Merkle proofs for the beaconState.validators list.\n     * @param validatorIndex Index of the validator in the beaconState.validators list.\n     * @param validatorsRoot The root hash of the beaconState.validators tree.\n     * @param beaconStateProof Merkle proof for the beacon state.\n     * @param nr_validators Number of validators in beacon state.\n     * @param beaconStateRoot The beacon state root used for verification.\n     * @param beaconBlockProof Merkle proof for the beacon block.\n     * @return success True if all validations pass, otherwise reverts.\n     */\n    function verifyProof(\n        uint256 blockTimestamp,\n        bytes32[] memory validatorChunks,\n        bytes32[] memory validatorsProof,\n        uint256 validatorIndex,\n        bytes32 validatorsRoot,\n        uint256 nr_validators,\n        bytes32[] memory beaconStateProof,\n        bytes32 beaconStateRoot,\n        bytes32[] memory beaconBlockProof\n    ) public returns (bool success) {\n        // Ensure the validator has not been slashed (the slashing flag in the chunk must be zero)\n        require(validatorChunks[3] != bytes32(0), \"Provided validator chunks indicate non-slashed validator\");\n\n        // Compute the hash tree root of the validator's chunks\n        bytes32 valHashTreeRoot = merkleizeChunks(validatorChunks, 8);\n\n        // Verify the validator's position and inclusion in the state's validator list\n        require(verify(validatorsProof, validatorsRoot, valHashTreeRoot, validatorIndex), \"Validator proof failed\");\n\n        // Calculate the validators hash tree root by mixing in the number of validators\n        bytes32 stateValidatorsHashTreeRoot = mixInLength(validatorsRoot, nr_validators);\n\n        // Verify the hash tree root of validators against the beacon state root\n        require(verify(beaconStateProof, beaconStateRoot, stateValidatorsHashTreeRoot, 11), \"BeaconState validation failed\");\n\n        // Additional verification against the beacon block\n        require(verify(beaconBlockProof, getRootFromTimestamp(blockTimestamp), beaconStateRoot, 3), \"Beaconblock proof failed\");\n        return true;\n    }\n}\n</code></pre>\n<p><em>The code for the verifier contracts is available <a href=\"https://github.com/nerolation/slashing-proofoor\" rel=\"noopener nofollow ugc\">here</a>.</em></p>\n            <p><small>2 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/slashing-proofoor-on-chain-slashed-validator-proofs/19421\">Read full topic</a></p>","link":"https://ethresear.ch/t/slashing-proofoor-on-chain-slashed-validator-proofs/19421","pubDate":"Tue, 30 Apr 2024 09:11:54 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19421"},"source":{"@url":"https://ethresear.ch/t/slashing-proofoor-on-chain-slashed-validator-proofs/19421.rss","#text":"Slashing Proofoor - On-chain slashed validator proofs"},"filter":false},{"title":"Enshrine AI into EVM","dc:creator":"axonon","category":"Layer 2","description":"<blockquote>\n<p>Axonum Sepolia testnet is undergoing beta testing. Check out our <a href=\"https://docs.axonum.io/\" rel=\"noopener nofollow ugc\">documentation</a>.</p>\n</blockquote>\n<h1><a name=\"introducing-axonum-the-brain-of-ethereum-1\" class=\"anchor\" href=\"https://ethresear.ch#introducing-axonum-the-brain-of-ethereum-1\"></a>Introducing Axonum: The Brain of Ethereum</h1>\n<p>Axonum enshrines AI into blockchain to build a decentralized supercomputer powered by global collective intelligence.</p>\n<h2><a name=\"the-age-of-ai-evm-2\" class=\"anchor\" href=\"https://ethresear.ch#the-age-of-ai-evm-2\"></a>The Age of AI EVM</h2>\n<p>We are building Axonum, an AI optimistic rollup, with the world’s first AI EVM.</p>\n<p>We aim to democratize access to AI-powered DApps, making AI model inferences both accessible and user-friendly.</p>\n<p>Axonum is an optimistic rollup with enshrined AI powered by opML and AI EVM. It enables users to seamlessly employ AI models natively within smart contracts without being encumbered by the intricacies of underlying technologies.</p>\n<h2><a name=\"overview-3\" class=\"anchor\" href=\"https://ethresear.ch#overview-3\"></a>Overview</h2>\n<h3><a name=\"ai-evm-enshrined-ai-4\" class=\"anchor\" href=\"https://ethresear.ch#ai-evm-enshrined-ai-4\"></a>AI EVM: Enshrined AI</h3>\n<p>To enable native ML inference in the smart contract, we need to modify the execution layer of the layer 2 chain. Specifically, we add a precompiled contract inference in EVM to build AI EVM.</p>\n<p>AI EVM will conduct the ML inference in native execution and then return deterministic execution results. When a user wants to use the AI model to process data, all the user needs to do is to call the precompiled contract inference with the model address and model input, and then the user can obtain the model output and use it natively in the smart contract.</p>\n<pre data-code-wrap=\"solidity\"><code class=\"lang-plaintext\">import \"./AILib.sol\";\n\ncontract AIContract {\n    ...\n    function inference(bytes32 model_address, bytes memory input_data, uint256 output_size) public {\n        bytes memory output = AILib.inference(model_address, input_data, output_size);\n        emit Inference(model_address, input_data, output_size, output);\n    }\n}\n</code></pre>\n<p>The models are stored in the model data available (DA) layer. All the models can be retrieved from DA using the model address. We assume the data availability of all the models.</p>\n<p>The core design principle of the precompiled contract inference follows the design principles of opML, that is, we separate execution from proving. We provide two kinds of implementation of the precompiled contract inference. One is compiled for native execution, which is optimized for high speed. Another is compiled for the fraud proof VM, which helps prove the correctness of the opML results.</p>\n<p>For the implementation for execution, we re-use the ML engine in opML. We will first fetch the model using the model address from the model hub and then load the model into the ML engine. ML engine will take the user’s input in the precompiled contract as the model input and then execute the ML inference task. The ML engine guarantees the consistency and determinism of the ML inference results using quantization and soft float.</p>\n<p>Besides the current AI EVM design, an alternative approach to enable AI in EVM is adding more machine learning-specific opcodes into EVM, with corresponding changes to the virtual machine’s resource and pricing model as well as the implementation.</p>\n<h3><a name=\"optimistic-rollup-5\" class=\"anchor\" href=\"https://ethresear.ch#optimistic-rollup-5\"></a>Optimistic Rollup</h3>\n<p>opML (Optimistic Machine Learning) and optimistic rollup (opRollup) are both based on a similar fraud-proof system, making it feasible to integrate opML into the Layer 2 (L2) chain alongside the opRollup system. This integration enables the seamless utilization of machine learning within smart contracts on the L2 chain.</p>\n<p>Just like the existing rollup systems, Axonum is responsible for “rolling up” transactions by batching them before publishing them to the L1 chain, usually through a network of sequencers. This mechanism could include thousands of transactions in a single rollup, increasing the throughput of the whole system of L1 and L2.</p>\n<p>Axonum, as one of the optimistic rollups, is an interactive scaling method for L1 blockchains. We optimistically assume that every proposed transaction is valid by default. Different from the traditional L2 optimistic rollup system, the transaction in Axonum can include AI model inferences, which can make the smart contracts on Axonum “smarter” with AI.</p>\n<p>In the case of mitigating potentially invalid transactions, like optimistic rollups, Axonum introduces a challenge period during which participants may challenge a suspect rollup. A fraud-proving scheme is in place to allow for several fraud proofs to be submitted. Those proofs could make the rollup valid or invalid. During the challenge period, state changes may be disputed, resolved, or included if no challenge is presented (and the required proofs are in place).</p>\n<h2><a name=\"workflow-6\" class=\"anchor\" href=\"https://ethresear.ch#workflow-6\"></a>Workflow</h2>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/4/1/41403b199568fcf421dabd3b05c089511968469a.png\" data-download-href=\"https://ethresear.ch/uploads/default/41403b199568fcf421dabd3b05c089511968469a\" title=\"workflow\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/4/1/41403b199568fcf421dabd3b05c089511968469a_2_690x405.png\" alt=\"workflow\" data-base62-sha1=\"9jeEgUnHp8B6gUDrtDfpsaLQAqK\" width=\"690\" height=\"405\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/4/1/41403b199568fcf421dabd3b05c089511968469a_2_690x405.png, https://ethresear.ch/uploads/default/optimized/3X/4/1/41403b199568fcf421dabd3b05c089511968469a_2_1035x607.png 1.5x, https://ethresear.ch/uploads/default/optimized/3X/4/1/41403b199568fcf421dabd3b05c089511968469a_2_1380x810.png 2x\" data-dominant-color=\"F5F5F7\"></a></div><p></p>\n<p>Here’s the essential workflow of Axonum, without considering mechanisms such as pre-confirmation or force exit:</p>\n<ol>\n<li>The basic workflow begins with users sending L2 transactions (we allow native AI inference in the smart contract) to a batcher node, usually the sequencer.</li>\n<li>Once the sequencer receives a certain number of transactions, it will post them into an L1 smart contract as a batch.</li>\n<li>A validator node will read these transactions from the L1 smart contract and execute them on their local copy of the L2 state. As for the AI inference execution, the validator needs to download the model from model DA and conduct the AI inference within the opML engine.</li>\n<li>Once processed, a new L2 state is generated locally and the validator will post this new state root into an L1 smart contract. (Note that this validator can also be the sequencer.)</li>\n<li>Then, all other validators will process the same transactions on their local copies of the L2 state.</li>\n<li>They will compare their resultant L2 state root with the original one posted to the L1 smart contract.</li>\n<li>If one of the validators gets a different state root than the one posted to L1, they can begin a challenge on L1.</li>\n<li>The challenge will require the challenger and the validator who posted the original state root to take turns proving what the correct state root should be. This challenge process is also known as fraud proof. The fraud proof of Axonum includes the fraud proof of L2 state transition and the fraud proof of opML.</li>\n<li>Whichever user loses the challenge, gets their initial deposit (stake) slashed. If the original L2 state root posted was invalid, it will be destroyed by future validators and will not be included in the L2 chain.</li>\n</ol>\n<h2><a name=\"fraud-proof-design-7\" class=\"anchor\" href=\"https://ethresear.ch#fraud-proof-design-7\"></a>Fraud Proof Design</h2>\n<p>The core design principle of the fraud proof system of Axonum is that we separate the fraud proof process of Geth (the Golang implementation of the Ethereum client on layer 2) and the opML. This design ensures a robust and efficient fraud proof mechanism. Here’s a breakdown of the fraud proof system and our separation design:</p>\n<ol>\n<li><strong>Fraud Proof System Overview:</strong>\n<ul>\n<li>The fraud proof system is a critical component that guarantees the security and integrity of transactions on the Axonum optimistic rollup Layer 2.</li>\n<li>It involves the verification of transactions and computations to ensure that any malicious behavior or inaccuracies are detected and addressed.</li>\n</ul>\n</li>\n<li><strong>Separation of Fraud Proof Processes:</strong>\n<ul>\n<li><strong>Geth Fraud Proof Process:</strong>\n<ul>\n<li>Geth, responsible for the Ethereum client on layer 2, handles the initial stages of fraud proof related to transaction validation and basic protocol adherence.</li>\n<li>It verifies the correctness of transactions and ensures that they comply with the rules and protocol of the layer 2 system.</li>\n</ul>\n</li>\n<li><strong>opML Fraud Proof Process:</strong>\n<ul>\n<li>opML, the Optimistic Machine Learning system integrated with Axonum, takes charge of the more intricate aspects of fraud proof related to machine learning model execution.</li>\n<li>It verifies the correctness of machine learning computations and ensures the integrity of AI-related processes within the layer 2 framework.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Benefits of Separation Design:</strong>\n<ul>\n<li><strong>Enhanced Efficiency:</strong>\n<ul>\n<li>By distributing the fraud proof responsibilities, we optimize the efficiency of the overall system. Geth focuses on transactional aspects, while opML handles ML-specific fraud proofs.</li>\n</ul>\n</li>\n<li><strong>Scalability:</strong>\n<ul>\n<li>The separation design allows for scalability, enabling each component to independently scale based on its specific processing requirements.</li>\n</ul>\n</li>\n<li><strong>Flexibility:</strong>\n<ul>\n<li>This separation provides flexibility for upgrades and improvements in either the Geth or opML components without compromising the entire fraud proof system.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2><a name=\"axonum-the-brain-of-ethereum-8\" class=\"anchor\" href=\"https://ethresear.ch#axonum-the-brain-of-ethereum-8\"></a>Axonum: The Brain of Ethereum</h2>\n<p>Axonum is the first AI optimistic rollup that enables AI on Ethereum natively, trustlessly, and verifiably.</p>\n<p>Axonum leverages optimistic ML and optimistic rollup and introduces innovations of AI EVM to add intelligence to Ethereum as a Layer 2.</p>\n<p>We enshrine AI into blockchain to build a decentralized supercomputer powered by global collective intelligence.</p>\n<blockquote>\n<p>Learn more about Axonum: <a href=\"https://docs.axonum.io\" rel=\"noopener nofollow ugc\">https://docs.axonum.io</a><br>\nBridge to Axonum Sepolia: <a href=\"https://app.axonum.io/bridge/deposit\" rel=\"noopener nofollow ugc\">https://app.axonum.io/bridge/deposit</a><br>\nAdd Axonum Sepolia to MetaMask: <a href=\"https://add.axonum.io/?network=testnet\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Add Axonum</a></p>\n</blockquote>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/enshrine-ai-into-evm/19418\">Read full topic</a></p>","link":"https://ethresear.ch/t/enshrine-ai-into-evm/19418","pubDate":"Mon, 29 Apr 2024 05:57:57 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19418"},"source":{"@url":"https://ethresear.ch/t/enshrine-ai-into-evm/19418.rss","#text":"Enshrine AI into EVM"},"filter":false},{"title":"Enabling light node to verify a field element from its column in peer das","dc:creator":"Athos","category":"Sharding","description":"<p>When we finally implements 2D PeerDAS, blob data will be formatted into a data matrix, where each element (aka cell) of the matrix is an array of field elements, with 64 field elements in each array. Peers are required to sample this matrix, with the minimum sampling unit being a cell.</p>\n<p>For nodes with limited resources, such as light nodes, we may allow them to perform sampling in the following manner:</p>\n<ol>\n<li>Light node requests:</li>\n</ol>\n<ul>\n<li>Column index</li>\n<li>Blob index</li>\n<li>Field element index within the cell</li>\n</ul>\n<ol start=\"2\">\n<li>The requested peer’s response:</li>\n</ol>\n<ul>\n<li>The value of the field element</li>\n<li>The KZG commitment of the cell polynomial</li>\n<li>The proof of the field element in the cell polynomial</li>\n<li>The KZG proof of the cell in the blob</li>\n</ul>\n<p>The light node performs verification on the aforementioned content.</p>\n<p>For a KZG multiproof, we have the following:</p>\n<p><span class=\"math\"> Proof_{cell}={\\frac {f(\\alpha)-I(\\alpha)}{z(\\alpha)}}G_1 </span></p>\n<p>The polynomial I(x), which represents the cell polynomial, is obtained by interpolating the field elements within the cell based on their indices within the blob, where <span class=\"math\">I(\\alpha)G_1</span> represents the KZG commitment of the cell polynomial</p>\n<p>For a specific field element within the cell, its proof format is as follows</p>\n<p><span class=\"math\"> Proof_{ele}=\\frac {I(\\alpha)-val_{ele}} {x_{ele}-\\alpha}G_1 </span></p>\n<p>These two proofs are related through <span class=\"math\">I(\\alpha)G_1</span>, which allows us to verify a specific field element within the blob without obtaining the complete cell data.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/enabling-light-node-to-verify-a-field-element-from-its-column-in-peer-das/19411\">Read full topic</a></p>","link":"https://ethresear.ch/t/enabling-light-node-to-verify-a-field-element-from-its-column-in-peer-das/19411","pubDate":"Mon, 29 Apr 2024 03:16:34 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19411"},"source":{"@url":"https://ethresear.ch/t/enabling-light-node-to-verify-a-field-element-from-its-column-in-peer-das/19411.rss","#text":"Enabling light node to verify a field element from its column in peer das"},"filter":false},{"title":"Making Ether A Better Money","dc:creator":"pantheraes","category":"Economics","description":"<h2><a name=\"tldr-1\" class=\"anchor\" href=\"https://ethresear.ch#tldr-1\"></a>TL;DR</h2>\n<p>In its current form, Ether (ETH) is not a good form of money. This is due to one critical limitation: its value is highly unstable. However, ETH can become stable by adjusting the rewards to validators (and thus the supply of ETH) to changes in demand for ETH. We can target a 0% inflation rate while ensuring validators are paid sufficiently to ensure network security. This new monetary policy can be called Stable Ether Monetary Policy (SEMP). With SEMP, ETH holders would have a great currency, and ETH validators would have exposure to the adoption of ETH.</p>\n<h2><a name=\"why-should-eth-be-stable-2\" class=\"anchor\" href=\"https://ethresear.ch#why-should-eth-be-stable-2\"></a>Why should ETH be stable?</h2>\n<p>It is widely accepted that a currency (i.e., a form of money) needs to function as a medium of exchange, a unit of account, and a store of value. ETH has the potential to excel at these functions, and ETH has many large advantages over existing currencies. However, ETH is not a good form of money for one reason: its value is highly unstable. <strong>Instability makes ETH a poor unit of account and a poor store of value.</strong></p>\n<h2><a name=\"why-is-eth-unstable-3\" class=\"anchor\" href=\"https://ethresear.ch#why-is-eth-unstable-3\"></a>Why is ETH unstable?</h2>\n<p>Currently, ETH is simultaneously an investment and a form of money (for more detail, see <a href=\"https://www.bankless.com/ether-a-new-model-for-money\" rel=\"noopener nofollow ugc\">Bankless’ triple point asset thesis</a>). ETH, as an investment, needs to have the potential to increase in value over time (i.e., it must be unstable). But ETH, as a money, needs stable value. <strong>Clearly, ETH cannot be both stable and unstable, and thus it cannot simultaneously be a good investment and a good form of money.</strong> Of course, to date, the value of ETH has varied over time, making it more of an investment than a form of money.</p>\n<h2><a name=\"what-are-the-implications-of-eths-instability-4\" class=\"anchor\" href=\"https://ethresear.ch#what-are-the-implications-of-eths-instability-4\"></a>What are the implications of ETH’s instability?</h2>\n<p>First, <strong>the adoption (and market cap) of ETH is held back by it being a poor form of money.</strong> There seems to be consensus that “monetary premium” (the value of something based on it being a form of money) is a more important driver of ETH’s value than the value it derives from burned ETH in a discounted cash flow (DCF) model. For example, in <a href=\"https://polynya.mirror.xyz/GPC26Y_rlwCyPpj_N3HeW_izY1-pIVwKW5bjuPNrGeQ\" rel=\"noopener nofollow ugc\">polynya’s ranking of the top 10 drivers of ETH demand</a>, four of the top five drivers rely on ETH being a good form of money. <strong>The value of ETH is heavily dependent on it being a good form of money.</strong></p>\n<p>In particular, the value of ETH is held back by it being a poor store of value. The monetary premium from being a good medium of exchange is minimal. When something is a good medium of exchange but not a good store of value, it will be bought for transactions. But it will often be sold by the recipient because it doesn’t store value. <strong>Monetary premium comes from the demand to buy and hold the currency, which requires it being a good store of value.</strong></p>\n<p>The second implication of ETH’s instability is that <strong>liquid staking tokens (LSTs) are threatening ETH as “the de facto money of the network”</strong> (see <a href=\"https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751\">this Ethereum Research post</a>). This “leads to Ethereum users being exposed to counterparty risk inherited by the LST by default.” As explained above, ETH is currently more of an investment than a form of money. And because many view an LST as an even better investment than ETH (more upside with small additional risk), it is no surprise that LSTs could become more popular than ETH itself.</p>\n<p>The third issue is the rise of stablecoins. Stablecoins are useful, but they have limitations. Stablecoins expose users to inflation and leave users dependent on the goodwill of centralized actors (central banks). Even though stablecoins lose value to inflation, they are still seen by many as a better store of value than ETH. This is reasonable; stablecoins’ value is much more stable than ETH’s. Stablecoins also suffer from <a href=\"https://fortunafi.beehiiv.com/p/the-stablecoin-trilemma\" rel=\"noopener nofollow ugc\">a trilemma</a>. <strong>Nobody to date has figured out how to design a stablecoin that is stable, decentralized, and capital efficient.</strong></p>\n<h2><a name=\"what-is-our-current-goal-5\" class=\"anchor\" href=\"https://ethresear.ch#what-is-our-current-goal-5\"></a>What is our current goal?</h2>\n<p><strong>We want to transform ETH into a better form of money while maintaining a way for people to financially benefit from Ethereum’s success.</strong></p>\n<p><strong>We want ETH to have 0% inflation, so that it is a better store of value and unit of account than other currencies.</strong> ETH would be a programmable, credibly neutral, censorship resistant, permissionless, and decentralized form of money that is a great store of value, medium of exchange, and unit of account. <strong>This would increase the market cap of ETH, avoid LSTs becoming the de facto money of the network, and solve the stablecoin trilemma.</strong></p>\n<h2><a name=\"what-is-the-proposed-solution-6\" class=\"anchor\" href=\"https://ethresear.ch#what-is-the-proposed-solution-6\"></a>What is the proposed solution?</h2>\n<p><strong>I think we can achieve our goals by constantly adjusting the ETH supply to target 0% inflation.</strong> As ETH is bought or burned, its price increases. The price increase could be counteracted by inflating the ETH supply in the form of rewards to validators until the price of ETH arrives back at the target 0% inflation. In the reverse direction, as ETH is sold, its price decreases. The price decrease could be counteracted by offering fewer rewards to validators for some time. The overall supply of ETH can decrease from burned ETH until the price of ETH arrives back at the target 0% inflation. We can call this new monetary policy Stable Ether Monetary Policy (SEMP).</p>\n<img src=\"https://ethresear.ch/uploads/default/original/3X/8/7/875db8dd59ac47e5111aa5623ef56dcedfb90aff.svg\" class=\"yuml\" data-dominant-color=\"\" width=\"447\" height=\"460\">\n<h2><a name=\"how-do-we-ensure-validators-will-always-secure-the-network-7\" class=\"anchor\" href=\"https://ethresear.ch#how-do-we-ensure-validators-will-always-secure-the-network-7\"></a>How do we ensure validators will always secure the network?</h2>\n<p>Like today, we would need a minimum issuance curve, which states the minimum rewards to validators depending on the percent of ETH staked. The optimal minimum issuance curve needed to maintain Ethereum’s security would need to be determined by research, just as it is today.</p>\n<p>The rewards should be in units of inflation-adjusted value. If ETH halves in value relative to the global cost of living, the ETH rewards should double. This ensures that we are paying enough for security even if ETH decreases in value.</p>\n<h2><a name=\"would-it-be-better-to-target-some-level-of-deflation-8\" class=\"anchor\" href=\"https://ethresear.ch#would-it-be-better-to-target-some-level-of-deflation-8\"></a>Would it be better to target some level of deflation?</h2>\n<p>Most economists agree that a deflationary currency is harmful to an economy. Deflation disincentives spending and makes debt more expensive to repay. In addition, it may be difficult to maintain both a level of deflation and network security. Lower levels of inflation (or higher levels of deflation) require fewer rewards to validators, but we need validators to receive enough rewards to incentivize them to secure the Ethereum network.</p>\n<h2><a name=\"would-it-be-better-to-target-a-low-level-of-inflation-eg-2-9\" class=\"anchor\" href=\"https://ethresear.ch#would-it-be-better-to-target-a-low-level-of-inflation-eg-2-9\"></a>Would it be better to target a low level of inflation (e.g., 2%)?</h2>\n<p>This would provide more incentive for ETH holders to become validators, providing greater assurance of the security of the network.</p>\n<p>However, one would expect that the increase in the adoption of ETH from targeting 0% rather than 2% inflation will lead to more demand for ETH. This will provide more value for validators and help secure the network. With 0% inflation, I believe ETH validators will be rewarded sufficiently from an increase in demand for ETH as a form of money. Later, ETH could have a high and stable level of adoption. At that time, I believe fee burn would allow us to sufficiently reward validators while still maintaining 0% inflation.</p>\n<p><strong>However, if we discover through research or experience that the minimum issuance curve makes 0% inflation unrealistic, we can adjust our goal to a stable and low level of inflation. In that case, ETH would still be a great currency because it would have low and predictable inflation.</strong></p>\n<h2><a name=\"how-should-we-calculate-inflation-10\" class=\"anchor\" href=\"https://ethresear.ch#how-should-we-calculate-inflation-10\"></a>How should we calculate inflation?</h2>\n<p><a href=\"https://docs.frax.finance/frax-price-index/overview-cpi-peg-and-mechanics\" rel=\"noopener nofollow ugc\">Frax Finance’s FPI</a> is a stablecoin designed to have 0% inflation and it “uses the CPI-U unadjusted 12 month inflation rate reported by the US Federal Government: <a href=\"https://www.bls.gov/news.release/cpi.nr0.htm\" rel=\"noopener nofollow ugc\">https://www.bls.gov/news.release/cpi.nr0.htm</a>. A specialized Chainlink oracle commits this data on-chain immediately after it is publicly released.”</p>\n<p>Rather than relying on an external service (a Chainlink oracle), validators could serve as oracles for inflation data.</p>\n<p>We could also improve the FPI approach by calculating a global measure of inflation, rather than one based on the US Dollar and the US economy. For example, we could calculate global inflation using country price/inflation data provided by <a href=\"https://www.numbeo.com/cost-of-living/\" rel=\"noopener nofollow ugc\">Numbeo</a>, <a href=\"https://www.worldbank.org/en/research/brief/inflation-database\" rel=\"noopener nofollow ugc\">The World Bank</a>, and/or, <a href=\"https://www.imf.org/en/Publications/WEO\" rel=\"noopener nofollow ugc\">the IMF</a>. We could create a global average of inflation, weighted by population.</p>\n<h2><a name=\"when-the-price-of-eth-increases-beyond-the-target-inflation-rate-how-do-we-know-how-much-eth-to-print-ie-reward-to-validators-11\" class=\"anchor\" href=\"https://ethresear.ch#when-the-price-of-eth-increases-beyond-the-target-inflation-rate-how-do-we-know-how-much-eth-to-print-ie-reward-to-validators-11\"></a>When the price of ETH increases beyond the target inflation rate, how do we know how much ETH to print (i.e., reward to validators)?</h2>\n<p>The Ethereum protocol can monitor prices on DEXs. Validators could serve as oracles to provide this data (in addition to inflation data). Validators that provide ETH price data and inflation data would be rewarded more than those that don’t serve as oracles. Validators that provide non-consensus price or inflation data can be penalized. These rewards and penalties can be determined by research.</p>\n<p>When issuance needs to be increased, validators would be rewarded based on a schedule until the price hits the target. The schedule of extra issuance can be determined by research.</p>\n<h2><a name=\"what-if-ethereum-network-activity-becomes-very-low-could-there-be-a-death-spiral-12\" class=\"anchor\" href=\"https://ethresear.ch#what-if-ethereum-network-activity-becomes-very-low-could-there-be-a-death-spiral-12\"></a>What if Ethereum network activity becomes very low? Could there be a death spiral?</h2>\n<p>In this case, little to no ETH would be burned. Assuming no net buy pressure, all ETH rewarded to validators would cause positive inflation (i.e., a decrease in value).</p>\n<p>A death spiral is possible where Ethereum network activity decreases, ETH inflates, more ETH is issued to validators, ETH supply increases, ETH inflates more, there is less demand for ETH as money, network activity decreases, and the cycle repeats (as shown below).</p>\n<img src=\"https://ethresear.ch/uploads/default/original/3X/9/4/9416c5c93b48a5cfaa43650071bfb363cf63a54d.svg\" class=\"yuml\" data-dominant-color=\"\" width=\"212\" height=\"500\">\n<p><strong>The possibility of a death spiral may seem like a major drawback of this new monetary policy (SEMP), but this is no different than today.</strong> If Ethereum network activity slowed greatly with the existing monetary policy, the value of ETH would continue to decrease. This would occur both because ETH would become a less popular form of money and because there is less burned ETH, reducing its value in a DCF model. <strong>The value of ETH depends on people using the Ethereum network or buying it as a store of value. That is true with the existing monetary policy and with SEMP.</strong></p>\n<p><strong>However, there is reason to believe the likelihood of a death spiral is lower with SEMP.</strong> Because ETH would be designed to have stable value, it’s much more likely that it inflates less when network activity decreases. As the value of ETH decreases, the expectation that it will return to the target inflation rate will provide economic incentive for ETH to be purchased at a discount, increasing its value. With the existing monetary policy, ETH is designed to vary in value (as an investment), so there is less of an expectation that ETH will return to higher prices, providing less economic incentive to buy it when its value decreases.</p>\n<p>The threat of a death spiral is not unique to ETH… Any fiat currency can also enter a death spiral. A situation can arise where a currency inflates, people lose trust in the currency as a store of value, people sell more of the currency, inflation increases, and the cycle repeats (as shown below).</p>\n<img src=\"https://ethresear.ch/uploads/default/original/3X/8/5/851388c38df8bf7819ca55972c18c14b5685121a.svg\" class=\"yuml\" data-dominant-color=\"\" width=\"690\" height=\"94\">\n<h2><a name=\"what-if-a-large-amount-of-eth-is-sold-in-a-short-time-period-13\" class=\"anchor\" href=\"https://ethresear.ch#what-if-a-large-amount-of-eth-is-sold-in-a-short-time-period-13\"></a>What if a large amount of ETH is sold in a short time period?</h2>\n<p>Like the scenario above (decreased network activity), a large selling event will cause the price of ETH to decrease, which could cause both of the death spirals shown above.</p>\n<p>However, everything stated above remains true. A large selling event could also cause a death spiral with the existing monetary policy. And there is reason to believe the likelihood of this death spiral is lower with SEMP. As stated above, there will be greater economic incentive to buy ETH at a discount with SEMP.</p>\n<p>However, there are differences with the scenario above (decreased network activity). A large selling event could drop the price of ETH quicker than decreased network activity. But economic incentives make this scenario less likely. The seller will incur large financial losses in the form of slippage. The more they sell, the larger the financial losses will be. This disincentivizes large selling events.</p>\n<h2><a name=\"what-if-the-supply-of-eth-remains-constant-over-a-long-time-period-14\" class=\"anchor\" href=\"https://ethresear.ch#what-if-the-supply-of-eth-remains-constant-over-a-long-time-period-14\"></a>What if the supply of ETH remains constant over a long time period?</h2>\n<p>There will be no value added to ETH from net buying. Thus, to achieve 0% inflation, all validator rewards will need to be offset by the burn. This is likely to be possible at high levels of ETH adoption. However, if the burn doesn’t offset validator rewards, the ETH supply will inflate because the validators still need to be paid enough to secure the network (see the section above about the possibility of a low level of inflation). Inflation above the target rate would decrease the value of ETH, which could cause a death spiral, as described above.</p>\n<p>Again, this is no different than with the current monetary policy. If there is an equal amount of buying and selling of ETH, there are no net inflows, and thus the price of ETH is not increased from buy pressure. In that scenario, ETH rewards to validators are either offset by the burn or the value of ETH will decrease in the long run. This could cause a death spiral, just like with SEMP.</p>\n<h2><a name=\"is-there-something-we-can-do-to-reduce-the-likelihood-of-a-death-spiral-15\" class=\"anchor\" href=\"https://ethresear.ch#is-there-something-we-can-do-to-reduce-the-likelihood-of-a-death-spiral-15\"></a>Is there something we can do to reduce the likelihood of a death spiral?</h2>\n<p>With SEMP, when ETH increases in value above the target rate of inflation, more ETH is rewarded to validators to increase the supply of ETH and return ETH to the target inflation rate. <strong>To decrease the likelihood of a death spiral, we could delay the increase in supply of ETH.</strong> To protect against a large sell event or lull in network activity, the extra ETH above the minimum issuance curve that would have been issued to validators can simply not be issued. In practice, this delay manifests as very minor levels of deflation and small reductions in validator rewards. But it would provide a buffer limiting the chances of a death spiral. The ideal amount of delay/deflation could be researched.</p>\n<p>Note that a buffer like this is not possible with the existing monetary policy. When ETH is bought or burned, there is no way to prevent that demand from being reflected immediately in the price of ETH. <strong>Thus, with the delay proposed above, a death spiral might be less likely under SEMP than under the existing monetary policy.</strong></p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/making-ether-a-better-money/19393\">Read full topic</a></p>","link":"https://ethresear.ch/t/making-ether-a-better-money/19393","pubDate":"Sun, 28 Apr 2024 01:19:31 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19393"},"source":{"@url":"https://ethresear.ch/t/making-ether-a-better-money/19393.rss","#text":"Making Ether A Better Money"},"filter":false},{"title":"Vanilla Based Sequencing","dc:creator":"Perseverance","category":"Layer 2","description":"<p><strong>Authors:</strong> <a href=\"https://twitter.com/GSpasov\" rel=\"noopener nofollow ugc\">George Spasov</a> (LimeChain), <a href=\"https://twitter.com/danielkivanov\" rel=\"noopener nofollow ugc\">Daniel Ivanov</a> (LimeChain)</p>\n<p><strong>Thanks for early feedback:</strong> <a href=\"https://twitter.com/drakefjustin\" rel=\"noopener nofollow ugc\">Justin Drake</a> (Ethereum Foundation), <a href=\"https://twitter.com/Brechtpd\" rel=\"noopener nofollow ugc\">Brecht Devos</a> (Taiko), <a href=\"https://twitter.com/thegaram33\" rel=\"noopener nofollow ugc\">Péter Garamvölgyi</a> (Scroll), <a href=\"https://twitter.com/orbmis\" rel=\"noopener nofollow ugc\">Simon Brown</a> (Linea), <a href=\"https://twitter.com/cooper_kunz\" rel=\"noopener nofollow ugc\">Cooper Kunz</a> (Aztec), <a href=\"https://twitter.com/smpalladino\" rel=\"noopener nofollow ugc\">Santiago Palladino</a> (Aztec), <a href=\"https://twitter.com/IAmNickDodson\" rel=\"noopener nofollow ugc\">Nick Dodson</a> (Fuel)</p>\n<p>This research is only possible thanks to the research grant support from <strong>Ethereum Foundation</strong>.</p>\n<h1><a name=\"tldr-1\" class=\"anchor\" href=\"https://ethresear.ch#tldr-1\"></a>TLDR</h1>\n<p>Vanilla Based Sequencing is a design for decentralised sequencing mechanism for rollups led by the L1 proposers. Its main goals are:</p>\n<ul>\n<li>Offer equal if not better UX compared to centralised sequencing in terms of preconfirmation time, guarantees and cost of transaction</li>\n<li>Enable the participating actors, including the rollup protocol itself, to generate revenue</li>\n<li>Enable composability with other rollups without the requirement to trust a single centralised sequencing layer</li>\n<li>Base its liveness guarantees on the Ethereum L1.</li>\n<li>Remove the need for trust towards a single party and ensure a sizable and diverse set of sequencers, so that within an acceptable timeframe a censored transaction will reach an honest sequencer that will be willing to sequence this transaction.</li>\n<li>Addresses cold-start problem of original <a href=\"https://ethresear.ch/t/based-rollups-superpowers-from-l1-sequencing/15016\">based sequencing</a> through naturally higher incentives for early participants.</li>\n</ul>\n<p>In the design, L2 sequencing is performed by L1 proposers who have opted-in to participate as L2 sequencers and be punished for misbehaviour. The default L2 sequencer is the current slot L1 proposer. If the current L1 proposer has not opted-in to be a part of the rollup protocol, another opted-in L1 proposer is chosen at random to replace them.</p>\n<p>The L2 sequencers have monopoly power over the rollup sequences during this L1 slot and can provide services (i.e. preconfirmations) to the users with quality on par with centralised sequencing.</p>\n<h1><a name=\"goals-of-decentralised-sequencing-2\" class=\"anchor\" href=\"https://ethresear.ch#goals-of-decentralised-sequencing-2\"></a>Goals of Decentralised Sequencing</h1>\n<p>The majority of the current Ethereum rollup landscape consists of rollups with centralised and/or permissioned sequencing. These centralised sequencing layers both introduce trust assumptions and improve user experience.</p>\n<p>The goal of a decentralised sequencing protocol would be to address the negatives, while maintaining or improving the positives that centralised sequencing offers.</p>\n<h2><a name=\"secure-without-ux-sacrifices-3\" class=\"anchor\" href=\"https://ethresear.ch#secure-without-ux-sacrifices-3\"></a>Secure without UX Sacrifices</h2>\n<p>Current rollups with centralised sequencers offer superior UX compared to both L1 and rollups with decentralised sequencers.</p>\n<p>Firstly, due to their constant monopoly, they offer a quick preconfirmation of inclusion and execution. This is now the standard that the users should come to expect for their transactions.</p>\n<p>Secondly, the rollups with currently centralised sequencers fulfil the rollup-centric future promise offering orders of magnitude cheaper transactions compared to L1.</p>\n<p>A goal for decentralised sequencing design would be to enable a rollup protocol to offer equal if not better UX compared to centralised sequencing in terms of preconfirmation time, guarantees and cost of transaction.</p>\n<h2><a name=\"economically-sustainable-4\" class=\"anchor\" href=\"https://ethresear.ch#economically-sustainable-4\"></a>Economically Sustainable</h2>\n<p>Current rollups with centralised sequencer generates revenue from the transactions fees in their rollups.</p>\n<p>A goal for a decentralised sequencing design would be to enable all the participating actors, including the rollup protocol itself, to be profitable.</p>\n<h2><a name=\"composability-over-walled-gardens-5\" class=\"anchor\" href=\"https://ethresear.ch#composability-over-walled-gardens-5\"></a>Composability over Walled Gardens</h2>\n<p>Rollups with centralised sequencing cannot offer synchronous composability with other rollups and are forced to remain fragmented unless they opt into a single centralised shared sequencer. This can lead to multiple rollups having a single trusted centralised sequencer and further exacerbating the downsides of centralised sequencing.</p>\n<p>A goal for a decentralised sequencing design would be to enable composability with other rollups without the requirement to trust a single centralised sequencing layer.</p>\n<h2><a name=\"inheriting-ethereum-liveness-over-external-liveness-guarantees-6\" class=\"anchor\" href=\"https://ethresear.ch#inheriting-ethereum-liveness-over-external-liveness-guarantees-6\"></a>Inheriting Ethereum Liveness over External Liveness Guarantees</h2>\n<p>Rollups with centralised sequencing rely on a single centralised sequencer to be live in order for the system to be operational.</p>\n<p>A goal for decentralised sequencing design would be to derive its liveness guarantees from the Ethereum L1 liveness guarantees.</p>\n<h2><a name=\"protocol-over-trust-based-censorship-resistance-7\" class=\"anchor\" href=\"https://ethresear.ch#protocol-over-trust-based-censorship-resistance-7\"></a>Protocol over Trust based Censorship Resistance</h2>\n<p>Rollups with centralised sequencing requires the users to trust them that they will not censor them. This makes the centralised sequencer a single point of failure and subjects the rollup, among other things to geopolitical risks.</p>\n<p>A goal for a decentralised sequencing design would be to remove the need of trust towards a single party and ensure a sizable and diverse set of sequencers, so that within an acceptable timeframe censored transactions will reach an honest sequencer that will be willing to sequence this transaction.</p>\n<h1><a name=\"types-of-decentralised-sequencing-8\" class=\"anchor\" href=\"https://ethresear.ch#types-of-decentralised-sequencing-8\"></a>Types of Decentralised Sequencing</h1>\n<p>The major decision of a decentralised sequencing design is the selection of the next L2 sequencer(s). Two groups of design ideas are currently being explored by the rollup teams - “Free for all” and “Leader election”.</p>\n<p>Free for all designs see the role of the sequencer as completely open at any time. In “Free for all” sequencing any user can act as sequencer and submit a sequencing transaction in L1. The order of inclusion in the L1 block of the (possibly multiple) sequencing transaction is decided by the L1 proposer and the L1 block building pipeline.</p>\n<p>The leader election design sees a single sequencer be elected to have monopoly over the sequencing rights for some timeframe (usually denoted in L1 blocks). Two subgroups of leader election designs ideas are being explored by the various rollup researchers - “External Sequencing” and “Based Sequencing”.</p>\n<p>External sequencing sees the leader election to be performed through an external to L1 consensus algorithm. The rollup has its own set of participants that have opted to be part of the consensus algorithm (with its crypto-economical incentives - i.e. staking) for selection of the L2 sequencer.</p>\n<p>Based sequencing sees the leader role (L2 sequencer) be assigned to the current L1 proposer. In order to be part of the rollup protocol selection process the L1 proposers need to opt-in for additional slashing conditions for their L1 stake.</p>\n<h1><a name=\"vanilla-based-sequencing-9\" class=\"anchor\" href=\"https://ethresear.ch#vanilla-based-sequencing-9\"></a>Vanilla Based Sequencing</h1>\n<p>This document outlines a design iteration over the based sequencing concept that aims to fulfil all the outlined goals of decentralised sequencing. The main difference between the original based sequencing concept and the vanilla based sequencing concept is the protocol behaviour when the current L1 slot proposer has not opted-in to be an L2 sequencer.</p>\n<p>An important design consideration aimed by the design is to address the “cold start” problem - achieve the desired goals of decentralised sequencing even early when the participation of L1 proposers is expected to be low.</p>\n<p>The following sections will review the three crucial design suggestions of “vanilla based sequencing” design.</p>\n<h2><a name=\"l2-sequencer-selection-10\" class=\"anchor\" href=\"https://ethresear.ch#l2-sequencer-selection-10\"></a>L2 Sequencer Selection</h2>\n<p>Vanilla based sequencing L2 sequencer selection starts with the same general idea as original based sequencing - that the L2 sequencer is the L1 proposer. For a L1 proposer to become eligible to be a certain rollup L2 sequencer, the L1 proposers would need to opt into slashing conditions - punishment for misbehaviour as a sequencer in the rollup.</p>\n<p>The design recognises that only a subset of the L1 proposers would opt-in to be L2 sequencer for the rollup. This nuance requires the mechanism to define the sequencer selection in the two possible scenarios:</p>\n<ol>\n<li><strong>Primary selection</strong> - When the current L1 proposer has opted-in to be a L2 sequencer for the rollup (depicted in green)</li>\n<li><strong>Fallback selection</strong> - When the current L1 proposer has <strong>not</strong> opted-in to be a L2 sequencer for the rollup (depicted in blue)</li>\n</ol>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/6/9/69c5d356bf06fae2d6c3594caada9f40161b067a.png\" data-download-href=\"https://ethresear.ch/uploads/default/69c5d356bf06fae2d6c3594caada9f40161b067a\" title=\"Preconfirmations-Vanilla Based Sequencing.drawio\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/6/9/69c5d356bf06fae2d6c3594caada9f40161b067a_2_590x500.png\" alt=\"Preconfirmations-Vanilla Based Sequencing.drawio\" data-base62-sha1=\"f5HYmM6JMu9t6w1komnk7X93JF0\" width=\"590\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/6/9/69c5d356bf06fae2d6c3594caada9f40161b067a_2_590x500.png, https://ethresear.ch/uploads/default/original/3X/6/9/69c5d356bf06fae2d6c3594caada9f40161b067a.png 1.5x, https://ethresear.ch/uploads/default/original/3X/6/9/69c5d356bf06fae2d6c3594caada9f40161b067a.png 2x\" data-dominant-color=\"F7F8F7\"></a></div><p></p>\n<h3><a name=\"primary-selection-11\" class=\"anchor\" href=\"https://ethresear.ch#primary-selection-11\"></a>Primary Selection</h3>\n<p>In the <strong>primary selection</strong> case the current L1 proposer has opted in to be a L2 sequencer (depicted in green above). The L1 proposer is automatically assigned as L2 sequencer for the duration of this slot by the rollup protocol. During this period, this proposer is able to provide the best security and timeliness guarantees - that the sequencing transaction(s) will get included in this block and exactly in this block (if no reorgs happen). As L2 sequencer the L1 proposer is able to get additional revenue from the transaction fees and extracted value from the rollup transactions.</p>\n<h3><a name=\"fallback-selection-12\" class=\"anchor\" href=\"https://ethresear.ch#fallback-selection-12\"></a>Fallback Selection</h3>\n<p>One drawback of the original based sequencing concept is the appearance of “sequencing gaps” - an L1 slot whose proposer has not opted-in to be L2 sequencer. These gaps lead to an unpredictably long sequencing time - the next opted-in L1 proposer might be beyond the current L1 lookahead. Such gaps result in rollup service degradation.</p>\n<p>In the <strong>fallback selection</strong> case the current L1 proposer has <strong>not</strong> opted in to be a L2 sequencer (depicted in blue above). In order to fight sequencing gaps, an L1 proposer is drawn at random from the other opted-in L1 proposers and is assigned to be the L2 sequencer. As L2 sequencer the L1 proposer is able to get additional revenue from the transaction fees and extracted value from the rollup transactions.</p>\n<p>Unlike the primary selection case, the L2 sequencer no longer has monopoly over the L1 slot too. This necessitates that the L2 sequencer employs strategies to maximise the chances of inclusion of their sequencing transaction in the assigned slot and exactly in the assigned slot. Such strategies might include, but are not limited to, sending the sequencing transaction to the public L1 transactions mempool with an increased base tip.</p>\n<h3><a name=\"rollup-slot-sequencing-frequency-13\" class=\"anchor\" href=\"https://ethresear.ch#rollup-slot-sequencing-frequency-13\"></a>Rollup Slot &amp; Sequencing Frequency</h3>\n<p>In the diagram above we’ve equated one rollup slot - the time that a single L2 sequencer has monopoly rights of sequencing - to one L1 slot.</p>\n<p>Depending on the usage and mechanics of the rollups, some rollups might want to temporarily or permanently lower the sequencing frequency by adjusting the rollup size to multiple L1 slots.</p>\n<p>In a rollup slot spanning multiple L1 slots, the sequencer selection criteria stay the same but are applied only considering the last L1 slot of the rollup slot.</p>\n<h3><a name=\"requirement-for-punishment-mechanism-in-the-protocol-14\" class=\"anchor\" href=\"https://ethresear.ch#requirement-for-punishment-mechanism-in-the-protocol-14\"></a>Requirement for Punishment Mechanism in the protocol</h3>\n<p>Similarly to any type of sequencing, vanilla based sequencing requires the sequencers to be punished for misbehaviour. Such punishment mechanism can vary and is a design decision of the rollup protocol. Several popular options are:</p>\n<ul>\n<li><strong>Staking</strong> - requiring the l2 sequencers to post a stake that is slashed on misbehaviour. Flavours of this can be restaking - putting forward your ETH validator stake or delegated staking - allowing other users to post stake on the sequencer behalf.</li>\n<li><strong>Bonding</strong> - requiring the l2 sequencers post a stake every time they sequence and getting it back upon finality is reached and no misbehaviour is detected.</li>\n</ul>\n<p>In the context of vanilla based sequencing, the only important requirement is for such a punishment mechanism to exist in order to disincentivise the sequencers to misbehave.</p>\n<h3><a name=\"selection-mechanics-15\" class=\"anchor\" href=\"https://ethresear.ch#selection-mechanics-15\"></a>Selection Mechanics</h3>\n<p>Most of the selection mechanics are performed off-chain and are verified on-chain. This verification is part of the onchain sequencing process and is part of the logic of either the smart contract that deals with sequencing for this rollup or the finality mechanism.</p>\n<p>First option is to have the selection performed in the L1 rollup smart contracts. The contracts need to verify the eligibility of the sequencer to be the current L2 sequencer. Due to the lack of access of L1 execution layer to the <strong>current</strong> L1 proposer, this check needs to be performed as a separate transaction in subsequent L1 block, or an optimistic challenge mechanism needs to be employed.</p>\n<p>Second option is for the rollup to include verification of the correct selection of the sequencer within their finality mechanism - validity or fraud proof.</p>\n<p>In both cases, the selection can be known in advance based on the current L1 lookahead, and can be efficiently verified offchain.</p>\n<p>The mechanisms of opting-in and selection verification are the subject of an <a href=\"https://github.com/LimeChain/based-preconfirmations-research/blob/732cb92474554c2529aabc61e83b8f0934ce6adf/docs/optin-mechanics.md\" rel=\"noopener nofollow ugc\">additional research document</a>.</p>\n<h3><a name=\"transaction-list-building-delegation-16\" class=\"anchor\" href=\"https://ethresear.ch#transaction-list-building-delegation-16\"></a>Transaction List Building Delegation</h3>\n<p>Being a sequencer for one or multiple rollups increases the sophistication requirements of the opted in L1 proposers. An important factor for the quality of service for any vanilla based sequencing rollup is the high L1 proposers participation rate. Increased L1 proposer sophistication requirements and high L1 proposers participation are at a proverbial “tug of war”.</p>\n<p>To combat this war a transaction list building delegation mechanism is proposed on top of vanilla based sequencing rollup. <strong>The vanilla based sequencing design can be fully functional without this delegation mechanism</strong>, but will require increased sophistication of the L1 proposers.</p>\n<p>In a MEV-boost manner, the opted-in L1 proposers are offered the ability to delegate their transaction list building to a secondary block-building pipeline.</p>\n<p>The list building pipeline is a subject of an <a href=\"https://github.com/LimeChain/based-preconfirmations-research/blob/732cb92474554c2529aabc61e83b8f0934ce6adf/docs/pipelines.md\" rel=\"noopener nofollow ugc\">additional research document</a>.</p>\n<h2><a name=\"preconfirmations-17\" class=\"anchor\" href=\"https://ethresear.ch#preconfirmations-17\"></a>Preconfirmations</h2>\n<p>Current centralised sequencer rollups offer a superior UX compared to L1 and decentralised sequencing designs. Such a UX is now becoming a minimum standard expected by the users. One major component of this UX is the ability to quickly pre-confirm the inclusion and/or execution of a transaction to its sender. It is a naturally important requirement for the vanilla based sequencing to strive to reach and surpass the expected UX.</p>\n<p>Two types of preconfirmations are expected of the system.</p>\n<p>First type of preconfirmation is transaction <strong>inclusion</strong> preconfirmation. This preconfirmation guarantees the inclusion of a transaction in the subsequent rollup slot. These are useful for use cases like simple transfers.</p>\n<p>Second type of preconfirmation is the stronger <strong>execution state</strong> preconfirmation. It allows specifying the desired values of parts of the state of the rollup pre or post execution of the transaction. These are useful for more complex use cases like DEX trades and/or arbitrage.</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/3X/e/2/e2f50f004cff46ee36b555ec23c92fe37393a9a2.png\" alt=\"Preconfirmation Flow - Simple\" data-base62-sha1=\"wnKWwwZSZIpoba5sLvSLbAWGA8i\" width=\"581\" height=\"271\"></p>\n<p>Both preconfirmation types require the sequencer to commit to the inclusion of a certain user transaction. The main difference comes in the ordering of the transactions. Within the context of the sequence, the inclusion of preconfirmed transaction can be located anywhere in the sequence. The state preconfirmation transaction requires a specific ordering of the transactions list up to this transaction.</p>\n<p>Inclusion preconfirmations require simple checks of transaction validity - account balance, nonce, etc.</p>\n<p>Execution state preconfirmation requires more sophistication to commit and price. Transactions prior to the target one can change the pre-execution state and make the desired post-state invalid, thus rendering the whole preconfirmation invalid. In practice this means that the sequencer must maintain and commit to an ordered list of transaction at the top of the list.</p>\n<p>To increase the UX usability and enable wallets to hide away the complexity of retries in case of rejection or preconfirmation reneging, a <code>deadline</code> field is suggested. Such a field enables wallets to retry without the user re-signing the transaction.</p>\n<p>Both types of preconfirmations post a certain constraint on the transaction list that the sequencer can sequence. Such constraints limit to various degrees the value that the sequencer can extract from the sequence. Therefore both preconfirmations require additional payment by the user to the proposer in exchange for their guarantee.</p>\n<p>The mechanics of preconfirmations and their pricing are the subject of a <a href=\"https://github.com/LimeChain/based-preconfirmations-research/blob/732cb92474554c2529aabc61e83b8f0934ce6adf/docs/preconfirmations-for-vanilla-based-rollups.md\" rel=\"noopener nofollow ugc\">separate research document</a>.</p>\n<h2><a name=\"rollups-revenue-18\" class=\"anchor\" href=\"https://ethresear.ch#rollups-revenue-18\"></a>Rollup’s Revenue</h2>\n<p>Transaction fees and MEV are the two major value sources of rollup. All of them are captured at sequencing time (assuming the sequenced transactions can be finalised).</p>\n<p>To ensure that the protocol is generating revenue and is not forced into altruism, a portion of the sequencing revenue is suggested to be captured by the rollup. The specific proportions and mechanics are design decisions of the rollup protocols themselves.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/0/6/06e63d56eeaa90af8ccb8d83fd2d7a7e5996f606.png\" data-download-href=\"https://ethresear.ch/uploads/default/06e63d56eeaa90af8ccb8d83fd2d7a7e5996f606\" title=\"sequencing-design-space-Value Flow.drawio\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/0/6/06e63d56eeaa90af8ccb8d83fd2d7a7e5996f606_2_690x286.png\" alt=\"sequencing-design-space-Value Flow.drawio\" data-base62-sha1=\"Z29m5qctMJlnKrihVL3COlUGkS\" width=\"690\" height=\"286\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/0/6/06e63d56eeaa90af8ccb8d83fd2d7a7e5996f606_2_690x286.png, https://ethresear.ch/uploads/default/original/3X/0/6/06e63d56eeaa90af8ccb8d83fd2d7a7e5996f606.png 1.5x, https://ethresear.ch/uploads/default/original/3X/0/6/06e63d56eeaa90af8ccb8d83fd2d7a7e5996f606.png 2x\" data-dominant-color=\"F7F7F8\"></a></div><p></p>\n<p>A simple example mechanism can see the rollup embedding a commission fee <strong>Z%</strong> over the L2 sequencer balance increase. <strong>Such commission fee is aligned with the success of the protocol, as more transactions indicate high quality service and lead to increased revenue for both the rollup and the sequencers.</strong></p>\n<h2><a name=\"universal-synchronous-composability-19\" class=\"anchor\" href=\"https://ethresear.ch#universal-synchronous-composability-19\"></a>Universal Synchronous Composability</h2>\n<p>The rollup design naturally lends itself to become part of a wider universal synchronous composability (USC) mechanism.</p>\n<p>Assuming multiple rollups using the vanilla based sequencing design, USC can be achieved for the L1 slots whose proposer has opted-in to be L2 sequencer to two or more rollups. In these slots the L1 proposer becomes a shared L2 sequencer. This shared L2 sequencer can offer additional cross-rollup services like atomic messaging and super transactions.</p>\n<h1><a name=\"sequencer-violations-20\" class=\"anchor\" href=\"https://ethresear.ch#sequencer-violations-20\"></a>Sequencer Violations</h1>\n<p>Regardless of the selection type, the sequencers have several ways to violate the protocol. In order to disincentivise the violations and misbehaviour the rollups are required to embed punishment mechanisms (discussed in the Sequencer Selection section). In case of violations, the sequencers are expected to be punished.</p>\n<p>Below you can find a short list of violations and faults applicable specifically to vanilla based sequencing that the actors are expected to be punished for. This is list is by no means exhaustive and rollups should adjust it to their specific design.</p>\n<h2><a name=\"sequencer-liveness-and-timeliness-faults-21\" class=\"anchor\" href=\"https://ethresear.ch#sequencer-liveness-and-timeliness-faults-21\"></a>Sequencer Liveness and Timeliness Faults</h2>\n<p>This violation is characterised by the L2 sequencer missing to get L1 sequencing transaction included within their rollup slot. This fault can be objectively proven by the L1 smart contract.</p>\n<p>It is important to note that the cause in <strong>primary</strong> sequencing can only be attributed to the L1 proposer or its delegation pipeline. Within the context of <strong>fallback</strong> sequencing the liveness fault can be caused by the L2 sequencer’s inability to guarantee the inclusion of the sequencing transaction on time due to a lack of monopoly over the L1 slot.</p>\n<p>This difference might change the severity of the punishment of the L2 sequencer. Furthermore, this is a risk that the fallback sequencers must account for and mitigate as much as possible - for example through increased L1 base tip on the sequencing transaction.</p>\n<h2><a name=\"preconfirmation-reneging-22\" class=\"anchor\" href=\"https://ethresear.ch#preconfirmation-reneging-22\"></a>Preconfirmation Reneging</h2>\n<p>This violation is characterised by the L2 sequencer reneging on their preconfirmation commitment. The specific way it can be proven to the L1 smart contracts is a design decision of the rollup but <a href=\"https://github.com/LimeChain/based-preconfirmations-research/blob/732cb92474554c2529aabc61e83b8f0934ce6adf/docs/preconfirmations-for-vanilla-based-rollups.md#preconfirmation-commitment\" rel=\"noopener nofollow ugc\">using a signed commitment is strongly suggested</a>.</p>\n<h1><a name=\"requirements-to-be-vanilla-based-rollup-23\" class=\"anchor\" href=\"https://ethresear.ch#requirements-to-be-vanilla-based-rollup-23\"></a>Requirements to be Vanilla Based Rollup</h1>\n<p>The following are two lists to help the reader to differentiate what is strongly required for a rollup to be considered using vanilla based sequencing, and what is a design decision to be made by the rollup. Both lists are likely to change over time and are subject to community consensus.</p>\n<h2><a name=\"minimal-viable-vanilla-based-sequencing-requirements-24\" class=\"anchor\" href=\"https://ethresear.ch#minimal-viable-vanilla-based-sequencing-requirements-24\"></a>Minimal Viable Vanilla Based Sequencing Requirements</h2>\n<ul>\n<li>The sequencer selection consists of the primary selection of the current L1 proposer - if opted in - and fallback selection among other opted-in L1 proposers - if not.</li>\n<li>Ability to provide inclusion preconfirmations</li>\n<li>Allows for revenue generation of the participating actors</li>\n</ul>\n<h2><a name=\"rollup-design-decisions-25\" class=\"anchor\" href=\"https://ethresear.ch#rollup-design-decisions-25\"></a>Rollup Design Decisions</h2>\n<ul>\n<li>Sequencing Frequency</li>\n<li>Punishment mechanism for sequencer violations</li>\n<li>Support for state preconfirmations</li>\n<li>Mechanism for discovering violations</li>\n<li>Source of revenue for the rollup protocol</li>\n<li>Deposit, finality and withdrawal mechanisms</li>\n</ul>\n<h1><a name=\"goals-achievement-analysis-26\" class=\"anchor\" href=\"https://ethresear.ch#goals-achievement-analysis-26\"></a>Goals Achievement Analysis</h1>\n<h2><a name=\"secure-without-ux-sacrifices-27\" class=\"anchor\" href=\"https://ethresear.ch#secure-without-ux-sacrifices-27\"></a>Secure without UX Sacrifices</h2>\n<p>Vanilla based sequencing increases the security of the rollup protocol through decentralisation of the sequencers group. By involving the L1 proposers as L2 sequencers the design provides the highest possible timeliness guarantees.</p>\n<p>A major focus of vanilla based sequencing and the ability to support UX on par if not better than centralised sequencing. This is achieved through preconfirmations and enablement of composability with other rollups.</p>\n<h2><a name=\"economically-sustainable-28\" class=\"anchor\" href=\"https://ethresear.ch#economically-sustainable-28\"></a>Economically Sustainable</h2>\n<p>No actor in the vanilla based sequencing design is asked to be altruistic. All actors, including the rollup protocol itself, generate revenue for the services they are providing.</p>\n<h2><a name=\"composability-over-walled-gardens-29\" class=\"anchor\" href=\"https://ethresear.ch#composability-over-walled-gardens-29\"></a>Composability over Walled Gardens</h2>\n<p>Vanilla based sequencing is a neutral design concept that is easily extendable to synchronous composability. Due to the reuse of L1 proposer as L2 sequencer, the L1 proposer can enable composability between rollups.</p>\n<h2><a name=\"inheriting-ethereum-liveness-over-external-liveness-guarantees-30\" class=\"anchor\" href=\"https://ethresear.ch#inheriting-ethereum-liveness-over-external-liveness-guarantees-30\"></a>Inheriting Ethereum Liveness over External Liveness Guarantees</h2>\n<p>The liveness guarantees of any rollup comes from the liveness of its sequencers as a whole. Due to the reuse of L1 proposer as L2 sequencer, the vanilla based sequencing concept inherits the Ethereum liveness guarantees.</p>\n<h2><a name=\"protocol-over-trust-based-censorship-resistance-31\" class=\"anchor\" href=\"https://ethresear.ch#protocol-over-trust-based-censorship-resistance-31\"></a>Protocol over Trust based Censorship Resistance</h2>\n<p>Rollups with centralised sequencing requires the users to trust them that they will not censor them. This makes the centralised sequencer a single point of failure and subjects the rollup, among other things to geopolitical risks.</p>\n<p>Unlike centralised sequencing, vanilla based sequencing rollup censorship resistance increases with the increase of the set of L1 proposers opting in to be L2 sequencers. Due to their equivalence, the trust assumptions towards the L2 sequencers are similar to the ones placed on L1 proposers themselves. The diversity of the L1 proposers group and the clear economic incentives to opt-in makes no single sequencer a long-term single point of failure and lowers geopolitical and technological risks.</p>\n<h1><a name=\"further-research-resources-32\" class=\"anchor\" href=\"https://ethresear.ch#further-research-resources-32\"></a>Further Research Resources</h1>\n<ul>\n<li><a href=\"https://github.com/LimeChain/based-preconfirmations-research/blob/732cb92474554c2529aabc61e83b8f0934ce6adf/docs/preconfirmations-for-vanilla-based-rollups.md\" rel=\"noopener nofollow ugc\">Preconfirmations - mechanics, pricing</a></li>\n<li><a href=\"https://github.com/LimeChain/based-preconfirmations-research/blob/732cb92474554c2529aabc61e83b8f0934ce6adf/docs/optin-mechanics.md\" rel=\"noopener nofollow ugc\">L2 Sequencer Opt-in Mechanics, Sequencer Discovery, L2 Sequencer Communication</a></li>\n<li><a href=\"https://github.com/LimeChain/based-preconfirmations-research/blob/732cb92474554c2529aabc61e83b8f0934ce6adf/docs/pipelines.md\" rel=\"noopener nofollow ugc\">L1 PBS pipeline required modifications</a></li>\n</ul>\n<p><a href=\"https://hackmd.io/JeLKP_pVQMe-_J-kpb3MkQ\" rel=\"noopener nofollow ugc\"><img src=\"https://ethresear.ch/uploads/default/original/3X/5/d/5d16b4aeb4937574bedd09ad574ceea3125f7e0b.svg\" alt=\"hackmd-github-sync-badge\" data-base62-sha1=\"dhv3cFMLVDV1RHyhG3pl4ZFkg8b\" width=\"147\" height=\"20\"></a></p>\n            <p><small>3 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/vanilla-based-sequencing/19379\">Read full topic</a></p>","link":"https://ethresear.ch/t/vanilla-based-sequencing/19379","pubDate":"Fri, 26 Apr 2024 14:22:38 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19379"},"source":{"@url":"https://ethresear.ch/t/vanilla-based-sequencing/19379.rss","#text":"Vanilla Based Sequencing"},"filter":false},{"title":"Uncrowdable Inclusion Lists: The Tension between Chain Neutrality, Preconfirmations and Proposer Commitments","dc:creator":"Julian","category":"Economics","description":"<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/1/d/1d5c2ab8b7b15232b55c806bcfbb744f02dab255.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/1d5c2ab8b7b15232b55c806bcfbb744f02dab255\" title=\"Screenshot 2024-04-25 at 10.49.52\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/1/d/1d5c2ab8b7b15232b55c806bcfbb744f02dab255_2_504x500.jpeg\" alt=\"Screenshot 2024-04-25 at 10.49.52\" data-base62-sha1=\"4bJjqecFhwQCsBgPuTZeAGHD4gJ\" width=\"504\" height=\"500\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/1/d/1d5c2ab8b7b15232b55c806bcfbb744f02dab255_2_504x500.jpeg, https://ethresear.ch/uploads/default/optimized/3X/1/d/1d5c2ab8b7b15232b55c806bcfbb744f02dab255_2_756x750.jpeg 1.5x, https://ethresear.ch/uploads/default/optimized/3X/1/d/1d5c2ab8b7b15232b55c806bcfbb744f02dab255_2_1008x1000.jpeg 2x\" data-dominant-color=\"A2B4C4\"></a></div><p></p>\n<p>By <a class=\"mention\" href=\"https://ethresear.ch/u/julian\">@Julian</a>, <a class=\"mention\" href=\"https://ethresear.ch/u/barnabe\">@barnabe</a> and <a class=\"mention\" href=\"https://ethresear.ch/u/soispoke\">@soispoke</a></p>\n<p>Validators are the most decentralized set of participants at any level of the Ethereum protocol and infrastructure. This is a design goal of Ethereum because only then can the protocol leverage this decentralization to obtain protocol resilience:</p>\n<ul>\n<li>As consensus service participants, a decentralized set of validators ensures resilience against correlated failures, either accidental (e.g., due to bugs taking offline a certain share of the validator set) or malicious (e.g., a share of the validator set producing a safety fault).</li>\n<li>As block producers, a decentralized set of validators creates resilience against extractive cartels, which may be looking for undue rents, e.g., via censoring or re-ordering.</li>\n</ul>\n<p>This post investigates the second role of validators and their duties as block producers. First, we explore how the protocol can leverage the decentralized validator set to uphold a concept we call chain neutrality. Then, we discuss how preconfirmations and PEPC may crowd out an in-protocol inclusion list from upholding chain neutrality, thereby introducing a property of inclusion lists that is the main topic of this post: uncrowdable inclusion lists. Finally, we compare unconditional inclusion lists with an instance of PEPC and investigate who should receive tips associated with transactions that are executed via the inclusion list.</p>\n<h2><a name=\"towards-recovering-chain-neutrality-1\" class=\"anchor\" href=\"https://ethresear.ch#towards-recovering-chain-neutrality-1\"></a>Towards recovering chain neutrality</h2>\n<p>In the past, validators (and miners before them) engaged <em>passively</em> both as <em>builders</em>, collecting transactions and sequencing them in blocks using simple heuristics, and as <em>proposers</em>, signing the block and gossiping it over the network.</p>\n<p>Maximal Extractable Value (MEV), the value which a validator may extract in their duties as block producer, changed the paradigm. Validators turned into passive proposers engaging with a distinct set of <a href=\"https://arxiv.org/abs/2307.01686\" rel=\"noopener nofollow ugc\">active</a> builders. Active builders do not simply collect transactions and mindlessly pack them into blocks; instead, they actively optimize for the value they can extract from the block’s contents. Meanwhile, validators, as passive proposers, simply listen for offers from builders who wish to build on their behalf.</p>\n<p>Unfortunately, as a numerically much smaller set with its own profit-maximizing goals, there is no reason for builders’ objectives and preferences to be aligned with validators regarding the inclusion or censorship of certain transactions or even with the preferences of network stakeholders broadly. More generally, the following principle may be put forward:</p>\n<blockquote>\n<p><strong>Chain neutrality:</strong> Any pending, fee-paying transaction ought to be included if it is available and if there is room to include it on-chain.</p>\n</blockquote>\n<p>Chain neutrality is meant to encompass principles larger than censorship resistance alone. Under censorship resistance, as we conceive of it, there is an active and targeted effort to prevent the access of some user to on-chain inclusion. However, with today’s dense supply networks of intermediated relationships, some user interactions may fall by the wayside for accidental reasons simply because their transaction does not participate in increasing the welfare of the supply network intermediaries. <a href=\"https://ethresear.ch/t/reducing-latency-games-by-levelling-the-playing-field-on-block-size-for-pbs/19356\">An example</a> is builders excluding transactions not to censor them per se, but to decrease the amount of time it would take a block to propagate over the network. In our opinion, users must always be able to “get on-chain” as long as they satisfy the two requirements spelled out in the definition of chain neutrality.</p>\n<p>Validators as passive block producers would simply fill up their blocks from the mempool, ensuring allocation of blockspace to users who indeed satisfy the requirements of chain neutrality. Yet, with today’s systems, proposers have the choice of either remaining passive (”local building”) or locking themselves out entirely from participating in block production (with MEV-Boost). To preserve chain neutrality, we are then looking for ways to recover the diversity that validators embody by allowing them to provide input into block production, even when they decide to delegate building.</p>\n<p><a href=\"https://eips.ethereum.org/EIPS/eip-7547\" rel=\"noopener nofollow ugc\">Inclusion lists</a> (ILs) are a proposed method to recover validator participation in block creation. The validator produces an IL that the builder must <em>satisfy</em> according to the IL model’s specifications (e.g., the builder must include all transactions from the IL in their block or produce a block that cannot contain more transactions from the IL).</p>\n<p>The first observation is that validators have not been forced into outsourcing their block building to active builders and have done so to maximize their own returns. It is then unclear why a validator would care to produce an IL binding the builders for their slot, forcing them to include transactions that the builders may wish to steer clear of. For this reason, the model of <em>forward</em> inclusion lists was proposed. Forward inclusion lists are built by the validator-proposer of slot <em>n</em> and constrain the block produced by the validator-proposer of slot <em>n</em> + 1. From the perspective of network stakeholders, constraints on validators to respect and preserve chain neutrality, <em>even when imposed externally by distinct validators</em>, appear to be legitimate.</p>\n<p>But how can we judge the success of inclusion lists in bolstering chain neutrality beyond other potential use cases for ILs? Naively, we could consider the quantity and quality of transactions in the IL. Are the transactions reported in the IL indeed susceptible to censorship, for example, or is the IL used for other goals? This is an important question since the inclusion list is a scarce protocol product, yet other use cases lie in wait!</p>\n<ul>\n<li><strong>Preconfirmations:</strong> <a href=\"https://0xjunger.github.io/eth-preconfs-sequencing/\" rel=\"noopener nofollow ugc\">Preconfirmations</a> are agreements between a proposer and a user regarding the conditions for inclusion of a user’s transaction, e.g., within some time interval or acting upon some specific state. Preconfirmations offered by the validator-proposer hence require the validator to have some input into the block-building process. ILs look like a good place for the proposer to register their commitments, which may mean that the IL might be used for preconfirmations instead of for its purpose of ensuring chain neutrality.</li>\n<li><strong>Partial block building:</strong> Allowing multiple builders to build different parts of the block may have value. ILs could offer a path to implementing a rough protocol for such partial building commitments, using the same pattern as mev-boost (think “IL-boost” with an IL builder feeding the data to the validator-proposer). More generally, the value proposition of <a href=\"https://ethresear.ch/t/unbundling-pbs-towards-protocol-enforced-proposer-commitments-pepc/13879\">PEPC</a> is to leverage the proposer’s ability to make commitments regarding the organization of block building, including commitments about partial contents. ILs allow for a limited class of commitments on building a block, so is there again a conflict between ILs as neutral input surfacing and ILs as coordination gadgets toward partial block building.</li>\n</ul>\n<p>Here lies the crux of the problem. Other use cases that potentially unlock a lot of value for the IL proposer can use the commitment power that the IL proposer receives from the protocol. Preconfirmations and certain PEPC-esque proposer commitments could be realized via inclusion lists. If a proposer makes more profit by constructing their IL for these use cases than for others, they likely will. When ILs are used for cases other than chain neutrality, we say that the chain neutrality use case is <em>crowded out</em>. This post suggests a new design property for inclusion lists: <em>uncrowdability</em>. We examine how a protocol can ensure that a protocol product becomes uncrowdable, and we apply this reasoning to the currently proposed inclusion list models.</p>\n<h2><a name=\"uncrowdable-inclusion-lists-2\" class=\"anchor\" href=\"https://ethresear.ch#uncrowdable-inclusion-lists-2\"></a>Uncrowdable Inclusion Lists</h2>\n<p>We informally define uncrowdable inclusion lists as inclusion lists that create more value for the inclusion list proposer when used for the purpose the protocol intended them to than when used for other purposes. Making ILs uncrowdable should be a protocol design goal. Similarly <a href=\"https://barnabe.substack.com/p/pbs\" rel=\"noopener nofollow ugc\">to work done on PBS</a>, we delineate the design space of inclusion lists into the allocation rule and the market structure.</p>\n<p>The market structure is the collection of rights and obligations that are assigned to the inclusion list. Who gets to make the inclusion list? To which block(s) does the inclusion list apply? How is the inclusion list enforced? We have seen a number of different proposed IL designs that each use a different market structure. <a href=\"https://notes.ethereum.org/@fradamt/forward-inclusion-lists\" rel=\"noopener nofollow ugc\">Forward inclusion lists</a> allow the beacon proposer of block <span class=\"math\">n</span> to create the IL that applies to block <span class=\"math\">n + 1</span>. Conditional ILs are enforced only if there is space for the transactions from the IL to be included in the block, whereas <a href=\"https://ethresear.ch/t/unconditional-inclusion-lists/18500\">unconditional ILs</a> are enforced regardless of the number of transactions in the main block body. Similarly, a <a href=\"https://ethresear.ch/t/cumulative-non-expiring-inclusion-lists/16520\">cumulative, non-expiring inclusion list</a> is also a different protocol product.</p>\n<p>The allocation rule of inclusion lists refers to the space of commitments that the IL proposer may implement. The ideal allocation would be including in the list a set of transactions that are “censored.” IL proposers may implement other allocation rules, such as outsourcing IL production to third parties, e.g., via the aforementioned hypothetical IL-boost. The protocol can create a market structure such that the allocation rule that an IL proposer chooses also achieves the goal of upholding chain neutrality.</p>\n<p>The protocol can use the market structure to incentivize the IL proposer to choose an allocation rule that upholds chain neutrality. Different market structures elicit different allocation rules used by IL proposers.</p>\n<p>Arguably, conditional inclusion lists could be closer to being uncrowdable than unconditional ones because they offer fewer guarantees about the inclusion of transactions. Unconditional ILs could more easily be used for other use cases. Therefore, using the market structure of conditional ILs may incentivize the IL proposer to choose an allocation rule that upholds chain neutrality, thereby making ILs more uncrowdable.</p>\n<p>The protocol could also make ILs uncrowdable by choosing a market structure that restricts the set of possible allocation rules. <a href=\"https://ethresear.ch/t/the-more-the-less-censored-introducing-committee-enforced-inclusion-sets-comis-on-ethereum/18835/3\">COMIS</a> is such a mechanism. It roughly states that an inclusion list is constructed out of the inclusion sets that each member of a large committee makes. If a transaction is included in at least a certain fraction of the inclusion sets, it must be included in the final inclusion list. Such a mechanism clearly leads to a market structure that is more opinionated about its purpose, and it would be more costly for an IL proposer to use the IL for any other purpose than for chain neutrality.</p>\n<p>There may be a more general argument to be made here. In upholding chain neutrality, the protocol makes an opinionated choice in favor of censored parties, even though this may lead to a welfare gap. A market-based solution, where the allocation rule is unconstrained, generally aims to find the welfare-maximizing solution, meaning that the MEV of one specific block is maximized. Therefore, if a protocol wants chain neutrality, maybe it must constrain the set of possible allocation rules. Potentially, this is analogous to a protocol that needs to specify an allocation rule for block construction if it wants to enforce a first-come, first-served block allocation rule.</p>\n<p>Given this welfare gap, one might also wonder whether it is bad for the protocol-specified purpose to be crowded out by other use cases. However, whether this welfare gap is desirable is a governance question and not one that should be answered by markets. Moreover, other use cases, such as preconfirmations, do not require protocol products and may be satisfied by out-of-protocol proposer commitments such as <a href=\"https://hackmd.io/@bchain/BJkarrEWp\" rel=\"noopener nofollow ugc\">PEPC-Boost</a> or EigenLayer-based systems. Chain neutrality is hard to improve with out-of-protocol systems.</p>\n<h2><a name=\"comparison-between-inclusion-lists-pepc-and-multiple-concurrent-block-producers-3\" class=\"anchor\" href=\"https://ethresear.ch#comparison-between-inclusion-lists-pepc-and-multiple-concurrent-block-producers-3\"></a>Comparison between Inclusion Lists, PEPC, and Multiple Concurrent Block Producers</h2>\n<p>We’ve touched upon the possibility of some use cases crowding out others, given the design of the inclusion list market structure. Use cases embody the demand for certain features or outcomes that market participants desire to achieve. If the demand cannot be satisfied directly by some product yet can be satisfied indirectly by some other product, then even if the indirect product is not <em>ex-ante</em> designed to accommodate this demand, some use cases will crowd out others. We must now understand where the demand for use cases such as preconfirmations or more general proposer commitments comes from and why inclusion lists allow for indirect expression of these use cases.</p>\n<p>Broadly, we think of inclusion lists, preconfirmations, proposer commitments, and other phenomena as instances of a larger will to participate in <strong>block co-creation</strong>. As an atomic unit, a block represents a <em>big</em> amount of space to allocate for the expression of user preferences, via transactions. A Hayekian argument may be advanced that no single party has the knowledge necessary to make a good block and thus must rely on decentralized knowledge of a larger number of market participants. Preventing the expression of this decentralized will to include creates pent-up demand, which will seek to realize itself in any way that it can.</p>\n<p>As an example, unconditional ILs reserve a certain amount of blockspace in block <span class=\"math\">n+1</span> for the inclusion list, which is proposed and built by the proposer of block <span class=\"math\">n</span>. This setup is very similar to an instance of PEPC with two partial builders. In fact, if the protocol could take delivery of both blocks separately instead of requiring one builder to bundle the inclusion list with the regular block, we would not need the “forward” property of the IL, and we would obtain exactly two parallel builders, although some transactions in the unconditional IL may already be executed in the main block body if it were appended to the end of block <span class=\"math\">n+1</span>.</p>\n<p>If a single funnel, such as inclusion lists, cannot adequately realize every type of demand from the market, can we look for differentiated “pipes” adapted to each type of use case that may occur? Barnabé suggests the idea of a “mixed IL”: an IL built up from <em>a)</em> a spot (applying to the block of the IL proposer) unconditional IL and <em>b)</em> a forward conditional IL. This would allow good quality preconfirmations to be issued in the spot unconditional IL, whereas the forward conditional IL could still be used for chain neutrality.</p>\n<p>Alternatively, we could also design a spot IL that upholds chain neutrality. By using a spot unconditional IL with COMIS, the role of the block proposer in the construction of the IL could be removed and replaced by a committee. This is a change in market structure similar to <a href=\"https://ethresear.ch/t/execution-tickets/17944\">execution tickets</a> since we question whether the beacon proposer of a slot should receive the rights to propose the inclusion list, instead of actors directly interacting with the protocol.</p>\n<p>Furthermore, unconditional ILs that apply to slot <span class=\"math\">n</span> but are made by another party than the beacon proposer of slot <span class=\"math\">n</span> result in something that looks very similar to <a href=\"https://ethresear.ch/t/multiplicity-a-gadget-for-multiple-concurrent-block-proposers/14962\">multiple concurrent block producers</a>. The differences and similarities between these different forms of block co-creation are currently under-explored.</p>\n<h2><a name=\"inclusion-list-tips-4\" class=\"anchor\" href=\"https://ethresear.ch#inclusion-list-tips-4\"></a>Inclusion List Tips</h2>\n<p>The current inclusion list spec says that the tips associated with the transactions in the IL are rewarded to the proposer of the block in which these transactions are executed, not to the proposer that included the transactions in the IL. In this part of the post, we argue that rewarding the tips to the IL proposer instead of the block proposer 1) creates a scheme in which tips can conditionally pay for censorship resistance, 2) may be inevitable, and 3) increases the cost of censorship. Note that if a credible mechanism to reward IL proposers existed, the protocol could reward IL proposers in different ways; it could use issuance, for example.</p>\n<p>First, by giving tips from transactions that are executed via the IL, we can reward the IL proposer for providing a valuable service of chain neutrality. Here, we would consider a transaction executed <em>via the IL</em> if the transaction is in the IL but not in the main block body of any earlier block. This is good for the proposer as it closes the welfare gap between using the IL for censorship resistance and for other purposes: the IL is more uncrowdable. Moreover, it is also good for the user because it allows them to pay for this service, conditional on the transaction not being included in any other way. This may be more difficult to do out-of-protocol.</p>\n<p>While conditional tips for censorship resistance may be more difficult out-of-protocol, it is trivial for the inclusion list proposer to demand that any transaction that wants to be included in the IL bribe the proposer and then allow these transactions to be included in the list with 0 tips.</p>\n<p>Finally, the cost of censorship, defined in “<a href=\"https://arxiv.org/abs/2301.13321\" rel=\"noopener nofollow ugc\">Censorship Resistance in On-Chain Auctions</a>” by Elijah Fox, Mallesh Pai, and Max Resnick as the minimum cost an adversary would have to pay to censor a transaction, doubles when rewarding the IL proposer with the tip instead of the block proposer. This is because an adversary would not only need to bribe the block proposer to exclude the transaction, but it would also need to bribe the inclusion list proposer by the amount of the tip instead of by any arbitrarily small amount.</p>\n<h2><a name=\"conclusion-5\" class=\"anchor\" href=\"https://ethresear.ch#conclusion-5\"></a>Conclusion</h2>\n<p>To summarize, this post argues that the forward property of inclusion lists is insufficient to guarantee that inclusion lists will be used to uphold chain neutrality. We introduce the concept of <em>uncrowdable</em> inclusion lists, which may explain why a certain inclusion list design will or will not achieve the protocol-specified goal of chain neutrality. Uncrowdable inclusion lists create more value for the list proposer when used for the protocol-desired goal than for any other goal. The protocol can be designed for uncrowdable inclusion lists by creating a market structure that induces a desirable allocation rule. Finally, we argue that it is beneficial for the tips associated with transactions in the inclusion list to be rewarded to the inclusion list proposer instead of to the block proposer.</p>\n            <p><small>7 posts - 4 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/uncrowdable-inclusion-lists-the-tension-between-chain-neutrality-preconfirmations-and-proposer-commitments/19372\">Read full topic</a></p>","link":"https://ethresear.ch/t/uncrowdable-inclusion-lists-the-tension-between-chain-neutrality-preconfirmations-and-proposer-commitments/19372","pubDate":"Thu, 25 Apr 2024 10:36:53 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19372"},"source":{"@url":"https://ethresear.ch/t/uncrowdable-inclusion-lists-the-tension-between-chain-neutrality-preconfirmations-and-proposer-commitments/19372.rss","#text":"Uncrowdable Inclusion Lists: The Tension between Chain Neutrality, Preconfirmations and Proposer Commitments"},"filter":false},{"title":"Testing the Capabilities of a Protocol based on Distributed Validator Technology (DVT)","dc:creator":"daniejjimenez","category":"Proof-of-Stake","description":"<h1><a name=\"holesky-safestake-testnet-private-mainnet-why-participate-now-1\" class=\"anchor\" href=\"https://ethresear.ch#holesky-safestake-testnet-private-mainnet-why-participate-now-1\"></a>Holesky SafeStake Testnet &amp; Private mainnet: Why participate now?</h1>\n<p>One of SafeStake’s great strengths in the last two years has undoubtedly been the hard work at the development level to deliver a highly resilient and decentralized staking framework and protocol, facilitating the onboarding of thousands of users to the fascinating world of ETH staking with DVT technology.</p>\n<p>After the successful and stable Galileo Testnet launch to the public on February 25, 2023, a key moment has arrived for SafeStake in its definitive step towards releasing its long-awaited public mainnet: Holesky Safestake Testnet!</p>\n<p>The Holesky Safestake Testnet is the final step before deploying the DVT-based protocol on the Ethereum mainnet in the first quarter of this year, as indicated in their “<a href=\"https://safestake.substack.com/p/safestake-annual-review-and-2024\" rel=\"noopener nofollow ugc\">SafeStake Annual Review and 2024 OKR Outline.</a>” It is a fundamental part of achieving the short-term goals and key results outlined for the project.</p>\n<p>The deployment of Holesky Safestake Testnet over a month aims to ensure a minimum of 20 professional node operators with a minimum performance rating of 98%+, enabling the service of over 2,000 Ethereum mainnet validators when the protocol is launched on the main network at the end of February.</p>\n<p><img src=\"https://ethresear.ch/uploads/default/original/3X/3/c/3c33f8cfd4fd4fdedfa8bc40280b2cb67a28f34e.png\" alt=\"Captura de pantalla 2024-04-24 122010\" data-base62-sha1=\"8AzZQBdDV2DlNI56iZzseSOEJVI\" width=\"670\" height=\"229\"></p>\n<h2><a name=\"advantages-of-holesky-safestake-testnet-2\" class=\"anchor\" href=\"https://ethresear.ch#advantages-of-holesky-safestake-testnet-2\"></a>Advantages of Holesky Safestake Testnet</h2>\n<p>One of the main questions the average user may address is why another month of testing in Holesky should be deployed. The reasons are straightforward: Holesky has been launched as a critical test environment for Ethereum developers to experiment with ambitious updates to enhance Ethereum’s scalability and performance.</p>\n<p>The Holesky Ethereum testnet allows for faster block times and greater stability, enabling the testing of large-scale features that the Safestake protocol will have in the future, such as liquid staking and the creation of validators with a low limit of only 4 ETH.</p>\n<p>The generation of 4 ETH mini-pools in Safestake is undoubtedly one of the most critical milestones in the project’s roadmap. Therefore, subjecting the protocol to the necessary stress tests in Holesky before its deployment on the mainnet is crucial to ensure that this functionality performs without any issues.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/9/6/9603d047a42f7f6ee4e6d1c904153fa8bd0382fa.png\" data-download-href=\"https://ethresear.ch/uploads/default/9603d047a42f7f6ee4e6d1c904153fa8bd0382fa\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/9/6/9603d047a42f7f6ee4e6d1c904153fa8bd0382fa_2_602x339.png\" alt=\"\" data-base62-sha1=\"lp5N3bUS3nUG0nlD3LfVPJ3d5mG\" width=\"602\" height=\"339\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/9/6/9603d047a42f7f6ee4e6d1c904153fa8bd0382fa_2_602x339.png, https://ethresear.ch/uploads/default/original/3X/9/6/9603d047a42f7f6ee4e6d1c904153fa8bd0382fa.png 1.5x, https://ethresear.ch/uploads/default/original/3X/9/6/9603d047a42f7f6ee4e6d1c904153fa8bd0382fa.png 2x\" data-dominant-color=\"E0DFF5\"></a></div><p></p>\n<p>Holesky Safestake Testnet will allow SafeStake operators and validators to activate a node and perform their functions similarly to Goerli but in a more scalable and secure environment, enabling the participation of a larger number of validators in running more comprehensive and rigorous tests, such as the generation of distributed keys (DKG), before they are implemented on the main network.</p>\n<p>This new testing phase is also designed to simulate the performance of Safestake in line with upcoming updates that will be implemented in Ethereum to improve the scalability of the main network.</p>\n<h2><a name=\"why-participate-in-holesky-safestake-testnet-3\" class=\"anchor\" href=\"https://ethresear.ch#why-participate-in-holesky-safestake-testnet-3\"></a>Why Participate in Holesky Safestake Testnet</h2>\n<p>With the successful launch of Safestake on Holesky, the protocol development team will be able to test the ambitious features of the protocol in a more realistic environment as it approaches the achievement of short-term goals, measuring the scalability and performance of the cutting-edge SafeStake Operator Node.</p>\n<p>Participating in the Holesky Safestake Testnet will, in the first instance, allow testing the capabilities of a unique protocol based on Distributed Validator Technology (DVT), aiming to become one of the leading DVT infrastructure providers for the Lido DAO and its operators in the short term.</p>\n<p>As is publicly known, the staking industry is one of the fastest-growing sectors projected for the next half of this decade. According to Staking Rewards data, the staking Industry MarketCap is currently around $273.99b, and Ethereum’s staking participation rate (26.3% of the total supply staked) still significantly lags behind other PoS networks such as Solana (71% staked) or Cardano (62% staked).</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/1/f/1f28bebd9dcd4a6ff130d329e81961a1cb2e7005.png\" data-download-href=\"https://ethresear.ch/uploads/default/1f28bebd9dcd4a6ff130d329e81961a1cb2e7005\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/1/f/1f28bebd9dcd4a6ff130d329e81961a1cb2e7005_2_602x284.png\" alt=\"\" data-base62-sha1=\"4rE62R1srNmJW1ZzBtwokjbTcKp\" width=\"602\" height=\"284\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/1/f/1f28bebd9dcd4a6ff130d329e81961a1cb2e7005_2_602x284.png, https://ethresear.ch/uploads/default/optimized/3X/1/f/1f28bebd9dcd4a6ff130d329e81961a1cb2e7005_2_903x426.png 1.5x, https://ethresear.ch/uploads/default/original/3X/1/f/1f28bebd9dcd4a6ff130d329e81961a1cb2e7005.png 2x\" data-dominant-color=\"DEDEE0\"></a></div><p></p>\n<pre><code>                         *Source: Staking Rewards*\n</code></pre>\n<p>Within the ETH staking market, it is important to provide a protocol with a DVT-based infrastructure to reduce risks of centralization.</p>\n<p>Safestake is working diligently to ensure that a portion of the Ops base on Ethereum joins our mainnet to achieve the goal of a more resilient, secure, and decentralized Ethereum.</p>\n<p>On the other hand, becoming a Node Operator in Holesky Safestake Testnet can represent a historic opportunity in the mission to help make Ethereum more decentralized with a distributed base of validators.</p>\n<p>Additionally, it is also an opportunity to become one of the Top 20 Operators with excellent performance, who will be incorporated into Safestake <a href=\"https://blog.safestake.xyz/2024/03/25/announcing-the-launch-of-the-safestake-road-to-mainnet-campaign/\" rel=\"noopener nofollow ugc\">Mainnet</a> Private in 2024, and start earning rewards (ETH) for keeping validators secure and online to perform Ethereum Proof-of-Stake consensus duties.</p>\n<h2><a name=\"how-to-join-holesky-safestake-testnet-4\" class=\"anchor\" href=\"https://ethresear.ch#how-to-join-holesky-safestake-testnet-4\"></a>How to join Holesky Safestake Testnet</h2>\n<p>In case you want to become a node operator on Holesky Safetake Testnet, you can find an excellent instructional video on how to join at this link:</p>\n<p><a href=\"https://www.youtube.com/watch?v=rvzSAP9XHbM&amp;ab_channel=SafeStake\" rel=\"noopener nofollow ugc\">How to run an Operator Node on SafeStake Testnet v2.0 -Version 2 (Holesky)</a></p>\n<p>In case you have already joined, you can expand the information in this instruction manual to upgrade to the last version:</p>\n<p><a href=\"https://github.com/ParaState/SafeStakeOperator/blob/main/docs/safestake-running-an-operator-node-on-going.md\" rel=\"noopener nofollow ugc\">SafeStakeOperator/docs/safestake-running-an-operator-node-on-going.md at main</a></p>\n<p>On the other hand, if you are a validator, you must obtain 32 HolETH via the <a href=\"https://holesky-faucet.pk910.de/#/\" rel=\"noopener nofollow ugc\">Holesky testnet PoW faucet</a>, which is free. In addition, you will require <a href=\"https://discord.com/channels/769941371581890580/1112339492364365889\" rel=\"noopener nofollow ugc\">120 test $DVT</a> to run a validator on Testnet v2.0.</p>\n<p>Go to <a href=\"https://holesky.safestake.xyz/\" rel=\"noopener nofollow ugc\">Holesky.SafeStake.xyz</a> and connect your wallet. Then click Run Validator and Visit Ethereum Launchpad to create a new validator. Wait for your new Validator to become active on Holesky before importing it to SafeStake.</p>\n<p>Then Import Existing to import your keystore file using the drag-and-drop interface. Choose the operators you want to run your validator, approve the $DVT transaction, and wait for your validator to become active on SafeStake (~15 min)</p>\n<p>SafeStake on Holesky marks a major milestone and the beginning of the final 30 days of testing before SafeStake launches on the Ethereum mainnet.</p>\n<h2><a name=\"benefits-of-joining-holesky-safestake-testnet-and-private-mainnet-5\" class=\"anchor\" href=\"https://ethresear.ch#benefits-of-joining-holesky-safestake-testnet-and-private-mainnet-5\"></a>Benefits of Joining Holesky Safestake Testnet and Private Mainnet</h2>\n<p>In addition to the advantages of promoting the adoption of Distributed Validator Technology (DVT) among stakers and node operators, joining our test network has substantial economic benefits through our <a href=\"https://parastate-safestake-community.notion.site/SafeStake-Incentive-Program-for-Professional-Operators-Mainnet-Validators-and-Home-Stakers-b9f884ebc1794e35b31999083101be6c\" rel=\"noopener nofollow ugc\">Mainnet Validator Incentive Pool</a>.</p>\n<p>There will be 1.4 million $DVT tokens as an incentive to distribute to ETH holders who decide to run an Ethereum mainnet validator on the Safestake mainnet. Additionally, there will be an extra incentive based on a points system for early birds.</p>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/1/a/1af24bc2219cc2804be03c181f05e677bff2af4c.jpeg\" data-download-href=\"https://ethresear.ch/uploads/default/1af24bc2219cc2804be03c181f05e677bff2af4c\" title=\"\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/1/a/1af24bc2219cc2804be03c181f05e677bff2af4c_2_602x316.jpeg\" alt=\"\" data-base62-sha1=\"3QnwUF5MHj98nrmDgcPX8DYBMXa\" width=\"602\" height=\"316\" role=\"presentation\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/1/a/1af24bc2219cc2804be03c181f05e677bff2af4c_2_602x316.jpeg, https://ethresear.ch/uploads/default/optimized/3X/1/a/1af24bc2219cc2804be03c181f05e677bff2af4c_2_903x474.jpeg 1.5x, https://ethresear.ch/uploads/default/original/3X/1/a/1af24bc2219cc2804be03c181f05e677bff2af4c.jpeg 2x\" data-dominant-color=\"B8D8F5\"></a></div><p></p>\n<p>As mentioned earlier, being an independent node operator or a validator in Holesky Safestake Testnet will allow you to familiarize yourself with the protocol and opt for one of the first 20 spots available for our upcoming Safestake Mainnet Private to be deployed on Ethereum by the end of February.</p>\n<p>Operators will need to spin up a new node on Holesky and keep it active for 60 days, followed by running a node on the Safestake mainnet that remains active for 60 days post-mainnet launch.</p>\n<p>Validators must join SafeStake mainnet to be eligible for rewards.</p>\n<h2><a name=\"safestake-the-path-to-efficient-staking-6\" class=\"anchor\" href=\"https://ethresear.ch#safestake-the-path-to-efficient-staking-6\"></a>Safestake: The Path to Efficient Staking</h2>\n<p>If you love financial freedom and resonate with the decentralized spirit that Ethereum embodied at its inception, the SafeStake Testnet and Mainnet Private are the ideal places to put your ideals into practice.</p>\n<p>Safestake, based on DVT, is emerging as a more efficient and decentralized staking solution, offering various advantages over traditional single-node staking solutions, allowing it to be at the forefront of the mission to achieve a more decentralized Ethereum.</p>\n<p>The DVT functionality in SafeStake is powered by three key components: Shamir Secret Sharing for BLS Signatures, Multi-Party Computation (MPC), and the BFT (Byzantine Fault Tolerance) consensus layer (HotStuff). The result is a validator that uses a ‘3 out of 4’ threshold signature scheme, where only three out of four operators are needed to attest on a validator’s behalf and where no single entity can ever gain control or recreate the private key.</p>\n<p>Safestake infrastructure is written in Rust for increased performance and security for the smart contracts that power its Ethereum staking. Nodes are managed by independent, decentralized operators on a permissionless network, creating a highly decentralized, ultra-secure, fault-tolerant environment.</p>\n<p>Furthermore, the protocol aims to cover all narratives around Ethereum staking, such as LSDfi, when it incorporates the ability to run validators through mini-pools of just 4 ETH in its final stage, seeking to diversify the offering of staking pools in the rapidly growing sector currently dominated by a few actors.</p>\n<p>We invite all those interested in testing the capabilities of our Testnet to actively participate as validators and ops.</p>\n<p>We value objective criticism and community input on the protocol, as the path to a sustainable Ethereum is decentralization, and Safestake based on DVT provides a way to achieve it.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/testing-the-capabilities-of-a-protocol-based-on-distributed-validator-technology-dvt/19368\">Read full topic</a></p>","link":"https://ethresear.ch/t/testing-the-capabilities-of-a-protocol-based-on-distributed-validator-technology-dvt/19368","pubDate":"Wed, 24 Apr 2024 16:16:28 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19368"},"source":{"@url":"https://ethresear.ch/t/testing-the-capabilities-of-a-protocol-based-on-distributed-validator-technology-dvt/19368.rss","#text":"Testing the Capabilities of a Protocol based on Distributed Validator Technology (DVT)"},"filter":false},{"title":"Reducing latency games by levelling the playing field on block size for PBS","dc:creator":"antonydenyer","category":"Block proposer","description":"<p><em>Thanks to <a class=\"mention\" href=\"https://ethresear.ch/u/simbro\">@simbro</a> for reviewing</em></p>\n<h2><a name=\"abstract-1\" class=\"anchor\" href=\"https://ethresear.ch#abstract-1\"></a>Abstract</h2>\n<p>For this post, block size refers to the number of serialised bytes in a block. Currently, the average block size is over 100k <a href=\"https://etherscan.io/chart/blocksize\" rel=\"noopener nofollow ugc\">https://etherscan.io/chart/blocksize</a>. Note that we are talking about bytes, not gas limit.</p>\n<p>Every block builder is motivated to submit a block to the PBS auction as late as possible. The more time a block builder has, the more time they have to accumulate transactions and, therefore, priority fees. For the purpose of this discussion, we assume no MEV is at play.</p>\n<h2><a name=\"discrete-blocks-and-latency-games-2\" class=\"anchor\" href=\"https://ethresear.ch#discrete-blocks-and-latency-games-2\"></a>Discrete Blocks and Latency Games</h2>\n<p>Apologies if this is old news, but it’s worth reiterating. The role of a block builder is multifaceted and requires proficiency in several infrastructure tasks.</p>\n<ol>\n<li>Block builders must be able to access transactions; solo validators often miss out on priority fees because they are not well connected to the public mempool. A block builder with good connectivity to the mempool will likely win more blocks. They may even partner with wallets and other transaction originators to fast-track mempool transactions into their builder pipeline.</li>\n<li>They must have good networking connectivity with the auctioneer, aka relay. The lower the latency between them and the auctioneer, the more time they have to build blocks.</li>\n<li>Because the block builder is privileged, they can offer value-added features that no other entity can offer. Namely, revert protection through eth_sendBundle. The builder who can build a block the fastest whilst protecting private order flow from reverts will win more blocks (once again, we assume no mev).</li>\n</ol>\n<p>Because blocks are discrete periods, pressure is applied to all parts of the stack towards the end of the block. Consequently, a block builder will do what it can to increase the amount of time it has to focus on its core activity, building the most profitable block.</p>\n<h2><a name=\"observations-3\" class=\"anchor\" href=\"https://ethresear.ch#observations-3\"></a>Observations</h2>\n<p>Block builders will likely submit multiple bids using multiple strategies. Sometimes, the bids for smaller blocks are received in time, while those for larger blocks are not. This is simply because larger blocks are slower. Consequently, transactions that could be included in a block are not being picked up.</p>\n<h2><a name=\"hypothetical-scenario-4\" class=\"anchor\" href=\"https://ethresear.ch#hypothetical-scenario-4\"></a>Hypothetical Scenario</h2>\n<p>A block builder has 100 transactions in their local mempool, totalling 0.5 eth in priority fees. The network is silent, and no other transactions are entering the mempool. The block builder submits the block (block a) to the auction. Near the very end of the block, another transaction enters the mempool with a whopping 1 eth in priority fees. The block builder now submits two more bids at the same time.</p>\n<p>block b - containing our single juicy priority fee transaction for 1 eth.<br>\nblock c - containing 101 transactions with all the transactions we have totalling 1.5 eth.</p>\n<p>Both bids are now higher than the previous bid. One of three scenarios now stands:</p>\n<ol>\n<li>\n<p>The original bid wins as neither subsequent bid was reached in time by the auction before the deadline.<br>\n<img src=\"https://ethresear.ch/uploads/default/original/3X/d/3/d329d630c072318e1c3244c344567c4cba725c3b.png\" alt=\"block_a_wins\" data-base62-sha1=\"u82nCkXNMDe8sPVNCA3xPeH7dNN\" width=\"668\" height=\"379\"></p>\n</li>\n<li>\n<p>The small block reaches the auction before the deadline, pushing out the previously submitted block of 100 transactions.<br>\n<img src=\"https://ethresear.ch/uploads/default/original/3X/f/f/ffc5cc2ec664852f1b14f994a5f4fd0fdc2078bc.png\" alt=\"block_b_wins\" data-base62-sha1=\"AuFx817b7Iq6j0znbfYZ90NpFp2\" width=\"668\" height=\"379\"></p>\n</li>\n<li>\n<p>The big block reaches in time, and all transactions are included.<br>\n<img src=\"https://ethresear.ch/uploads/default/original/3X/9/a/9a46b15ba954c2447f02744a92623a7f33c26940.png\" alt=\"block_c_wins\" data-base62-sha1=\"m0MZk2G1o9HICbQU88SXOhlKJQQ\" width=\"668\" height=\"379\"></p>\n</li>\n</ol>\n<p>It is easy to imagine an interplay between latency, block size and priority fees that are entirely opaque to users and sophisticated actors.</p>\n<h2><a name=\"real-world-example-5\" class=\"anchor\" href=\"https://ethresear.ch#real-world-example-5\"></a>Real-world example</h2>\n<p>JetBuilder built <a href=\"https://etherscan.io/block/19598122\" rel=\"noopener nofollow ugc\">https://etherscan.io/block/19598122</a> and only used 12% of the block space available, paying ~0.15 eth for the block. We observed them missing at least 40 transactions that could have been included in that block. The example transactions were in the mempool for at least five blocks (thanks to <a href=\"https://www.ethernow.xyz\" rel=\"noopener nofollow ugc\">https://www.ethernow.xyz</a>). They all landed on-chain in either the next block or the one after.</p>\n<p><a class=\"attachment\" href=\"https://ethresear.ch/uploads/short-url/m28oil5kIiAPgjrYROY42OBPcYX.txt\">block_19598122_missed.txt</a> (4.2 KB)</p>\n<h1><a name=\"proposal-6\" class=\"anchor\" href=\"https://ethresear.ch#proposal-6\"></a>Proposal</h1>\n<p>We should have some floor usage in gas terms to prevent transactions from bullying other transactions out of a block.</p>\n<p>The gas floor target could be calculated in many ways, such as a predefined fixed target, half the gas limit, or a dynamic adjustment based on previous consumption (similar to 1559). It doesn’t need to be elegant or exact; it just needs to be something to incentivise block builders to utilise block space.</p>\n<p>The penalty for not ‘filling’ the block would be something like <code>gas target missed * base fee</code>. This is the same price as putting a transaction in the block, except the block builder doesn’t get priority fees. Theoretically, a block builder could make a transaction with themselves, but the result is the same.</p>\n<p>We are simply putting a price on what the network perceives as the underutilisation of block space.</p>\n            <p><small>13 posts - 3 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/reducing-latency-games-by-levelling-the-playing-field-on-block-size-for-pbs/19356\">Read full topic</a></p>","link":"https://ethresear.ch/t/reducing-latency-games-by-levelling-the-playing-field-on-block-size-for-pbs/19356","pubDate":"Tue, 23 Apr 2024 11:00:30 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19356"},"source":{"@url":"https://ethresear.ch/t/reducing-latency-games-by-levelling-the-playing-field-on-block-size-for-pbs/19356.rss","#text":"Reducing latency games by levelling the playing field on block size for PBS"},"filter":false},{"title":"A concrete proposal for correlated attester penalties","dc:creator":"vbuterin","category":"Economics","description":"<p>See previous work: <a href=\"https://ethresear.ch/t/analysis-on-correlated-attestation-penalties/19244\" class=\"inline-onebox\">Analysis on ''Correlated Attestation Penalties''</a> and <a href=\"https://ethresear.ch/t/supporting-decentralized-staking-through-more-anti-correlation-incentives/19116\" class=\"inline-onebox\">Supporting decentralized staking through more anti-correlation incentives</a></p>\n<p>This post introduces a concrete proposal for how correlated penalties could be done, in a way that maximizes (i) simplicity, and (ii) consistency with valuable invariants that exist today.</p>\n<h3><a name=\"goals-1\" class=\"anchor\" href=\"https://ethresear.ch#goals-1\"></a>Goals</h3>\n<ol>\n<li>Replicate the spirit of the basic design proposed <a href=\"https://ethresear.ch/t/supporting-decentralized-staking-through-more-anti-correlation-incentives/19116\">here</a>.</li>\n<li>Maximum simplicity (the same type of simplicity as we can see in eg. the <a href=\"https://notes.ethereum.org/@vbuterin/proto_danksharding_faq#How-does-the-exponential-EIP-1559-blob-fee-adjustment-mechanism-work\">EIP-4844 blob gas market design</a>)</li>\n<li>Same average validator revenue as today, at all levels of “percent attesting correctly”. This should hold as a hard invariant, even against attackers with a large percent of stake trying to break it</li>\n<li>Same penalty as today from failing to make one attestation, on average</li>\n<li>Validators should only be rewarded for sending an attestation, never passively</li>\n</ol>\n<h3><a name=\"mechanism-2\" class=\"anchor\" href=\"https://ethresear.ch#mechanism-2\"></a>Mechanism</h3>\n<ul>\n<li>We set two constants: <code>PENALTY_ADJUSTMENT_FACTOR = 2**12</code>, <code>MAX_PENALTY_FACTOR = 4</code></li>\n<li>We add a counter to the state, <code>NET_EXCESS_PENALTIES</code></li>\n<li>During a slot, let <code>non_attesting_balance</code> be the total balance that is <em>not</em> correctly attesting in that slot</li>\n<li>Let: <code>penalty_factor = min((non_attesting_balance * PENALTY_ADJUSTMENT_FACTOR) //  (NET_EXCESS_PENALTIES * total_active_balance + 1), MAX_PENALTY_FACTOR)</code></li>\n<li>Let <code>R</code> be the current reward for attesting correctly (computed based on <code>base_reward</code> and adjusted based on the fraction allocated to the job in question). This stays the same.</li>\n<li>If a validator <em>fails</em> to attest correctly, they get penalized <code>penalty_factor * R</code> (as opposed to <code>R</code> as today)</li>\n<li>At the end of a slot, set: <code>NET_EXCESS_PENALTIES = max(1, NET_EXCESS_PENALTIES + penalty_factor) - 1</code></li>\n</ul>\n<h3><a name=\"rationale-3\" class=\"anchor\" href=\"https://ethresear.ch#rationale-3\"></a>Rationale</h3>\n<p>It should be easy to see that <code>NET_EXCESS_PENALTIES</code> tracks <code>sum(penalty_factor[slot] for slot in slots) - len(slots)</code>. Hence, if <code>penalty_factor</code> on average exceeds 1 for a sustained period of time, <code>NET_EXCESS_PENALTIES</code> will keep rising until that’s no longer the case. <code>NET_EXCESS_PENALTIES</code> is part of the denominator in the calculation of <code>penalty_factor</code>, and so <code>NET_EXCESS_PENALTIES</code> rising will push the average <code>penalty_factor</code> values down until the average is below 1 (and likewise in reverse, if it decreases).</p>\n<p><code>penalty_factor</code> is proportional to the total <code>non_attesting_balance</code> of the current slot, and so for it to average 1, it must roughly equal the <code>non_attesting_balance</code> of the current slot divided by the long term average - exactly the design proposed <a href=\"https://ethresear.ch/t/supporting-decentralized-staking-through-more-anti-correlation-incentives/19116\">here</a>.</p>\n<p>Because <code>penalty_factor</code> averages 1, average non-participation penalties are equal to <code>R</code>, as today. And so average rewards for a validator are the same as today, for any correct attestation rate, assuming that their incorrect attestations are uncorrelated with those of other validators.</p>\n<p><code>PENALTY_ADJUSTMENT_FACTOR</code> affects how quickly penalties can adjust.</p>\n<h3><a name=\"possible-extensions-4\" class=\"anchor\" href=\"https://ethresear.ch#possible-extensions-4\"></a>Possible extensions</h3>\n<ul>\n<li>Make <code>penalty_factor</code> more “continuous”, eg. by putting the <code>base_reward</code> into the numerator that computes the <code>penalty_factor</code> (and into the maximum, and into the per-slot decrement) and then using it to compute penalties directly.</li>\n<li>Explore smarter ways to apply this mechanism across multiple jobs (correct head attestation, target attestation…). The naive approach is to just apply it sequentially for each job, but there may be a smarter approach.</li>\n</ul>\n            <p><small>5 posts - 5 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/a-concrete-proposal-for-correlated-attester-penalties/19341\">Read full topic</a></p>","link":"https://ethresear.ch/t/a-concrete-proposal-for-correlated-attester-penalties/19341","pubDate":"Fri, 19 Apr 2024 13:25:51 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19341"},"source":{"@url":"https://ethresear.ch/t/a-concrete-proposal-for-correlated-attester-penalties/19341.rss","#text":"A concrete proposal for correlated attester penalties"},"filter":false},{"title":"Backward-Compatible Post-Quantum Migration","dc:creator":"tanteikg","category":"Cryptography","description":"<p>Continuing the discussion from <a href=\"https://ethresear.ch/t/how-to-hard-fork-to-save-most-users-funds-in-a-quantum-emergency/18901/23\">How to hard-fork to save most users' funds in a quantum emergency</a>:</p>\n<p>We are excited to share a <a href=\"https://github.com/pqcee/EIPs/blob/67d445496f8f40d4b515146946de3bef37fbe716/EIPS/eip-7693.md\" rel=\"noopener nofollow ugc\">draft of an EIP</a> that we have been working on. The proposal aims to present a solution for integrating a post-quantum signature scheme into the Ethereum blockchain while maintaining backward compatibility with existing ECDSA. The PQC signature scheme, targets integration with a quantum-safe zero-knowledge proof system such as zkSTARK or MPC-in-the-Head, to ensure the long-term security of Ethereum transactions against quantum attacks without requiring immediate upgrades to existing infrastructure. Looking forward to your thoughts on the proposal</p>\n            <p><small>3 posts - 2 participants</small></p>\n            <p><a href=\"https://ethresear.ch/t/backward-compatible-post-quantum-migration/19340\">Read full topic</a></p>","link":"https://ethresear.ch/t/backward-compatible-post-quantum-migration/19340","pubDate":"Fri, 19 Apr 2024 12:51:57 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19340"},"source":{"@url":"https://ethresear.ch/t/backward-compatible-post-quantum-migration/19340.rss","#text":"Backward-Compatible Post-Quantum Migration"},"filter":false},{"title":"Portal Network & Verkle","dc:creator":"morph-dev","category":"Data Structure","description":"<h1><a name=\"portal-network-verkle-1\" class=\"anchor\" href=\"https://ethresear.ch#portal-network-verkle-1\"></a>Portal Network &amp; Verkle</h1>\n<h2><a name=\"summary-2\" class=\"anchor\" href=\"https://ethresear.ch#summary-2\"></a>Summary</h2>\n<p>The goal of this research is to describe how Verkle trie (see <a href=\"https://eips.ethereum.org/EIPS/eip-6800\" rel=\"noopener nofollow ugc\">EIP-6800</a>) can be efficiently stored in a Portal Network.</p>\n<p>Firstly, I will give quick overview of what Portal Network and Verkle Tries are and how they work, with the focus on parts that are most relevant to this research. Afterwards, I will specify my proposed solution.</p>\n<h2><a name=\"portal-network-overview-3\" class=\"anchor\" href=\"https://ethresear.ch#portal-network-overview-3\"></a>Portal Network overview</h2>\n<p>Portal Network (<a href=\"https://www.ethportal.net/\" rel=\"noopener nofollow ugc\">https://www.ethportal.net/</a>) can be explained as decentralized Ethereum archive node. It’s designed as a peer-to-peer network, where each node holds only small portion of data, while the entire Portal Network stores entire historical Ethereum data. Each Portal Network node decides which content to store locally based on the distance between its <code>Node-Id</code> and <code>Content-Id</code>.</p>\n<p>The Portal Network is actually several peer-to-peer networks (beacon, history and state). Each of these networks stores a specific subset of the data stored by an Ethereum full node. The focus of this research is the state network, whose specification for the Merkle Patricia Trie (MPT) can be found here: <a href=\"https://github.com/ethereum/portal-network-specs/blob/master/state-network.md\" rel=\"noopener nofollow ugc\">link</a>.</p>\n<p>In short, Portal Network stores every trie node and smart contract bytecode from Ethereum state that ever existed. Each entry (trie node/bytecode) is represented with an unique <code>Content-Key</code>. The hash of the <code>Content-Key</code> is 32-byte long <code>Content-Id</code>.</p>\n<p>In the case of the MPT, there are 3 types of content:</p>\n<ul>\n<li>Account Trie Node - the trie node of the main state trie\n<ul>\n<li>It is uniquely identified by the path from the root of the trie and the hash of the trie node</li>\n<li>The value is the rlp encoding of the trie node itself</li>\n</ul>\n</li>\n<li>Contract’s Storage Trie Node - the trie node that belongs to the smart contract’s storage trie\n<ul>\n<li>It is uniquely identified by the address of the smart contract, path from the root of the storage trie, and the hash of the trie node</li>\n<li>Same as above, value is the rlp encoding of the trie node itself</li>\n</ul>\n</li>\n<li>Contract bytecode - the bytecode of the smart contract\n<ul>\n<li>It is uniquely identified by the address of the smart contract and the hash of the bytecode</li>\n<li>The value is the bytecode itself</li>\n</ul>\n</li>\n</ul>\n<p>In order to prevent bad actors from polluting the network with invalid data, the gossiped content contains the Merkle proof, so receiver can verify that the content is canonical to the Ethereum chain. This proof is not stored locally in order to improve performance.</p>\n<h2><a name=\"verkle-trie-overview-4\" class=\"anchor\" href=\"https://ethresear.ch#verkle-trie-overview-4\"></a>Verkle Trie overview</h2>\n<p>The Verkle Trie is the new structure for storing Ethereum state. Current plan is for Ethereum to switch from Merkle Patricia Trie to Verkle Trie two forks from now (sometime in 2025).</p>\n<p>The details of why Ethereum wants to switch to the Verkle trie, and how Verkle trie works can be found in the <a href=\"https://eips.ethereum.org/EIPS/eip-6800\" rel=\"noopener nofollow ugc\">EIP-6800</a>. However, we are going to provide overview of its properties that are relevant for this research.</p>\n<h3><a name=\"pedersen-commitment-5\" class=\"anchor\" href=\"https://ethresear.ch#pedersen-commitment-5\"></a>Pedersen commitment</h3>\n<p>Each trie node is uniquely represented with a Pedersen commitment, which are based on Elliptic curves. The curve that is used is Banderwagon (the variation of the <a href=\"https://ethresear.ch/t/introducing-bandersnatch-a-fast-elliptic-curve-built-over-the-bls12-381-scalar-field/9957\">Bandersnatch curve</a>).</p>\n<p>The Pederson commitment is calculated using following formula:</p>\n<div class=\"math\">\nC = a_0 B_0 + a_1 B_1 + ... + a_{255} B_{255} = Commit(a_0, a_1, ... , a_{255})\n</div>\n<p>where <span class=\"math\">B_i</span> (basis) are already known, fixed points on the elliptic curve and <span class=\"math\">a_i</span> are values we are being committed to. The values <span class=\"math\">a_i</span> have to be from the scalar field of the elliptic curve, which in our case has less than <span class=\"math\">2^{253}</span> elements.</p>\n<p>The Pedersen commitment itself is a point on the elliptic curve. The Pedersen hash is a value in a scalar field derived from the Pederson commitment.</p>\n<h3><a name=\"structure-overview-6\" class=\"anchor\" href=\"https://ethresear.ch#structure-overview-6\"></a>Structure overview</h3>\n<p>Verkle Trie is a trie structure with a branching factor of 256, and is used for storing key-value pairs where both key and value are arrays of 32-bytes.</p>\n<p>Verkle trie has 2 types of nodes:</p>\n<ul>\n<li>Inner node - contains up to 256 Pedersen hashes, representing the 256 children nodes</li>\n<li>Extension node - contains the path from the root of the trie (31-byte long, called stem) and up to 256 values (for each value of the last byte, suffix)\n<ul>\n<li>Each value is 32-byte long array</li>\n</ul>\n</li>\n</ul>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/7/a/7a12397bf7f94f89c116dc88c4ccfb5d07e0c52c.png\" data-download-href=\"https://ethresear.ch/uploads/default/7a12397bf7f94f89c116dc88c4ccfb5d07e0c52c\" title=\"image - Verle trie\"><img src=\"https://ethresear.ch/uploads/default/optimized/3X/7/a/7a12397bf7f94f89c116dc88c4ccfb5d07e0c52c_2_690x341.png\" alt=\"image - Verle trie\" data-base62-sha1=\"hpTiqQWhSKE2abSjgX7TUU8qjnu\" width=\"690\" height=\"341\" srcset=\"https://ethresear.ch/uploads/default/optimized/3X/7/a/7a12397bf7f94f89c116dc88c4ccfb5d07e0c52c_2_690x341.png, https://ethresear.ch/uploads/default/original/3X/7/a/7a12397bf7f94f89c116dc88c4ccfb5d07e0c52c.png 1.5x, https://ethresear.ch/uploads/default/original/3X/7/a/7a12397bf7f94f89c116dc88c4ccfb5d07e0c52c.png 2x\" data-dominant-color=\"F3F5F4\"></a></div><p></p>\n<p><em>Representation of a walk through a Verkle tree for the key <code>0xfe0002abcd..ff04</code>: the path goes through 3 internal nodes (children: 254, 0, 2) and an extension node. Image taken from <a href=\"https://blog.ethereum.org/2021/12/02/verkle-tree-structure\" rel=\"noopener nofollow ugc\">here</a>.</em></p>\n<h4><a name=\"extension-node-structure-7\" class=\"anchor\" href=\"https://ethresear.ch#extension-node-structure-7\"></a>Extension node structure</h4>\n<p>Value that is stored in a trie is 32-bytes, which can’t be uniquely mapped to the elliptic curve scalar field (less then <span class=\"math\">2^{253}</span> elements). To circumvent this problem, the 256 values of the Extension node are split into two groups of 128 each, and each value is split into two 16-bytes values: <span class=\"math\">v_i = v_i^{low} + v_i^{high}</span>.</p>\n<p>Also, in order to distinguish absent from zero values, when <span class=\"math\">v_i</span> is accessed, the marker is set on the 129th bit of the <span class=\"math\">v_i^{low}</span>, resulting in: <span class=\"math\">v_i^{low+access} = v_i^{low} + 2^{128}</span></p>\n<p>Two commitments of each group of 128 values is calculated separately (<span class=\"math\">C_1</span> and <span class=\"math\">C_2</span>):</p>\n<div class=\"math\">\n\\begin{align*}\nC_1 &amp;= Commit(v_0^{low+access}, v_0^{high}, v_1^{low+access}, v_1^{high},  ... , v_{127}^{low+access}, v_{127}^{high}) \\\\\nC_2 &amp;= Commit(v_{128}^{low+access}, v_{128}^{high}, v_{129}^{low+access}, v_{129}^{high},  ... , v_{255}^{low+access}, v_{255}^{high})\n\\end{align*}\n</div>\n<p>They are then combined with the stem to calculate commitment of the entire Extension node:</p>\n<div class=\"math\">\nC = Commit(1, stem, C_1, C_2)\n</div>\n<h3><a name=\"other-important-details-8\" class=\"anchor\" href=\"https://ethresear.ch#other-important-details-8\"></a>Other important details</h3>\n<p>There are many more details regarding Verkle trie, but only important ones for the rest of this research are given below:</p>\n<ul>\n<li>Unlike MPT, smart contract’s bytecode and storage values are stored within the same trie\n<ul>\n<li>contract’s storage slots are mapped to 32 bytes, which are used as a key of the Verkle Trie</li>\n<li>bytecode is split into chunks of 31 bytes</li>\n</ul>\n</li>\n<li>Values that are likely to be accessed together (e.g. balance and nonce, consecutive contract’s storage slots, bytecode chunks) are likely going to end up in the same Extension node</li>\n<li>It’s possible to update node’s commitment if we know previous commitment, which children changed, and previous and new commitment of those children\n<ul>\n<li>Most EL clients use this and only store commitments of the Inner nodes (not inner nodes themselves)</li>\n<li>Same approach doesn’t work well for archive nodes</li>\n</ul>\n</li>\n</ul>\n<h2><a name=\"portal-network-verkle-trie-9\" class=\"anchor\" href=\"https://ethresear.ch#portal-network-verkle-trie-9\"></a>Portal Network &amp; Verkle Trie</h2>\n<p>The simplest approach would be do the same for Verkle Trie as we are doing in the case of Merkle Patricia Trie: <em>store every trie node</em>. However, this leads to much higher storage requirement for the Portal Network.</p>\n<p>When block is executed, updated trie nodes (except the root and the first layer nodes) would have only few children values updated while majority would stay the same. If we do naive approach (store every trie node as it is), Portal Network as a whole would need significantly more storage. This issue is significantly more impactful with Verkle trie in comparison with MPT because of the bigger branching factor (256 vs. 16).</p>\n<p>However, we can do this in a smarter way. Let’s consider Inner node <span class=\"math\">C</span> with children <span class=\"math\">C_0, C_1, ... ,C_{255}</span>. Instead of storing it as a single node, we can split it into 2-layer trie:</p>\n<div class=\"math\">\n\\begin{alignat*}{3}\nC &amp;=&amp; C'_0 + C'_1 &amp;+…+ C'_{15} \\\\\nC'_0 &amp;=&amp; C_0B_0 + C_1B_1 &amp;+…+ C_{15}B_{15} &amp;&amp;= Commit(C_0,…,C_{15}, 0,…,0) \\\\\nC'_1 &amp;=&amp; C_{16}B_{16} + C_{17}B_{17} &amp;+…+ C_{31}B_{31} &amp;&amp;= Commit(0,…,0, C_{16},…,C_{31}, 0,…,0) \\\\\n&amp;&amp;&amp;\\quad… \\\\\nC'_{15} &amp;=&amp; C_{240}B_{240} + C_{241}B_{241} &amp;+…+ C_{255}B_{255} &amp;&amp;= Commit(0,…,0, C_{240},…,C_{255}) \\\\\n\\end{alignat*}\n</div>\n<p></p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ethresear.ch/uploads/default/original/3X/9/e/9ec60b31ab7d4af8127fe23d8f077b2ab9430f6c.png\" data-download-href=\"https://ethresear.ch/uploads/default/9ec60b31ab7d4af8127fe23d8f077b2ab9430f6c\" title=\"image - Inner node split\"><img src=\"https://ethresear.ch/uploads/default/original/3X/9/e/9ec60b31ab7d4af8127fe23d8f077b2ab9430f6c.png\" alt=\"image - Inner node split\" data-base62-sha1=\"mEzKjGWf18V3ad8ofvLYV2Xc0JS\" width=\"690\" height=\"257\" data-dominant-color=\"B8C9D8\"></a></div><p></p>\n<p>This would result in the root node of such trie having the same commitment as it would have otherwise. But this way, we would have a trie with a branching factor of 16. Similar approach can be applied to the Extension node as well.</p>\n<p>It’s also worth noting that this splitting can be done again (leading to 4-layer trie with branching factor of 4), or even multiple times (8-layer binary trie), or even in alternative way (3-layer trie with branching factors <span class=\"math\">[4,8,8]</span>). Deeper tries would probably require less total storage, but they would make lookup queries more expensive.</p>\n<h3><a name=\"other-considered-solutions-10\" class=\"anchor\" href=\"https://ethresear.ch#other-considered-solutions-10\"></a>Other considered solutions</h3>\n<h4><a name=\"path-based-content-id-11\" class=\"anchor\" href=\"https://ethresear.ch#path-based-content-id-11\"></a>Path based <code>Content-Id</code></h4>\n<p>Another alternative solution that seems plausible is to make <code>Content-Key</code> and/or <code>Content-Id</code> dependent only on the path from the root, not the trie commitment itself.</p>\n<p>Benefits of this approach would be that the same Portal Network node can optimize its storage in order not to duplicate values that don’t change over time (e.g. by storing only diffs).</p>\n<p>However, this approach has a main downside, which is that content space is not uniformly distributed. The trie nodes associated with frequently used smart contracts (e.g. WETH, DEX pools, Layer-2, Oracles, …) will have significantly more updates than the average trie node. This non-uniformity breaks linear relationship between node’s radius and the amount of data that it needs to store, which is the major assumption about Portal Network design.</p>\n            <p><small>1 post - 1 participant</small></p>\n            <p><a href=\"https://ethresear.ch/t/portal-network-verkle/19339\">Read full topic</a></p>","link":"https://ethresear.ch/t/portal-network-verkle/19339","pubDate":"Fri, 19 Apr 2024 12:46:04 +0000","discourse:topicPinned":"No","discourse:topicClosed":"No","discourse:topicArchived":"No","guid":{"@isPermaLink":"false","#text":"ethresear.ch-topic-19339"},"source":{"@url":"https://ethresear.ch/t/portal-network-verkle/19339.rss","#text":"Portal Network & Verkle"},"filter":false}]}}}